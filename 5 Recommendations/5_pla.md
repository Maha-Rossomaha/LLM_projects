# Plan

## День 1: Введение в рекомендательные системы
### Темы и задачи:
* Понять роль рекомендательных систем в современных сервисах (e-commerce, стриминг, соцсети и др.).
* Разобрать типы рекомендаций: коллаборативные vs контентные vs гибридные
* Изучить основные вызовы: холодный старт (нет данных о новом пользователе/предмете), разреженность данных, масштаб (миллионы пользователей/товаров), необходимость персонализации.
* Рассмотреть схему данных: матрица пользователь–товар с явным (рейтинги, лайки) и неявным фидбэком (просмотры, клики)
* Понять, что цель рекомендаций – предложить пользователю новое, актуальное.
### Рекомендуемые материалы:
* Шпаргалка по рекомендательным системам (статья на Хабре) – вводный обзор с примерами
* Статья OTUS «Рекомендательные системы в современном мире» – бизнес-обзор использования рекоммендеров и их преимуществ
* Курс ODS DataFest 2020: «Ваша первая рекомендательная система с нуля» – лекция 1: введение (Даниил Потапов).
* Глава 1 из курса IntSys MIPT «Задача рекомендаций» – постановка проблемы и виды подходов
### Практика:
* Установите знакомый датасет (например, MovieLens 100K). Проанализируйте его: сколько пользователей, фильмов, плотность матрицы.
* Постройте Long Tail график популярности фильмов (распределение взаимодействий), убедитесь, что есть «голова» (немного очень популярных фильмов) и «длинный хвост» малоизвестных. Это поможет понять разреженность и проблему популярности.
* Выпишите, какие метрики и бизнес-цели важны для рекомендаций вашего домена (например, удержание пользователей, увеличение продаж, удовлетворённость). Сформулируйте 2–3 примера, где неправильные рекомендации могут навредить (и как этого избежать).

## День 2: Памятные методы – User-based и Item-based Collaborative Filtering
### Темы и задачи:
* Разобрать коллаборативную фильтрацию (CF): подходы user–to–user и item–to–item. Понять идею: «похожие пользователи любят похожие вещи» и «похожие товары нравятся одним пользователям». CF опирается только на матрицу взаимодействий без явных признаков контента.
* User-based CF: поиск «соседей» текущего пользователя (с схожими оценками) и рекомендация товаров, популярные у этих соседей.
* Item-based CF: для каждого товара найти похожие (по оценкам одних и тех же пользователей), и рекомендовать пользователю товары, похожие на уже понравившиеся ему.
* Метрики схожести: косинусное сходство, корреляция Пирсона (для явных рейтингов), мера Жаккара (для бинарных предпочтений). Понятие K ближайших соседей (KNN) – используем топ-K похожих пользователей/товаров для предсказания рейтинга или релевантности
* Преимущества и ограничения memory-based CF: простота, интерпретируемость, но страдает при холодном старте (недостаточно данных о новом юзере/товаре) и может не улавливать скрытые факторы предпочтений.
### Рекомендуемые материалы:
* Раздел «Коллаборативная фильтрация» в шпаргалке – объяснение user2user и item2item, формула расчёта похожести.
* Статья на Habr «Создание простого рекомендательного алгоритма (коллаб. фильтрация)» – практический разбор с кодом.
* Coursera: «Recommender Systems Specialization» – курс от University of Minnesota, Week 2 посвящена memory-based методам.
* Глава 2 курса IntSys – модели на основе ближайших соседей (конспект лекции).
### Практика:
* Реализуйте простую item-item CF на MovieLens: для топ-100 популярных фильмов найдите 10 самых похожих по профилям оценок (можно использовать косинусное сходство). Проверьте интуитивно результаты: похожи ли фильмы жанрово?
* Напишите функцию для user-user CF: по историям просмотров определите 5 ближайших «соседей» данного пользователя и порекомендуйте топ-5 фильмов, которые они смотрели, а наш пользователь – ещё нет. Оцените качество на паре знакомых вам пользователей.
* Попробуйте библиотеку surprise (Sci-kit Surprise) – алгоритм KNNBasic для коллаб. фильтрации. Сравните результаты user-based и item-based на MovieLens (можно измерить Precision@5/Recall@5).
* Подумайте, как вы бы учли неявный фидбэк (просмотры без оценки): возможно, через бинаризацию матрицы (просмотр = 1) и меру Жаккара.
* Попробуйте реализовать на упрощённых данных.

## День 3: Матричная факторизация – SVD, ALS, BPR и скрытые факторы
### Темы и задачи:
* Изучить модель скрытых факторов: предположение, что у каждого пользователя и товара есть векторы в общем латентном пространстве. Matrix Factorization (MF) пытается разложить матрицу взаимодействий R на произведение латентных факторов пользователей (U) и товаров (V). Это позволяет обобщать предпочтения и справляться с разреженными данными лучше, чем память-ориентированные методы
* Понять связь с SVD: сингулярное разложение матрицы – классический метод линейной алгебры. В рекомендательных задачах обычно оптимизируют разложение, минимизируя ошибку восстановления известных рейтингов. Формально, решаем $\argmin||R - U\cdot V^T||$.
* Разобрать ALS (Alternating Least Squares) – метод, популярный для implicit данных (в библиотеке Spark/MLlib и python-линеге implicit). Он чередует оптимизацию по U и V, сходится к неплохому решению, устойчив к масштабам данных.
* Изучить BPR (Bayesian Personalized Ranking) – pairwise подход оптимизации: вместо минимизации MSE по рейтингам, максимизирует разницу прогнозов между реально взаимодействованными и невзаимодействованными парами для пользователя. BPR-loss широко используется для ранжирования по implicit-фидбэку.
* Обратить внимание на обработку неявного фидбэка: в отличие от явных рейтингов, отсутствие взаимодействия не равно «0 рейтинг». Обычно implicit MF (как ALS/BPR) трактует наблюдаемые взаимодействия как положительные, а ненаблюдаемые – как отрицательные с весом.
### Рекомендуемые материалы:
* «Матричные факторизации» в шпаргалке – интуиция SVD, как сжимается разреженная матрица взаимодействий.
* Пост «Простые рекоммендательные системы: ALS, NDCG» – блог с примером реализации ALS и вычисления метрик.
* Medium: Better Recommender Systems with LightGCN – раздел про MF как baseline, включает формулу факторизации и BPR-лосс
* Статья «Factorization Machines (FM)» – (шпаргалка, раздел 3) расширяет MF для учёта дополнительных признаков, дает понимание, как факторизация используется в рекомендациях.
### Практика:
* Примените библиотеку implicit (Python) – обучите модель ALS на MovieLens (implicit версии, трактуя рейтинг >3 как положительный сигнал). Сгенерируйте топ-10 рекомендаций для нескольких пользователей. Оцените Recall@10, Precision@10 (сформировав тест/трейн разбиение по пользователям).
* Реализуйте с нуля небольшую MF-модель: используйте SGD для минимизации MSE на известных рейтингах MovieLens. Попробуйте 10–20 факторов, 5 эпох. Посмотрите на получившиеся латентные векторы: какие фильмы оказались близки в латентном пространстве?
* Реализуйте упрощённый BPR-оптимизатор: для выбранного пользователя возьмите один его просмотренный товар и один не просмотренный, скорректируйте их векторы, чтобы предпочтённый имел больший скор. Это поможет прочувствовать идею pairwise обучения. (Можно ограничиться парой итераций для понимания).
* Если есть возможность, попробуйте библиотеку lightfm (оптимизирует BPR или WARP-loss) на датасете MovieLens. Сравните с ALS по качеству Recall@K.

## День 4: Графовые методы – LightGCN и распространение предпочтений
### Темы и задачи:
* Осознать, что взаимодействия пользователей и предметов можно рассматривать как двухчастный граф: узлы двух типов, ребро – факт взаимодействия. LightGCN – современный метод, упрощающий графовые нейронные сети для рекомендательных систем. Он использует только агрегирование соседей на графе, без сложных нелинейностей, и показывает отличные результаты на коллаборативной задаче.
* Понять идею LightGCN: каждое итеративное распространение берет среднее векторов соседей (пользователи собирают предпочтения своих понравившихся товаров, товары – предпочтения пользователей) и обновляет эмбеддинги. Многократное применение позволяет учитывать высокие порядки связи: пользователь связан не только с товарами, которые он лайкал, но и с товарами, похожими на них через общих других пользователей, и т.д.
* Разобрать, почему LightGCN проще классических GCN: убраны весовые матрицы и активации – по сути это факторизация, обогащённая многократным усреднением по графу. Это даёт лучший учет структуры данных при минимальных вычислениях
* Посмотреть на результаты: LightGCN обычно превосходит обычную MF в сценариях с разреженными данными, так как учитывает косвенные связи между пользователями и предметами.
* Отметить, что графовые подходы – активно развивающаяся область (есть и более сложные: PinSage, NGCF), но LightGCN достаточно знать для интервью как пример state-of-the-art CF модели.
### Рекомендуемые материалы:
* Статья «LightGCN: Simplifying and Powering GCN for Recommendation» (He et al, 2020) – хотя бы прочитать введение и выводы (можно на ArXiv).
* Кейс-стади: «Построение рекомендателя рецептов с LightGCN» – блог на Medium (пример применения LightGCN для рекомендаций рецептов, с кодом и объяснениями преимуществ над SVD).
* Раздел про LightGCN в обзоре RecBole (opensource-библиотека) – кратко об идее и реализации.
* Видео «Building RecSys with GNN – Part 2: LightGCN (PyTorch)» – подробный разбор реализации LightGCN.
### Практика:
* Используя библиотеку RecBole или torch_geometric, обучите LightGCN на упрощённом датасете (например, MovieLens 1M). Получите эмбеддинги и сделайте рекомендации. Сравните метрики (Recall@10, nDCG@10) с ALS из Дня 3.
* Если не получается запустить готовую реализацию, попробуйте реализовать один шаг агрегирования LightGCN вручную: постройте список соседей для каждого узла и обновите эмбеддинги как среднее соседей. Протестируйте 2–3 итерации на маленьком примере (5 пользователей, 5 товаров). Убедитесь, что эмбеддинги пользователей с общими соседями становятся похожими.
* Проанализируйте проблему холодного старта в LightGCN: что если новый товар без связей? (Ответ: никак не порекомендуется без содержания – нужно гибридный подход). Запишите свои мысли, как бы вы интегрировали контентный сигнал (например, создавали бы узлы атрибутов или использовали бы предварительно обученные эмбеддинги в начальных векторах).
* Для понимания графовой перспективы: визуализируйте небольшой подграф пользователей и товаров с ребрами (можно networkx): посмотрите, как кластеры формируются по интересам. Это даст интуицию, почему графовые методы работают.

## День 5: Контентные подходы, Item2Vec и эмбеддинги для рекомендаций
### Темы и задачи:
* Разобрать контентно-ориентированные рекомендации: когда доступны описания товаров (текст, категории, фичи), можно рекомендовать похожие по содержанию. Простой пример – tf-idf + cosine: на основе слов в описании найти похожие товары.
* Понять ограничения контентного метода: он не учитывает коллективные предпочтения, может не различать качественно товары с похожим описанием, склонен загонять пользователя в «информационный пузырь» (всегда больше того же). Но плюс – решает холодный старт для новых товаров (есть описание) и не зависит от наличия других пользователей с таким вкусом.
* Изучить подход Item2Vec: по аналогии с Word2Vec, рассматриваем последовательности действий пользователей (просмотрел товар A, потом B, потом C) как «предложения» и обучаем модель, которая выдаёт близкие эмбеддинги для товаров, часто встречающихся рядом в пользовательских сессиях. В результате получаются вектора товаров, отражающие схожесть по поведенческому контексту, а не по описанию.
* Рассмотреть пример: в маркетплейсе товары «мини WiFi-камера» и «SQ11» могут не иметь общих слов в названии, но пользователи просматривают их вместе – Item2Vec поймает это сходство
* Обратить внимание: Item2Vec требует достаточно данных о совместных просмотрах. Этот метод использовался в промышленных системах (пример – Авито улучшил рекомендации похожих объявлений с помощью Item2Vec).
* Кратко про two-tower (двухбашенные) нейросети: это обобщение идеи эмбеддингов – одна башня энкодит пользователя (на основе его истории или профиля), другая – товар (на основе контента), их скалярное произведение даёт рейтинг. По сути, collaborative и content признаки можно совместить таким образом. Такие модели (например, DSSM, Youtube DNN) популярны для стадии ретривера.
### Рекомендуемые материалы:
* Статья Авито «Как мы используем item2vec для рекомендаций похожих товаров» – практический взгляд, улучшение item-item рекомендаций с учётом поведения пользователей.
* Доклад на YouTube «Item2vec: как и зачем векторизовать посты и пользователей (Одноклассники)» – Дмитрий Решетников рассказывает о внедрении item2vec в рекомендации соцсети.
* Habr: «Расчёт вкусов пользователя... с помощью эмбеддингов» – про интеграцию item2vec в новостной ленте.
* Medium: «Personalised Search with BERT & ANN» – пример, как контентные эмбеддинги (LABSE, BERT) + ANN используются для рекомендаций фильмов.
### Практика:
* Возьмите датасет MovieLens и соберите для каждого пользователя последовательность фильмов по времени. Обучите простую модель Item2Vec: например, с помощью gensim.models.Word2Vec, где «слова» – ID фильмов, «предложения» – истории пользователей. Получите эмбеддинги фильмов. Затем найдите ближайших соседей для нескольких выбранных фильмов (по косинусному сходству эмбеддингов). Соответствуют ли они интуитивно похожим жанрам/темам?
* Попробуйте контентный подход на MovieLens: у фильмов есть жанры, теги. Сконструируйте для каждого фильма вектор признаков (например, жанры как one-hot). Реализуйте функцию рекомендации: по набору любимых фильмов пользователя верните фильмы с наиболее похожим набором жанров. Оцените, насколько разнообразны такие рекомендации.
* Используя библиотеку sentence-transformers, получите эмбеддинги описаний фильмов (если есть sinopsis или теглайн). Постройте dense retrieval: выбирая любимый фильм пользователя, находите 10 ближайших по эмбеддингу описания (можно через Faiss). Сравните с рекомендациями по collaborative method – есть ли качественная разница?
* (Продвинуто) Если уверенно владеете PyTorch: реализуйте упрощённую двухбашенную сеть для MovieLens (берём ID пользователя и жанры фильма в качестве входов, оптимизируем предсказание рейтинга). Потренируйте, проверьте качество RMSE. Это даст понимание, как в индустрии строят большие модели рекомендаций (например, архитектура YouTube DNN).

## День 6: Многоступенчатый пайплайн – получение кандидатов (retrieval)
### Темы и задачи:
* Осознать, что в промышленных системах прямой перебор всех товаров для каждой рекомендации невозможен – используют многоступенчатый пайплайн. Этап 1: Retrieval (генерация кандидатов) – быстрый отбор небольшой доли наиболее релевантных кандидатов из всех объектов. Цель ретривера – обеспечить высокий Recall: не упустить потенциально интересные варианты, пусть даже с потерей точности.
* Изучить виды retrieval: Sparse Retrieval (разреженный) и Dense Retrieval (плотный).
  * Sparse: традиционный поиск по словам или категориям. Пример – BM25 в поисковых системах или простые бизнес-правила («топ популярные в категории»). Он быстрый, масштабируемый, но может не учитывать скрытых предпочтений.
  * Dense: векторный поиск по эмбеддингам. Пользователю соответствует вектор (например, среднего по его историям), и у каждого товара – вектор (из MF, или контентной модели). Ищем ближайшие по расстоянию (cosine, dot). Требует индексирования (ANN – Approximate Nearest Neighbors) для скорости.
  * Hybrid: сочетание подходов. Например, взять объединение кандидатов: коллаборативные (latent факторы), контентные (по ключевым словам или категориям), популярные новинки и т.д. Смешивание увеличивает разнообразие и покрытие.
* Разобрать, как реализовать ANN-поиск: библиотеки FAISS, hnswlib позволяют за миллисекунды искать ближайшие векторы в базе из миллионов. Понимать параметры: размер графа, торговля между точностью и латентностью.
* Подумать, какие источники кандидатов использовать для разных задач. Например, для медиа-контента: можно завести отдельные ретриверы по жанрам, по друзьям пользователя, по глобальным трендам, по коллаборативным эмбеддингам и объединять их списки (как делает YouTube).
### Рекомендуемые материалы:
* The ML Architect (Part 1) – совет 1 и 2: про баланс скорости и качества, и соответствие моделей этапам. Описывает 4-ступенчатую архитектуру: Retrieval, Filtering, Scoring, Ordering.
* Статья «A layered approach to search (ColBERT)» – описывает, как сначала получить 1000 кандидатов BM25, потом сузить до 100 ColBERT, и 50 – реранкером. Аналогично можно делать и в рекомендациях.
* Пост на Medium «ANN для похожих новостей» – разбор использования FAISS для быстрой выдачи похожих элементов (англ.).
* Документация FAISS – кратко прочитать про типы индексов (Flat vs HNSW vs IVFPQ), понять, как они работают.
### Практика:
* Реализуйте простой sparse retrieval: для каждого пользователя возьмите его последние N просмотренных товаров и соберите объединение похожих (например, через item-item CF соседей из Дня 2) – это и будет список кандидатов. Посчитайте, сколько кандидатов так получается в среднем.
* Попробуйте библиотеку FAISS: возьмите эмбеддинги фильмов из Дня 5 (Item2Vec или MF) и построите индекс. Для пары пользователей получите их средний вектор любимых фильмов и выполните index.search – посмотрите, какие top-10 кандидатов возвращаются. Это имитация dense retrieval.
* Если есть данные, попробуйте BM25: например, на описаниях фильмов MovieLens с помощью rank_bm25 (при наличии текстов). Сформируйте запрос из ключевых слов любимых фильмов пользователя и получите топ результатов. Оцените, отличаются ли они от векторных.
* Скомбинируйте разные источники кандидатов: коллаборативный (например, LightGCN топ-N для пользователя), контентный (BM25), популярные у похожих по профилю пользователей. Объедините списки, устраните дубликаты. Получится более широкий набор. Попробуйте измерить его Recall@100 по отношению к реальным интересам пользователя (например, проверяя, вернулся ли любимый фильм пользователя, если убрать его перед рекомендацией).
* Напишите функцию, оценивающую Coverage кандидатов: доля всех товаров, которые могут появиться в кандидатах хотя бы у одного пользователя. Попробуйте повысить coverage, добавляя разные источники (например, случайные или новые товары).

## День 7: Персонализация и фильтрация – учитываем контекст и бизнес-правила
### Темы и задачи:
* Рассмотреть шаг персонализации/фильтрации между кандидатом и ранжированием. В некоторых системах это явный отдельный этап (например, Filtering как у Netflix): удаляет из кандидатов неподходящие или нерелевантные объекты.
* Фильтрация бизнес-правил: убираем контент, который пользователь уже видел, или который ему не доступен (гео-регион, возрастные ограничения), или нежелательный (например, пользователь скрыл товар – не показывать снова). Это обязательно нужно учесть прежде чем ранжировать.
* Персонализация контента: если ретривер дал общие популярные кандидаты, на этом этапе можно отфильтровать/отранжировать их по простым персональным сигналам. Например, у пользователя категории интересов – можно повысить кандидатов из его любимых категорий. Или учесть текущий контекст: время дня, платформа (mobile/desktop), сессионные данные.
* Онлайн vs офлайн модели: обсудить, как офлайн-вычисленные рекомендации дополняются онлайновыми. Пример: Авито имеет офлайн MF модель (обучается раз в несколько дней, даёт качественные но не свежие рекомендации) и онлайн-модель, мгновенно реагирующую на новые действия пользователя, но более грубую (по контенту). На этапе персонализации можно объединять их результаты: часть кандидатов – из офлайн (глубокие предпочтения), часть – онлайн (учитывающие последний клик).
* Учесть контекстные факторы: можно отфильтровать товары, уже купленные, или недавно виденные. Можно добавлять обратную связь реального времени: если пользователь явно дизлайкнул рекомендацию, не показывать ей подобные (контентные фильтры на основе фичей дизлайкнутого объекта).
* Подготовиться, что на собеседовании могут спросить: «Как бы вы спроектировали систему, которая сразу реагирует на новые тренды (например, вирусное видео)?» – здесь как раз важно разделение: быстрый pipeline обновления (онлайн персонализация) + периодический пересчёт (офлайн модель).
### Рекомендуемые материалы:
* The ML Architect (Part 1) – раздел Filtering: описывает второй этап, где убираются неподходящие кандидаты (пример – недоступный контент, out-of-stock товары).
* Статья «Многорукие бандиты в рекомендациях» (Avito) – раздел про выбор целевых действий показывает, как разные конфиги выдачи можно переключать онлайн. Это про explore-exploit, но и про персонализацию в реальном времени.
* Блог Netflix Tech: «Personalized Ranking Systems» – хоть и англ., содержит описание, как Netflix фильтрует каталог для пользователя (учёт жанров, продолжительности сериалов и пр.).
* Книга «Рекомендательные системы на практике» (Х. Бен-Шабат, на рус.) – глава про бизнес-правила, контекстные рекомендации.
### Практика:
* Снова возьмите ваш список кандидатов (из Дня 6). Реализуйте набор фильтров: а) исключить контент, который пользователь уже видел или оценил; б) исключить явно несоответствующий профилю (если у пользователя указан регион «DE», а товар доставляется только в US, и т.п. – придумайте критерий); в) ограничить максимум N кандидатов от одной категории (для разнообразия). Примените эти фильтры – насколько сократился список?
* Создайте простую модель персонализации: возьмите фичи пользователей (например, жанры, которые он чаще смотрит) и фичи товаров (жанр). Придумайте heuristic score: например, доля жанрового совпадения. Пересортируйте кандидатов по этому скору. Посмотрите, улучшилось ли соответствие вкусу (можно вручную проверить для знакомого пользователя).
* Имитация реального времени: выберите пользователя и предположите, что он только что посмотрел фильм X. Добавьте фильм X в его историю и измените кандидатов: добавьте «похожие на X» (контентно или item2vec). Это и есть реакция онлайн. Оцените, насколько новые кандидаты обогащают выдачу (например, появились фильмы в новом жанре, связанном с X).
* Experiment: Возьмите два разных профиля пользователя (например, любитель боевиков vs любитель мелодрам) и примените свои фильтры/персонализацию к одному и тому же набору кандидатов (например, топ-50 популярных фильмов). Сравните финальные топ-10: разные ли они? Если нет, подумайте, какие персональные сигналы ещё учесть.
* Подготовьтесь ответить: как бы вы справились с ситуацией, когда бизнес хочет продвигать определённые товары? (намёк: можно на этапе фильтрации/персонализации инъектировать гарантированные кандидаты – “вставки”, или помечать их для повышения на этапе ранжирования).

## День 8: Ранжирование кандидатов – модели и фичи (LTR, re-ranking)
### Темы и задачи:
* Изучить заключительный этап основного пайплайна – ранжирование (Scoring, Re-ranking). Здесь из ~100 кандидатов нужно выбрать финальный топ (скажем, 5–20 рекомендаций) в наилучшем порядке. Используются более тяжёлые ML-модели, учитывающие множество признаков. Цель – максимально точно предсказать интерес пользователя к кандидату (клик, просмотр, покупка).
* Разобрать, что такое Learning-to-Rank (LTR): методы машинного обучения, оптимизирующие порядок выдачи. Популярны градиентные бустинги (GBDT) – например, LightGBM Ranker, CatBoost Ranking – и нейросетевые модели.
* Понять разницу между pointwise, pairwise и listwise подходами в LTR: pointwise предсказывает некое скалярное значение для каждого кандидата (например, вероятность лайка) и сортирует; pairwise учится на паре объектов, пытаясь упорядочить их правильно (пример – лосс типа BPR); listwise оптимизирует метрику списка (например, nDCG) напрямую, но сложнее. Для начала сконцентрируемся на pointwise – проще реализовать.
* Рассмотреть примеры признаков для ранжирования:
  * Признаки пользователя (демография, исторические интересы: любимый жанр, средняя цена покупок и т.д.),
  * Признаки товара (категория, популярность, новизна, цена, рейтинг),
  * Признаки взаимодействия пользователя и товара (например, был ли этот товар уже показан раньше, позиция в каталоге, схожесть товара с тем, что пользователь лайкал).
* Изучить классический pipeline: берём кандидатов, вычисляем для каждого набор фичей (user, item, user-item), подаем в модель (например, LightGBM) и получаем скор. Модель обучена офлайн на данных о том, что пользователь взаимодействовал (или не взаимодействовал) с какими-то объектами.
* Обсудить нейросетевые re-rankers:
  * Cross-Encoder (BERT): модель, которая на вход берет сразу пару (user context + item описание) и выдаёт релевантность. Очень точная, учитывает глубокие взаимосвязи (например, текст отзыва и профиль), но дорогая – нужна отдельная инференс на каждый кандидат. Обычно применима только на top-10 (иначе задержки высоки).
  * ColBERT: компромисс, использует late interaction – предварительно вычисляет эмбеддинги токенов документов, а запрос обрабатывает на лету, сопоставляя с токенами. Это ускоряет ранжирование сотен кандидатов с более тонким учетом текста, чем простой dot-product эмбеддингов. ColBERT требует памяти на хранение токеновых эмбеддингов, но позволяет ранжировать эффективнее, чем полный cross-encoder, на больших N.
  * Нейросети на табличных данных: например, DLRM (Facebook) – комбинирует эмбеддинги id-шников и плотные признаки, проходя через MLP. Такие модели могут быть использованы для re-ranking вместо GBDT, особенно когда много категориальных фич (ID товаров, ID категорий и т.д.).
* Отметить, что финальная модель – это то, что непосредственно влияет на метрики качества (CTR, конверсия). На собеседовании часто спрашивают: «Какую модель вы бы выбрали для ранжирования и почему?». Хороший ответ: «GBDT (LightGBM) – потому что легко интерпретировать фичи, быстрый inference, хорошо работает с табличными данными. Но можно улучшить нейросетью, если данных много и хочется учесть сложные взаимодействия признаков.»
### Рекомендуемые материалы:
* Ultralytics Glossary: «Реранкер» – кратко и по-русски, что делает re-ranker и зачем нужен второй этап после ретривера.
* Статья «Cross-Encoders, ColBERT и LLM Re-Rankers» (Michael Ryaboy, 2025) – свежий обзор trade-off между разными подходами ранжирования.
* Документация LightGBM (Ranker) – как использовать LGBMRanker (примеры, параметры).
* CatBoost Rank – статья на Habr «Learning to Rank с CatBoost» с примерами на поисковом датасете.
* Видео Exponent «Design a Recommendation System (ML interview)» – там есть часть про выбор моделей для ранжирования.
* (Дополнительно) Medium: «Как ColBERT улучшает поиск без боли» – практический гайд по ColBERT (англ.).
### Практика:
* Возьмите кандидатов из предыдущих дней и создайте для них табличку признаков. Например, для каждой пары (user, item) запишите: количество взаимодействий пользователя, средний рейтинг пользователя, жанр фильма, год выпуска фильма, популярность фильма (число рейтингов), индикатор совпадения жанра с самыми часто смотримыми жанрами пользователя и т.п. Получится таблица, где строка – кандидат. Также пометьте таргет: interacted / not interacted (например, был ли рейтинг >3).
* Разбейте данные на обучающую и тестовую выборку (например, по времени: кандидаты из более старых взаимодействий vs новые). Обучите LightGBM Ranker или даже простой LGBMClassifier для предсказания вероятности взаимодействия. Оцените NDCG@10 на тесте: насколько высоко в ранжированном списке модель ставит реально просмотренные фильмы.
* Проанализируйте важность признаков (feature importance) в модели – какие оказались наиболее значимы? Например, часто это «был ли фильм популярным в целом» или «наличие жанра X, который любит пользователь». Подумайте, нет ли у модели bias к популярности.
* Интегрируйте модель в пайплайн: напишите функцию recommend(user) – генерирует кандидатов (Retrieval), фильтрует (Filtering), вычисляет признаки и применяет модель ранжирования, возвращает топ-N. Протестируйте для нескольких пользователей. Это ваш первый прототип end-to-end рекомендательной системы!
* Если успеете, попробуйте CatBoost с режимом Ranking (parameters loss_function=YetiRank или PairLogit). Сравните с LightGBM – где получили лучше NDCG?
* (Дополнительно) Попробуйте внедрить простую нейросеть вместо GBDT: например, MLP на тех же признаках. Посмотрите на качество и время обучения. Это покажет, почему GBDT часто выигрывает на табличных данных.
* Для понимания pairwise обучения: возьмите выученную модель и убедитесь, что средний скор реальных позитивных взаимодействий выше, чем у негативных для каждого пользователя. Можно вычислить MRR (Mean Reciprocal Rank) или MAP@K на вашем тесте.

## День 9: Инженерия признаков для рекомендаций
### Темы и задачи:
* Понять, что качество модели ранжирования сильно зависит от информативности признаков (features). Из исходных данных (логи взаимодействий, профили, контент) нужно придумать и рассчитать те характеристики, которые коррелируют с предпочтениями.
* Обзор типов фичей:
  * User features: демография (возраст, пол, местоположение), агрегаты поведения (сколько всего просмотрено, любимый жанр/категория, день недели наибольшей активности, давность последней активности, сегмент лояльности и т.п.).
  * Item features: статичные – категория, цена, рейтинг товара, создатель, текстовое описание, длина (для медиа), popularity (частота просмотров, продажи), новизна (время добавления).
  * Context features: время суток, устройство (mobile/desktop), реферальный источник, текущая сессия (например, пользователь пришёл из поискового запроса «комедия» – учесть это как признак).
  * User-Item interaction features: ключевой тип. Примеры: было ли у пользователя взаимодействие с этим товаром раньше (и сколько раз?), сколько похожих товаров пользователь уже посмотрел (например, сколько фильмов того же режиссера он видел), позиция товара в списке кандидатов (можно учесть на этапе ранжирования для разницы между топовым кандидатом и десятым), embeddings взаимодействия (например, скалярное произведение эмбеддингов user и item из MF – хороший признак!).
* Отдельно: embeddings как признаки. Можно заранее обучить модели (как в Дни 3-5) и потом взять их output (например, 32-мерный вектор пользователя и товара) – и включить эти числа как дополнительные фичи в бустинг. Часто так делают: предсказания более простой модели MF становятся фичой для более сложной модели (так называемый two-stage learning).
* Обсудить Feature Store: в продакшене признаки вычисляются офлайн и хранятся в специальном хранилище, откуда online-сервисы могут быстро их получать. Важно поддерживать консистентность – те же самые фичи на этапе обучения и в реальном времени для новых данных. Инженеру важно уметь описать, как бы он организовал хранение и обновление фич для рекоммендера (например, обновление агрегатов раз в сутки, или подсчёт в потоковом режиме с выдержкой).
* Подумать о дрейфе признаков: со временем распределение фичей меняется (например, вошёл в моду новый жанр, и старые любимые жанры уже не так важны). Обновлять фичи нужно регулярно, а модель – переобучать, чтобы подстраиваться.
* Интерпретация и контроль: некоторые признаки могут вызывать нежелательные эффекты (например, слишком сильный вес популярности приведёт к тому, что все видят одно и то же – filter bubble). Нужно следить за этим и, возможно, вводить ограничения или регуляризации.
### Рекомендуемые материалы:
* Feature engineering for RecSys (Netflix TechBlog) – статья о том, какие фичи Netflix использует (англ.).
* «Feature Store Architecture» (Qwak blog) – общие концепции, зачем нужен feature store и как он интегрируется.
* Доклад «Building High-Performance Recommender with Feature Stores» – видео (англ., YouTube) об опыте использования Feast/Hopsworks.
* Хабр: «Метрики оценки реком. систем» – хотя о метриках, затрагивает и анализ покрытий, персонализации, что по сути являются признаками качества рекомендаций.
* Статья ODS: «Ваш второй RecSys» – возможно, содержит секцию про генерацию фич (если у вас есть доступ к материалам ODS).
### Практика:
* Дополните вашу таблицу признаков из Дня 8 новыми фичами: посчитайте для пользователя количество разных жанров среди просмотренных фильмов (мера разнообразия интересов), для товара – позицию в популярных у данного пользователя жанра (например, если фильм – комедия, а комедии составляют 40% его просмотров, то 0.4 как фича), добавьте бинарный признак «новинка» (фильм вышел в последние 2 года), признак «бестселлер» (фильм в топ-100 по просмотрам). Обучите модель заново и проверьте, вырос ли NDCG/Precision.
* Попробуйте вместо ручных эмбеддингов добавить user_embedding и item_embedding из LightGCN или MF модели как фичи (да, можно прямо 16-32 чисел добавить). Посмотрите, улучшит ли это качество на валидации. (Часто улучшает, так как бустинг сам скоррелирует их).
Создайте Feature Store прототип: сохраните рассчитанные фичи пользователей и товаров в CSV/SQL. Напишите функцию, которая по user_id и списку item_id быстренько вытягивает соответствующие фичи из ваших хранилищ и склеивает для модели. Это небольшая симуляция того, что происходит в реальной системе с feature store и онлайн запросом.
Оцените проблему давности данных: например, посмотрите, отличаются ли признаки пользователя, вычисленные на данных полугодовой давности, от текущих. Имитация: отрежьте последние 6 месяцев истории и сгенерируйте фичи; затем сравните с фичами на всей истории (метрики сходства, например, жанровый профиль). Представьте, что профиль пользователя со временем меняется – как быстро вы обновите фичи? Напишите план: например, «будем пересчитывать еженочно все агрегаты» или «будем обновлять инкрементально при каждом новом действии через стриминг».
Вопрос на подумать (для интервью): «Как бы вы добавили новый признак в уже работающую модель? Как проверить, что он действительно улучшает рекомендации?» – Хороший ответ: через офлайн эксперимент (обучить модель с новым признаком и без, сравнить метрики, убедиться что приносит выигрыш), а затем провести A/B-тест двух версий ранжировщика, т.к. не все офлайн улучшения дают онлайн эффект.

## День 10: Постобработка – финальные правки: новизна, диверсификация, ограничения
### Темы и задачи:
* Понять, что после получения финального ранжированного списка, могут применяться ещё некоторые правила постобработки (Ordering stage).
* Они не основаны на модели, а продиктованы бизнес-логикой или UX-требованиями.
* Диверсификация: часто модель склонна рекомендовать много похожих объектов (например, 10 фильмов одного франчайза). Нужно искусственно разнообразить список – добавить различия (жанры, артисты и пр.). Методы: перестановка с учётом максимизации разнообразия (например, Maximal Marginal Relevance), или простое ограничение – не более N из одной категории. Метрики вроде Intra-list Similarity измеряют разнообразие выдачи.
* Новизна и сюрпризность: чтобы удерживать интерес, иногда включают объекты, которые пользователь не ожидал, но они новые. Например, один слот отдать недавнему релизу или менее популярному предмету (это также борется с проблемой popularity bias, когда алгоритм показывает только популярное). Метрика Coverage отслеживает, сколько разных объектов система вообще рекомендует. Постобработка может увеличить coverage, вставляя редкие товары.
* Бизнес-правила: гарантированные позиции – например, закрепить на первом месте вручную выбранный промо-товар или партнерский контент. Или наоборот, фильтрация нежелательного: возможно, на этапе фильтрации уже сделали, но на постобработке можно подстраховаться.
* UI/презентация: возможно, нужно сгруппировать рекомендации по темам (например, «Потому что вы смотрели X», «В тренде в вашем регионе»). Тогда постобработка распределяет результаты по этим группам.
* Этичность и fairness: может потребоваться скорректировать выдачу для соблюдения справедливости – например, не отдавать все рекомендации только с одного источника/провайдера контента. Постобработка может чередовать контент от разных поставщиков, чтобы не было монополии.
* Персональные ограничения: если пользователь явно указал, что не хочет видеть жанр ужасов – на постобработке можно выкинуть оставшиеся ужасы, даже если модель их выбрала.
* Понять, что постобработка – это немного «ручное управление» поверх модели, и злоупотреблять им не стоит. Но в реальных системах он почти всегда есть.
* Interview insight: Вас могут спросить: «Как бы вы внедрили диверсификацию в список рекомендаций?». Лучше сказать, что есть алгоритмические способы (оптимизация с regularizer на diversity) или эвристики (не больше N из одного класса, перетасовка). И что важно не сильно уронить релевантность при этом – нужен баланс.
### Рекомендуемые материалы:
* Habr: «Воронка метрик реком. систем: дочитывания» – там описано, как на верхнем уровне метрики (показы, клики, дочитывания) влияют. В контексте постобработки – понимаем, что нужна баланска между кликабельностью (можно повысить популярным контентом) и удовлетворённостью (выше, если контент персональный и разнообразный).
* Статья «Personalization vs. Popularity» – исследование о влиянии разнообразия рекомендаций (англ.).
* RecSys community blog: «Diversification in Recommender Systems» – обзор академических методов диверсификации (если интересно, про метрики типа ILD – intra-list diversity).
* Метрика Personalization в статье OTUS – как измеряют, насколько рекомендации для разных пользователей различаются. Хорошо иллюстрирует проблему filter bubble.
* Пример бизнес-правил: пост на Medium «Rules in recommender systems» – о том, как Product-менеджеры часто просят «показывать X не более Y раз», и как это учитывают.
### Практика:
* Реализуйте диверсификацию для своего списка: измерьте сходство всех пар элементов в топ-10 рекомендаций пользователя (например, по жанрам или по эмбеддингам). Попробуйте переставить элементы, чтобы среднее сходство уменьшилось (например, не ставьте очень похожие подряд). Можете использовать жадный алгоритм: берём самый релевантный, потом ищем следующий из оставшихся, который минимизирует сходство с уже выбранными, и т.д. Сравните метрику intra-list similarity до и после.
* Coverage check: Возьмите топ-10 рекомендаций для 100 пользователей. Посчитайте, сколько уникальных товаров фигурирует. Теперь предположите, что вы добавили правило: 10% рекомендаций – случайные мало-популярные товары. Смоделируйте это и снова посчитайте coverage. Улучшилось ли? (Скорее всего, да, но ценой релевантности). Подумайте, насколько coverage важен для вашего продукта.
* Попробуйте вписать новизну: создайте признак «товар новый (меньше X дней)» и при прочих равных поднимайте новые товары на несколько позиций вверх. Например, если товар новый и у него позиция 8, сделайте его 5-м. Проследите, чтобы не испортить сильно другие рекомендации. Это вручную, но вы увидите, как вводятся бизнес-требования.
* Если есть возможность, объедините рекомендации нескольких разных моделей: например, у вас есть модель для общей популярности и модель персонализированная. Смешайте их выдачу: 70% от персональной, 30% – популярное (но не просмотренное этим юзером). Посмотрите, станет ли список более разнообразным. Заодно поймёте, как на практике иногда поступают (ensemble нескольких рекомендательных алгоритмов).
* Контроль результатов: придумайте 2–3 правила, которые ваш финальный список должен соблюдать (например: не больше 2 товаров из одной категории, хотя бы 1 новинка, не показывать контент 18+ несовершеннолетним и т.д.). Проверьте ваши рекомендации на соответствие этим правилам. Это хорошая привычка – валидировать, что система не генерирует недопустимых выдач.

## День 11: Метрики качества рекомендаций (оффлайн)
### Темы и задачи:
* Изучить основные метрики ранжирования, используемые для оценки качества рекомендательных алгоритмов на исторических данных (оффлайн):
  * Precision@K – доля рекомендованных элементов в топ-K, которые являются «релевантными». В рекомендациях обычно «релевантность» определяют как факт взаимодействия (например, фильм был просмотрен или оценён пользователем) в тестовом периоде. Precision@K показывает точность: из того, что мы порекомендовали, сколько действительно понравилось пользователю.
  * Recall@K – доля от всех релевантных для пользователя элементов, которые оказались в рекомендациях. Показывает покрытие интересов пользователя. В связке с precision: если precision низкий, значит много нерелевантного посоветовали; если recall низкий – значит упустили что-то, что пользователь бы хотел. Обычно важно Recall@K для рекоммендеров (пользователь найдет хотя бы что-то полезное).
  * MRR (Mean Reciprocal Rank) – мера ранжирования, чувствительная к тому, на какой позиции находится первый релевантный элемент. Высокий MRR означает, что система выдает хотя бы один хороший рекомендация очень высоко (например, первое же – понравилось).
  * MAP@K (Mean Average Precision) – усреднённая точность на разных уровнях отсечения, усреднённая по пользователям. Интуитивно: учитывает и precision, и позицию релевантных в списке. Хорошо подходит, когда важно всю десятку сделать релевантной.
  * nDCG (Normalized Discounted Cumulative Gain) – учитывает позиции и релевантности элементов (если есть градуированная релевантность, например, рейтинг 5 – очень понравилось, 4 – понравилось и т.д.). nDCG@K нормализован относительно идеального ранжирования. Широко используется в академических работах и поиске.
* Понять, как готовится «тестовый набор» для вычисления этих метрик: обычно данные делят по времени (train – до определенной даты, test – взаимодействия после этой даты). Для каждого пользователя, имеющего тестовые интеракции, генерируют рекомендации по модели, и сверяют с его фактическими действиями.
* Обратить внимание: метрики top-K чувствительны к популярности. Например, рекомендатель, предлагающий всем самые популярные товары, может показать неплохой Recall@10 (потому что большинство и так взаимодействуют с хитами), но при этом персонализация будет слабой. Поэтому иногда смотрят и дополнительные метрики: Coverage (скольким из доступных товаров вообще кто-то был рекомендован хотя бы раз), Personalization (разнообразие рекомендаций между пользователями), Novelty (например, средняя популярность рекомендованных – ниже лучше, значит система не только попсовое советует).
* Разобрать, что такое доверительные интервалы для метрик: так как у нас ограниченное число пользователей и рекомендаций, оценка метрики – случайная величина. Можно применять бутстрэп, чтобы получить интервал, где с 95% вероятностью лежит истинное значение метрики. Например: «Precision@5 = 0.25 ± 0.02». Если два алгоритма дают 0.25 ±0.02 и 0.27 ±0.02, интервалы пересекаются – разница не статистически значима. Это важно для выводов.
* Освоить интерпретацию: например, Precision@5 = 20% значит «в среднем 1 из 5 рекомендаций пользователь действительно оценил положительно». Recall@10 = 50% – «половину того, что пользователь положительно оценил, мы угадали в топ-10». Для бизнеса recall важен, т.к. упущенные релевантные товары – упущенная выгода. Precision важен для UX – чтоб не раздражать мусорными рекомендациями.
* RMSE/MAE – упомянуть, что в ранних работах (Netflix Prize) оптимизировали RMSE предсказания рейтингов. Сейчас для топ-N рекомендации эти метрики не так полезны, потому что нам важен порядок, а не точное предсказание чисел. Однако, если задача – предсказать рейтинг/оценку, то RMSE всё ещё применяется.
* Пример вопроса: «Почему мы не используем Accuracy для оценки рекоммендера?». Ответ: определение «точности» (Accuracy) затруднено – у нас много больше негативов (неинтересных товаров) и если считать все ненаблюдаемые как отрицание, точность будет доминироваться тривиальными негативами. Поэтому используют precision/recall@K по top-K спискам, а не глобальную accuracy.
### Рекомендуемые материалы:
* Habr: «Метрики оценки для рекомендательных систем» – описание MAP@K, MAR@K (mean average recall), coverage, personalization с иллюстрациями.
* Weaviate Blog: «Evaluation Metrics for Search and RecSys» – понятные определения Precision@K, Recall@K, MRR, MAP, nDCG.
* Fang et al.: «Recommender System Evaluation Metrics» – серия статей (Part 1,2…) с примерами.
* Конспект IntSys: «Оффлайн-оценивание рекомендаций» – должна охватывать эти метрики (если доступен).
* Статистический basics: «Как оценить качество A/B-теста» (Habr) – там раздел про доверительные интервалы, бутстрэп, значимость.
### Практика:
* Возьмите свою модель ранжирования (или алгоритм) и проведите оффлайн-оценку: разделите данные по времени: всё до, скажем, 1 ноября – это train, а ноябрь – test. Для каждого активного в тесте пользователя сформируйте рекомендации (без учёта его ноябрьских действий, конечно) и измерьте Precision@5, Recall@5, Recall@10. Реализуйте эти метрики сами, это полезно – убедиться, что понимаете их расчёт.
* Посчитайте средний ранг первого релевантного и MRR. Например, для каждого пользователя определите позицию первого фильма из его тестовых просмотренных в рекомендованном списке. Усредните Reciprocal Rank (1/позиция). Посмотрите, насколько модель быстро «угадывает» предпочтение.
* Постройте Precision-Recall кривую: варьируя K от 1 до 20, посчитайте Precision@K и Recall@K, нанесите на график. Это покажет компромисс: с ростом K recall растёт, precision падает. Выберите точку (например, K=10), которая кажется хорошим балансом.
* Измерьте Catalog coverage: сколько разных товаров появилось в всех рекомендациях для тестовых пользователей. Сравните с общим числом товаров, с которыми были взаимодействия в тестовом периоде. Если coverage << 100%, подумайте, не слишком ли модель концентрируется на узком наборе.
* Если у вас есть несколько алгоритмов (например, ALS из дня 3 и LightGCN из дня 4), сравните их метрики на одних и тех же тестовых данных. Используйте бутстрэп (перемешивая пользователей) для оценки доверительного интервала разницы. Определите, статистически значимо ли один лучше другого. Это хорошая практика для offline experiment.
* A/B offline симуляция: иногда делают следующее – берут исторические A/B (например, одна группа видела алгоритм A, другая B) и сравнивают их оффлайн метрики. Если есть такие данные, можно попробовать. Если нет – ничего, но понимание, что оффлайн оценки не гарантируют онлайн успех, важно.
* Ответьте письменно: «Какая метрика важнее – Precision@K или Recall@K – для [ваш домен]?». Например, для Netflix, где пользователь может смотреть много, наверное, recall@10 важнее (главное что-то найти интересное). А для интернет-магазина, где редко покупают – precision@5 важнее (не отвлекать лишним). Это поможет вам формулировать метрики успеха на интервью.

## День 12: A/B-тесты и онлайн-метрики
### Темы и задачи:
* Понять, что итоговая проверка качества рекомендательной системы – онлайн: реальные пользователи взаимодействуют с рекомендациями. A/B-тестирование – золотой стандарт для измерения влияния изменений. Нужно уметь спроектировать и проанализировать A/B-тест.
* Компоненты A/B:
  * Разбиение пользователей на группы (случайно, желательно равномерно и независимо).
  * Группа А (контроль) получает старую версию рекомендаций, группа B (тест) – новую (например, новый алгоритм).
  * Период эксперимента – достаточно длинный, чтобы набрать статистику (несколько недель, учитывая сезонность).
  * Метрики онлайн: CTR (click-through rate) – доля просмотренных рекомендаций, конверсия (например, покупки), время, проведенное на сайте, доход на пользователя. Выбирают целевую метрику – например, увеличение CTR на +x% или удержания.
  * По окончании – сравнение метрик групп, статистический тест (например, z-test или t-test, бутстрэп) на значимость разницы. Вывод – значимо лучше/хуже или «без разницы».
* Обратить внимание на значимость: обычно берем уровень 95%. Если p-value < 0.05, считаем изменение значимым. Также смотрим доверительный интервал разницы (например, улучшение CTR = +2% ± 1%, значит изменение положительное). Если интервал захватывает 0, результат неубедителен.
* Обсудить Pitfalls A/B-тестов:
  * SRM (Sample Ratio Mismatch) – если группы получились неравные или разные по характеристикам (плохая рандомизация). Надо проверять метрики до влияния (например, количество просмотренных страниц в контроль/тест – должно быть близко).
  * Новизна и обученность пользователя – иногда пользователи реагируют на новое UI алгоритм по-другому первые дни. Потому тесты должны длиться достаточно, чтобы эффект стабилизировался.
  * Перетекание – если пользователь может попасть и в контроль, и в тест (например, через разные устройства), это портит эксперимент. Нужно разделять чётко по пользователям (а иногда по кластерам, если могут влиять друг на друга).
  * Метрика vs цель – иногда улучшение одной метрики (например, кликов) может ухудшить другую (например, длительность сессии). Надо выбирать метрики осознанно и наблюдать за вспомогательными показателями (не вырос ли bounce rate?).
* Онлайн-метрики специфичные: Retention (удержание – возвращаемость пользователей), Engagement (число действий за сессию), Conversion (покупка или подписка). Рекомендательная система должна в итоге влиять на бизнес-метрики: увеличить retention, ARPU и т.д., а не только клики. Поэтому в крупных системах A/B-тест измеряет много показателей и анализ комплексный.
* Кроме классического A/B существуют и многорукие бандиты (о них завтра), но знать AB обязателельно.
* Знать понятия: power of test (мощность – вероятность обнаружить эффект заданного размера), минимально детектируемый эффект (MDE) – если хотите заметить +1% к CTR при 95% значимости, сколько нужно пользователей/сессий? Уметь прикинуть формулу или использовать онлайн-калькуляторы (например, mindbox).
* Interview: могут дать кейс: «Вот у вас новый алгоритм дал +5% оффлайн precision. Будете ли вы раскатывать?». Ваш ответ: «Нужно провести A/B-тест, т.к. оффлайн – прокси. Определим метрику (например, CTR), проведем 2-недельный тест на 10% аудитории, посмотрим значимость...».
### Рекомендуемые материалы:
* Статья hh.ru на Хабре: «Как оценить качество системы A/B-тестирования» – много про статистику, но начало про шаги A/B очень полезно.
* Книга «Trustworthy Online Controlled Experiments» (Kohavi et al.) – настольная книга по A/B-тестам, хотя бы просмотрите оглавление и главу про pitfalls.
* Блог GoPractice: «Дизайн A/B-тестов» – на русском, о практической стороне (как формулировать гипотезу, какие параметры выбирать).
* Mindbox калькулятор A/B – попробуйте его, поймёте связь выборки, конверсий и интервала.
* Видео Евгения Медведникова «A/B-тесты: ошибки и заблуждения» – на YouTube, рассказано, как неправильно интерпретируют иногда.
### Практика:
* Допустим, у вас уже был запущен старый алгоритм и логились клики по рекомендациям. Разделите этих пользователей случайно (e.g., случайным битом от user_id) на две группы и симулируйте A/B: предположите, что группа B получила ваш новый алгоритм. Смоделируйте, что новый алгоритм, скажем, поднял CTR на 10% для активных пользователей. Внесите эти изменения для группы B в данных (искусственно повысив их клики на 10%). Теперь примените t-test для долей или бутстрэп – получаете ли вы p-value < 0.05? Это упражнение для понимания статистики: генерируйте случайные биномиальные данные и проверьте значимость.
* Посчитайте минимальный размер выборки для выявления эффекта. Например, текущий CTR ~ 5%. Хотим заметить относительное изменение +5% (то есть будет 5.25%). При alpha=0.05, beta=0.2. Можно воспользоваться известными формулами или онлайн-калькулятором (mindbox). Полученное число пользователей или просмотров – каково оно? Достаточно ли у вас трафика, чтобы тест длился разумное время? Это учит оценивать реализуемость экспериментов
* Составьте план гипотетического A/B-теста для внедрения вашей новой системы: Опишите, какая метрика основная (например, CTR увеличится), какие guardrail-метрики (например, не просел ли average watch time), длительность (например, 2 недели, чтобы захватить два выходаных), процент аудитории (например, 50/50 или 30% тест). Это полезно – систематизировать мысли, на собеседовании покажет ваш структурный подход.
* Проанализируйте риски: Что если в тесте новая реком. система покажет лучшие клики, но вы заметите, что пользователи стали меньше времени проводить на платформе? Как интерпретировать? (Возможно, кликают на менее релевантные вещи и быстро уходят – значит, улучшение некачественное). Продумайте такие сценарии, чтобы быть готовым отвечать на тонкие вопросы.
* Ознакомьтесь с библиотекой PyMC3/PyMC или Ax (Facebook) – они позволяют реализовать байесовский подход к A/B (определение распределения конверсий, вероятности улучшения). Не обязательно глубоко, но знать, что существует Bayesian AB тестинг – плюс. Попробуйте, если интересно, проанализировать свои симулированные данные байесовским способом и получить «постериорное распределение разницы».

## День 13: Explore-Exploit дилемма и многорукие бандиты
### Темы и задачи:
* Понять проблему Exploration vs Exploitation: если всегда рекомендуем то, что наиболее вероятно понравится (exploitation), мы можем упустить новые интересы пользователя или недооценить новые товары. Exploration – намеренное пробование новых или менее уверенных вариантов, чтобы собрать данные. Нужно балансировать: слишком много нового – пользователь недоволен (слишком «рандом»), слишком мало – алгоритм может попасть в локальный оптимум и не узнать про скрытые интересы.
* Изучить классический пример Multi-Armed Bandit (многорукий бандит): у вас несколько вариантов (рук), с неизвестной наградой, надо постепенно узнать, какая лучше, при этом максимизируя выигрыш. В рекомендациях «руки» – это варианты алгоритмов или варианты конкретных рекомендаций.
* Простые стратегии бандитов:
  * Epsilon-Greedy: с вероятностью ε (скажем 5%) выбираем случайную рекомендацию (explore), остальное время – лучшую по алгоритму (exploit).
  * UCB (Upper Confidence Bound): формула, которая выбирает вариант с максимальным (средняя награда + коефф * доверительный интервал). То есть, поначалу склоняется к малоисследованным вариантам (у них интервал большой), а со временем – к действительно хорошим.
  * Thompson Sampling: байесовский подход – считаем распределение вероятности успеха для каждой руки, сэмплируем из них, выбираем руку с наибольшим сэмплом. Постепенно распределения концентрируются, и выбирается лучший, но с учётом неопределённости.
* Применение: Например, у вас есть два рекомендательных алгоритма – можно не сразу всем раскатывать лучший, а запустить бандит, который будет постепенно менять трафик на основе того, кто показывает лучший CTR. Бандит заменяет А/B-тест в случаях, когда нужно не столько оценить, сколько сразу оптимизировать.
* В контенте: новостные ленты, рекомендатели статей часто используют бандиты, чтобы с помощью небольшого процента трафика понять, какие новые статьи потенциально «выстрелят» (exploration), и быстро увеличить их показ, если видят успех (exploit).
* Contextual Bandits: ситуация, когда есть контекст (например, фичи пользователя или контента) – бандит может принимать решение, глядя на них (пример – модель LinUCB или нейросетевые бандиты). Это ближе к реальным рекомендациям, где решение зависит от пользователя. Современные подходы (Reinforcement Learning) к рекомендациям иногда формулируются как contextual bandit problem.
* Обратить внимание: бандиты оптимизируют короткосрочную метрику (клик), могут игнорировать долгосрочное удовлетворение. Иногда нужно более сложное – Reinforcement Learning с состоянием (учитывать последовательность рекомендаций, диверсификацию опыта). Но RL очень сложен в продакшене, поэтому чаще используют бандиты для определённых компонентов.
* Bias/feedback loops: без exploration, рекомендация может зациклиться на одном: пользователь смотрит то, что мы рекомендовали => алгоритм получает подтверждение и ещё больше рекомендует похожее. Это feedback loop. Бандиты (и просто заложенная доля рандома) разрывают цикл, позволяя «проветривать» рекомендации.
* Изучить кейс Авито: они применяли бандиты для оптимизации похожих объявлений конфигов, увидели рост просмотров, но… обнаружили, что увеличились пустые просмотры без контактов. То есть, бандит повысил одну метрику, но в ущерб другой (некачественные показы). Вывод: нужно аккуратно выбирать целевую функцию бандита (может, лучше оптимизировать конверсии, а не просто клики).
* Interview: могут спросить, «Как бы вы решили проблему новго контента, который алгоритм не показывает?». Можно упомянуть: «добавил бы ε-greedy exploration – небольшую вероятность показа неизвестных товаров, собирал бы фидбэк. Либо использовал бы multi-armed bandit для новых vs старых, чтобы автоматически регулировать долю показов новых товаров».
### Рекомендуемые материалы:
* Habr: «Многорукие бандиты в рекомендациях (Avito)» – подробно с примерами, как на Авито улучшали похожие объявления бандитами, описаны стратегии и результаты.
* Доклад HighLoad++ «Многорукие бандиты в Avito» (Михаил Каменщиков) – видео, дублирует статью, но может лучше усвоится визуально.
* Статья на Хабр «Контекстные многорукие бандиты для рекомендации контента» – введение в epsilon-greedy, UCB, Thompson Sampling с кодом (хорошо для практики).
* Shaped.ai blog: «Exploration vs Exploitation in RecSys» – рассказывает про предупреждение DeepMind о баиасах и feedback loops.
(Для интересующихся) * Книга «Bandit Algorithms for Website Optimization» – примеры и теория (англ.).
### Практика:
* Смоделируйте простую ситуацию: есть 3 алгоритма рекомендации с неизвестным истинным CTR (скажем, 5%, 4%, 6%). Пользовательские сессии генерируйте случайно, где вероятность клика определяется этими CTR. Теперь реализуйте epsilon-greedy бандит: начните с равных вероятностей показывать любой алгоритм, постепенно обновляйте свои оценки CTR и уменьшайте ε. Прогоните симуляцию на 10000 шагов. Удалось ли вашему бандиту приблизиться к оптимальному алгоритму (6%)? Сколько «потерь» (кликов упущено) было во время обучения? Это даст вам почувствовать механику.
* Реализуйте UCB1 алгоритм на той же симуляции. Сравните скорость сходимости. Нарисуйте график: доля выбранного лучшего алгоритма по мере шагов. UCB обычно быстрее находит лучший.
* Попробуйте Thompson Sampling (для биномиальной награды). Нужно обновлять Beta-распределения для каждого алгоритма. Просимулируйте – TS обычно работает очень хорошо.
* Теперь, предположим, эти 3 алгоритма – просто разные настроенные рекомендатели. А можно бандит применить и к элементам: например, у вас 100 новых товаров и вы хотите выяснить, какие из них «зайдут». Смоделируйте 100 рук с разными вероятностями клика, примените ε-greedy или TS. Посмотрите, в топ-5 самых выбранных рук попали ли действительно топ-5 по истинной CTR. Это имитирует отбор лучших из новых товаров.
* Настройте небольшое контекстуальное эксперимент: пусть у пользователей есть признак – например, «любитель жанра A или B». А алгоритма два – один лучше для A, другой для B. Реализуйте LinUCB: для каждого алгоритма ведите модель (линейную) на контекст и обновляйте ее. Проверьте, сможет ли LinUCB научиться выбирать алгоритм по предпочтению пользователя. Это сложнее, но даёт понимание контекстных бандитов.
* Подумайте, где бы вы применили бандит в вашей системе: может, для рекомендаций новинок, может, для выбора между коллаборативной и контентной моделью на лету (когда непонятно, что для данного пользователя лучше). Сформулируйте 1-2 таких кейса, это пригодится для интервью. Например: «Если пользователь новый – используем бандит между разными стратегиями: популярное, профиль демографический, похожее на последние просмотры, и быстро определяем, что работает».
* В качестве закрепления: напишите короткое описание (псевдокод) алгоритма Thompson Sampling и поясните, почему он автоматически уравновешивает exploration/exploitation – это поможет осмысленно рассказать, если спросят.

## День 14: Сместимость и справедливость (Bias & Fairness) в рекомендациях
### Темы и задачи:
* Понять основные виды bias (смещений) в данных рекомендаций:
  * Popularity bias: популярные товары получают непропорционально много внимания. Алгоритм может переоценивать их релевантность, потому что у них больше взаимодействий. В итоге новые или нишевые товары почти не показываются (богатые богатеют).
  * Selection bias: данные о предпочтениях неполные – мы видим только что пользователь выбрал, а что проигнорировал – не явно. Например, 5-звёздочные рейтинги ставят реже, чем смотрят – это смещение.
  * Position bias: (более актуально в поиске) – пользователь кликает топ-1 часто просто из-за позиции. В рекомендациях тоже – первые слоты получают больше кликов независимо от качества. Это нужно учитывать при обучении (например, через пропуски или нормализацию по позиции).
  * Feedback loop: уже упоминали – если алгоритм рекомендует ограниченный набор, пользователь только с ним и взаимодействует, данные становятся все более узкими, алгоритм ещё сильнее сужает – получаем filter bubble.
* Fairness (справедливость): в рекоммендерах обычно говорят о двух аспектах:
  * Справедливость по отношению к пользователям: чтобы разные группы пользователей получали качественные рекомендации. Например, не дискриминировать по каким-то признакам – скажем, рекомендательная система вакансий должна одинаково хорошо предлагать вакансии женщинам и мужчинам (есть известная проблема, что ML мог ущемлять некоторым группам, предлагая меньше возможностей).
  * Справедливость по отношению к товарам/продавцам: чтобы даже менее популярные или новые артисты имели шанс быть рекомендованными. Иначе, платформа может быть обвинена в том, что даёт трафик только топ-брендам.
* Метрики fairness:
  * Disparate Impact / Treatment: сравнить метрики (CTR, конверсия) между группами пользователей (например, по полу, возрасту). Если сильно различается – может быть сигналом дискриминации/смещения.
  * Calibration: мера того, насколько список рекомендаций соответствует истинным пропорциям интересов пользователя. Например, пользователь слушает 30% джаза, 70% рока, а рекоммендер даёт ему 100% рок – это несбалансировано. Калиброванный рекоммендер дал бы близко к 30/70.
  * Coverage (со стороны каталога): сколько % товаров хотя бы кому-то рекомендовано (мы уже считали). Низкое coverage – значит, многие товары никогда не получат экспозицию, несправедливо для их создателей.
  * Novelty / Serendipity: какие-то метрики, оценивающие, насколько рекомендации не тривиальны (для пользователя это тоже fairness: даём шанс «удивить»).
* Bias mitigation:
  * Корректировать данные: например, при обучении по implicit-фидбэку давать меньший вес популярным item, добавить искусственные негативы для популярных, чтобы модель не считала отсутствие прослушивания популярной песни таким же «негативом», как отсутствие взаимодействия с редкой.
  * Пост-обработка: вводить диверсификацию, квоты на новые товары.
  * Контролировать на онлайн этапе: например, «если пользователь за последние 10 сессий ни разу не получил ничего нового, подмешать новинку».
* Bias case: В Facebook обнаружилось, что пользователи, которые мало контента лайкают, постепенно получают все меньше контента (алгоритм думает, что им ничего не интересно) – самосбывающееся упущение. Решение: иногда всё же подбрасывать разные посты, смотреть реакцию – возможно, интерес есть, просто раньше не показывали.
* Обратить внимание на этические аспекты: рекоммендеры могут усиливать filter bubble (человек видит только согласующиеся с его мнением новости -> поляризация). Или на YouTube был эффект «rabbit hole» – рекомендации уводили всё к более экстремальному контенту, т.к. он больше удерживает внимание. Компании сейчас стараются смягчить такие эффекты, вводя ограничения, ручные правила. Важно показать, что вы осведомлены об этой ответственности.
* Interview: Вопрос может звучать: «Какие проблемы могут возникнуть при использовании только данных кликов для обучения рекомендации?». Ожидается упоминание feedback loop, popularity bias и т.д., и предложения: «вводить exploration, корретировать вес популярных, собирать явный фидбэк».
### Рекомендуемые материалы:
* Simulative.ru: «Рекомендательные системы: как ML-инженеры создают... (часть про метрики)» – там упоминается про fairness, качество работы системы
* Курс Coursera «AI Ethics» – содержит модуль о bias, можно применить к рекоммендерам (англ.).
* Medium: «When you hear Filter Bubble…» – разбирает мифы и факты о filter bubble, интересная точка зрения.
* Артикул «Filter Bubbles in RS: Fact or Fallacy» (возможно сложноват, но хотя бы прочесть выводы).
* Habr «Дропаем ранжирующие метрики…» – там инженер делится опытом, как они отслеживали результаты экспериментов, есть про логирование метрик – опосредованно связано.
* Доклад RecSys «Fairness in Recommender Systems» – академический, для широкого взгляда (если время останется).
### Практика:
* Анализ popularity bias: посчитайте долю рекомендаций, приходящихся на топ-10% самых популярных товаров. Например, среди 1000 рекомендаций 800 уникальных пользователей, посмотрите сколько из них – это популярные фильмы. Если обнаружите сильный дисбаланс, попробуйте скорректировать: уберите из каждого списка топ-1 самый популярный фильм (если он там есть) и замените на следующий. Посмотрите, как изменится метрика (Precision или Recall). Чуть упадёт? Но coverage вырастет. Это имитация борьбы с popularity bias.
* Calibration: для 5 пользователей возьмите их профиль жанров (скажем, % просмотра комедий, боевиков…). Для их рекомендаций (топ-10) определите тоже % жанров. Сравните. Вероятно, рекомендации сдвинуты к более популярным жанрам. Попробуйте откалибровать: искусственно понизьте скор для жанра, который переизбыточен, и переранжируйте. Проверьте, стали ли пропорции ближе.
* Fairness по пользователям: если у вас есть атрибуты пользователей, попробуйте разбить метрики по группам. Например, мужчины vs женщины: у кого выше средний Precision? Если различие >5-10%, задумайтесь, не связано ли с bias в данных (например, женской аудитории меньше, модель хуже обучилась для них). Конечно, на учебных данных это скорее шума ради, но процесс анализа важен.
* Serendipity check: Посмотрите для нескольких пользователей – есть ли в рекомендациях что-то, чего у них не было в истории? Если нет, значит система лишь повторяет прошлое (может быть ок, но часто хотят немного нового). Подумайте, какие 1-2 вещи можно сделать, чтобы повысить новизну: «добавим 1 неожиданный элемент, схожий только отдаленно» или «немного повысим вес новых категорий».
* Почитайте ту самую статью про Авито бандитов – в конце описан интересный кейс: бандиты увеличили просмотры, но контакты (цель) упали. Подумайте, почему так могло выйти. Напишите свои выводы: возможно, бандит нашёл способ увеличить кликабельность показывая более общие результаты (без фильтра по региону), люди больше кликали но меньше находили что им надо. Отсюда урок: метрика не та. Это отличный пример для интервью о важности правильной цели оптимизации.
* Сформулируйте политику fairness для своего рекомендателя: например, «Будем гарантировать, что новые товары получают минимум 5% показов», или «Будем следить, чтобы гендер/возраст не влияли на вероятность увидеть определённый контент». Представьте, что вас спросили: «Как убедиться, что наш рекоммендер не дискриминирует некоторых продавцов?». Ваш ответ: «Будем отслеживать coverage по продавцам, возможно, вводить штраф в ранжировании, если один продавец слишком доминирует, и внедрим explore-мехнизм для новых продавцов». Запишите такой ответ для себя.

## День 15: Архитектура продакшн-системы рекомендаций
### Темы и задачи:
* Свести всё вместе и понять, как выглядит архитектура рекомендационной системы в продакшене на высоком уровне. Основные компоненты:
  * Data pipeline (оффлайн): сбор сырых данных (логи кликов, просмотров, покупок) -> хранение (data lake, warehouse) -> обработка фич (batch jobs, Spark) -> обучение моделей (Spark/TensorFlow/PyTorch) -> сохранение моделей и артефактов (эмбеддинги, индексы).
  * Feature Store: центральное хранилище рассчитанных признаков (как статичных, так и обновляющихся). Должно иметь интерфейс и для оффлайн (обучение), и для онлайн (сервису выдачи). Например, Feast, Hopsworks, или внутренняя реализация.
  * Candidate generation service (онлайн): сервис/микросервис, который по запросу (user_id) быстро формирует кандидатов. Например, сохраняет индексы эмбеддингов в памяти или использует внешнюю базу (Faiss, Milvus, Elasticsearch). Может состоять из нескольких под-сервисов (collaborative retrieval, content retrieval, etc.).
  * Ranking service (онлайн): принимает кандидатов + собирает фичи (из feature store, в реальном времени, как просмотры сессии) -> применяет модель ML (может быть встроенный LightGBM, или вызов TensorFlow Serving/ONNX) -> возвращает отсортированный список.
  * Post-processing & Business rules: модуль, который накладывает финальные ограничения (например, diversity, вставка определённых элементов). Может быть часть ранкера или отдельно.
  * Delivery/API: собственно, возвращает рекомендации в приложение/веб. Должен быть очень быстрым (обычно всё онлайн на всю цепочку – десятки миллисекунд). Часто используют кэширование результатов для неактивных пользователей или common queries.
* Обсудить Latency (задержка): допустимый бюджет времени на выдачу рекомендаций – например, <50ms. Если глубокая модель (BERT) не вписывается, её не используем онлайн – либо применяем только для оффлайн оценки. Способы снижения латентности: предварительное вычисление (precompute) – например, топ-200 кандидатов для каждого пользователя ежедневно складывать в кеш (как Авито делал), использовать Approximate методы (ANN вместо точного), параллелизация (несколько источников кандидатов параллельно собираются, потом мердж).
* Scalability (масштабирование): система должна выдерживать рост пользователей/items. Горизонтально масштабируют сервисы (несколько инстансов candidate service за балансером). Хранят индексы распределённо (sharding по пользователям или товарам). Используют асинхронные очереди для обновления моделей (чтобы new data -> re-train -> swap model).
* Обновление моделей и индексов:
  * Модельная деградация (drift): со временем поведение меняется, модель устаревает. Нужно переобучать регулярно (еженедельно, ежедневно). Иметь конвейер MLops: обучение -> валидация -> деплой новой модели (A/B-тест, shadow deploy).
  * Обновление индекса кандидатов: если используется ANN для эмбеддингов, при появлении новых товаров нужно их добавить. Можно реализовать shadow index: построить новый индекс на фоне и переключить атомарно (или поддерживать динамические структуры, некоторые ANN библиотеки позволяют добавлять).
  * Cold start: для новых пользователей – возможно, отдельный путь (хранить популярные или опросник для них). Для новых товаров – использовать контентные фичи, пока нет исторических – поэтому pipeline должен уметь: если item холодный, кандидат-сервис извлекает его по схожести контента или помещает в explore.
* Monitoring: в продакшене важно наблюдать: согласованы ли оффлайн и онлайн фичи (нет ли рассинхрона), какова latency (95-й перцентиль), метрики качества (CTR ежедневно, распределение оценок). Настраивают алерты, дашборды.
* Logging/Feedback loop: собираются пользовательские реакции на выданные рекомендации, логируются с метками (какой алгоритм, какая конфигурация), чтобы потом использовать для обучения следующей модели (то есть система самосовершенствуется).
* Пример: Netflix в TechBlog 2013 описывал свою архитектуру: offline batch генерация, online blending. Сейчас еще сложнее. В интервью достаточно знать компоненты и их взаимодействие. Главное – показать понимание, что offline часть (данные, обучение) и online (сервисы выдачи) – связаны, но разделены для эффективности.
* Interview tasks: могут попросить набросать диаграмму или перечислить компоненты системы рекомендаций для условного сервиса (например, «Spotify рекомендации»). Упомяните: data pipeline, модельные сервисы, latency, monitoring. Также часто: «Как бы вы справились с ростом числа пользователей x10?». Ответ: «Закэшировать больше, распараллелить retrieval, использовать более простую модель на ранжировании (или двухуровневую), шардировать по userID, возможно, при такой нагрузке перейти с Python на C++ для критичных частей».
### Рекомендуемые материалы:
* Netflix TechBlog (2013): «System Architectures for Personalization» – хоть старый, но классический обзор архитектуры Netflix.
* The ML Architect (Part 1) – совмещает всё: многослойный pipeline, оффлайн vs онлайн, примеры из практики. Обязательно прочесть выводы.
* NVIDIA Merlin Docs: Best Practices – про GPU-рекоммендеры, но там есть диаграммы архитектур (например, как составные модели работают, feature store).
* HighLoad++ доклад «Архитектура рекомендаций в Одноклассниках» – на YouTube, на русском, про то как делается реально.
* Habr «Как мы строили рекомендатель для Х» – возможно, найдете историю реализации (например, статья про Кинопоиск или Я.Музыку).
* (Обязательно) Список вопросов по System Design – найдите любой ресурс, где перечислены аспекты (scalability, consistency, etc.) и убедитесь, что все релевантные упомянуты в вашем понимании.
### Практика:
* Нарисуйте схему (можно от руки или диаграмму): блоки Offline ETL -> Model Training -> Candidate Index -> Online Serving. Отметьте места: где хранятся данные, где кэшируются результаты, куда приходит запрос от пользователя, как он проходит через систему и что возвращается. Эта схема – ваш ориентир, пригодится при ответе на открытые вопросы.
* Опишите последовательность действий при запросе рекомендаций от пользователя X: (1) сервис Y вытаскивает топ-N кандидатов из хранилища Z, (2) для каждого запрашивает фичи из feature store Q, (3) применяет модель M (в памяти или через RPC) и (4) результаты направляются в компонент D для пост-обработки и выдачи. Чем детальнее представите, тем легче будет объяснить.
* Проверьте узкие места: например, если одновременно 100k пользователей запросят рекомендации – где может быть bottleneck? Может, обращение к feature store – оно должно быть высокопроизводительным (мемоизация, prejoin фичей с кандидатами). Или inference модели – нужно, чтобы модель была либо лёгкой, либо можно батчить запросы. Подумайте о каждом блоке, как его масштабировать (поднять несколько инстансов, распределить данные).
* Напишите план развертывания новой модели: например, обучили LightGBM новую – как катить? Shadow-mode: отдавать новую модель параллельно старой, но не показывать, а только логировать её рекомендации для сравнения. A/B-тест: 5% пользователей на новую. Rollback plan, если метрики упадут. Это показывает зрелость подхода.
* Представьте вопрос: «Как обеспечить свежесть рекомендаций, если пользователь только что что-то купил?». Ваш pipeline может обновлять данные раз в сутки, а нужно сразу исключить купленное. Решение: «Хранить онлайн-черный список на пользовательской сессии, либо иметь возможность быстрым запросом фильтровать кандидатов по недавним событиям (например, перед ранжированием убрать товары, купленные <1 дня назад)». Или иметь streaming обновление: событие покупки триггерит обновление фичи 'recent_purchased'. Сформулируйте такой механизм.
* Реализуйте упрощенный сервер: напишите функцию get_recommendations(user_id) в стиле продакшна: обращается к заранее созданным структурам (напр., словарю user->top_candidates, dict с фичами пользователей, dict с эмбеддингами товаров, модель загружена). Пусть она симулирует задержки (например, time.sleep(0.001) в местах). Теперь замерьте время ответа. Представьте, что будет при 1000 RPS (requests per second). Это конечно упрощение, но даст понять важность оптимизации.
* Задумайтесь про хранилище кандидатов: возможно, стоит предвычислить матрицу «user -> топ100 товаров» и держать в Redis. Тогда online только берёт список и ранжирует. Напишите, какие плюсы (мгновенная выдача) и минусы (меньше персонализации на текущий момент, задержка обновления). Решают иногда так: *«предвычислим для активных юзеров, а для неактивных – онлайн полную генерацию».
* Вспомните все пройденные дни и убедитесь, что вы можете вписать каждый компонент в общую картину: алгоритмы (Days 2-5) – это часть кандидатов или модели, метрики (Day 11) – для офлайн оценки, bandits (Day 13) – могут реализовываться как часть рантайм системы (exploration module), fairness (Day 14) – ставит требования к post-processing и к мониторингу. Видите, всё связано. Сформулируйте для себя 2-3 big picture вывода: например, «Рекоммендер – это не только модель, а целая экосистема, где данные, инфраструктура и алгоритмы соединяются ради опыта пользователей».

## День 16: Масштабирование, отказоустойчивость и производительность
### Темы и задачи:
* Углубиться в аспекты системного дизайна:
  * Scalability: что если пользователей и товаров становится очень много? Методы: горизонтальное масштабирование сервисов (stateless ранкеры, которые можно множить), шардинг индексов (например, разбить пользователей по сегментам или товаров по категориям, хранить на разных узлах), кэширование результатов (например, топ-10 для каждого популярного пользователя обновлять заранее и хранить).
  * Throughput vs Latency: ваша система должна одновременно обслуживать множество запросов. Например, 1000 запросов/сек, latency p95 < 50ms. Возможно использование асинхронности: кандидаты получать параллельно от разных источников (не ждать последовательно), использовать неблокирующие I/O. Batching: если библиотека ML позволяет, обрабатывать несколько запросов вместе (GPU особенно эффективно в батчах).
  * Auto-scaling: нагрузка может меняться (ночью меньше, днём больше) – целесообразно уметь автоматом поднимать дополнительные инстансы микросервисов. В Kubernetes можно настроить HPA (horizontal pod autoscaler) по CPU/Memory.
  * Fault Tolerance: что если упал сервис кандидатов? Система должна деградировать грациозно. Например, если нет ML-ранкера, вернуть просто популярных – но не отказать в ответе. Нужно иметь fallback логику: таймауты на сервисы, если не ответил – либо зовем другой простой алгоритм, либо берём из кеша. Circuit breaker паттерны.
  * Consistency: не так критично, но, например, feature store: оффлайн обновил признаков, а онлайн ещё старые, может быть рассинхрон. Решение – версионирование фичей, double-buffering (новые вычислили -> переключили).
  * A/B deployment: способность разным пользователям отдавать разную логику. Это архитектурно: можно на уровне gateway решать или иметь флаг в рекоммендер-сервисе. Важно, чтобы группы изолированы и результаты логировались раздельно.
  * Security & Privacy: хранение персональных данных, GDPR (право пользователя запросить удаление данных – система должна уметь перестраивать модель без них). Анонимизация логов, контроль доступа к API рекомендаций (чтобы наружные злоумышленники не получили данные).
* Понять, что в реальной жизни ML-инженер работает не только над качеством модели, но и над тем, чтобы сервисы выдерживали продакшн нагрузку. Привести примеры: Alibaba, Amazon – миллиарды рекомендаций, решения: precompute + heavy caching, FPGA acceleration, и т.п. (Не нужно подробно, но осознавать масштаб).
* Interview system design question: «Design a system to recommend tweets to users in real-time». Вам нужно будет сказать: *«ок, у нас 200M пользователей, 1M твитов в день, нельзя для каждого всё – будем хранить embedding каждого твита, user embedding; новый твит -> пишем его embedding в ANN индекс; запрос пользователя -> его embedding-> ANN -> 500 кандидатов -> ранкнер (GBT) -> ..., latency 100ms». И упомянуть: «распределим по региону, кэшируем для тихих пользователей». Примерно так.
* Проговорить сложность некоторых операций: сортировка 100 кандидатов – ок O(100 log100). А вот сортировка всех товаров (наивно) – O(N logN), N=1e6 – неприемлемо, потому retrieval. Embedded search ~ O(log N) при ANN. Важно уметь прикинуть: если 1000 запросов/с, каждый ищет 100 ближайших среди 1e6 vectors – прямой поиск 1e61000=1e9 операций – не вариант, поэтому ANN: ~1e60.01*1000=1e7 (подъем). Это демонстрирует, что вы думаете о производительности.
### Рекомендуемые материалы:
* System Design Primer (GitHub) – разделы про scaling, caching, load balancing.
* HighScalability blog – есть статьи про масштаб рек систем (например, «How Pinterest Real-Time recommends items»).
* Книга «Designing Data-Intensive Applications» – для background по хранению и обработке данных.
* AWS Architecture Blog: «Scaling Recommender Systems» – практические советы (на англ).
* Доклады на YouTube: «Операционные аспекты ML систем» – бывают доклады, где инженеры делятся, как мониторят ML сервисы (пример – Facebook «ML Platforms»).
### Практика:
* Возьмите вашу функцию `get_recommendations` (из прошлых дней) и замерьте её время на 1000 вызовах подряд (можно с временем). Затем, допустим, решили разбить функционал: `get_candidates` и `rank_candidates`. Можно ли выполнять `get_candidates` параллельно из двух источников? В Python можно с `concurrent.futures.ThreadPoolExecutor`. Попробуйте смоделировать: один поток спит 5ms (имитируя запрос в ANN), другой 7ms (другой источник). Вместе они займут ~7ms, а последовательно 12ms. Параллелизация сэкономила время. Это демонстрация concurrency.
* Смоделируйте фолбэк: сделайте `get_recommendations` так, чтобы если `rank_candidates` не ответил за 50ms, она возвращала некрасиво отсортированные кандидаты или популярные. Можно использовать `timeout` в Python (например, через `threading` или просто флаг). Тест: специально задержите ранкер дольше и убедитесь, что функция всё равно возвращает результат. Это иллюстрирует отказоустойчивость.
* Настройте кеширование: например, используйте `functools.lru_cache` на функцию get_recommendations для недавно вызванных пользователей. Проверьте, что при повторных вызовах время практически нулевое. Но подумайте: что если профиль пользователя обновился? Тогда кеш может устареть – возможно, нужно инвалидировать кэш по событию (в реальных системах – при новом действии пользователя очищаем его запись в кэше).
* Представьте, что нужно отрекомендовать не 10, а 1000 элементов (например, бесконечная лента). Явно ранжировать 1000 может быть дорого. Решение: pagination – сначала 20, потом подгружаем следующие. Подумайте, как бы реализовали: может, сохранять расширенный список в кэше и отдавать по частям. Или генерировать on the fly при прокрутке – но с тем же порядком. Опишите решение.
* Вопрос на спекулирование: «Выдача рекомендаций – критичный сервис. Как обеспечить 99.9% аптайм?». Ваш ответ: «Развернуть несколько инстансов в разных зонах, за балансировщиком; реплицировать данные индексов; иметь деградационный режим (например, если ML упал – отдавать заглушки); мониторить latency и ошибки, при отклонениях посылать алерты; регулярно проводить нагрузочное тестирование». Запишите пункты – пусть у вас будет заготовка.
* Нагрузочное тестирование: если есть время, можете с помощью `asyncio` в Python или JMeter написать скрипт, бомбардирующий вашу функцию `get_recommendations` N параллельными запросами, и посмотреть среднее/максимальное время. Это игра, но она покажет, где узкое место (в Python, скорее всего, GIL помешает, но всё же).
* Определите, какие части системы требуют ACID транзакций, а где можно обойтись eventual consistency. Например: логирование просмотров – может быть асинхронным, eventual ok. Обновление модели – eventual ok (может запаздывать). А вот выдача списка – должна быть in-time консистентна с самим собой (если показали позиции – надо именно их логировать). Это, правда, детали, но лишним не будет.
* Наконец, соберите всё в голове: попробуйте без запинки устно рассказать, как работает ваша разработанная RecSys, начиная с данных и заканчивая UI. Если чувствуете пробел – пересмотрите соответствующий день. На интервью вам могут дать большой дизайн-кейс, и нужно уверенно пройтись по всем этапам.

## День 17: Подготовка к интервью – аналитические задачи
### Темы и задачи:
* Повторить и отточить навыки решения нестандартных задач по рекоммендательным системам. Интервью могут спросить что-то вроде задачника: например, «У вас упал CTR на 10% после релиза – какие шаги диагностики?» или «Как посчитать долю удовлетворенных пользователей рекомендациями?». Нужно уметь мыслить логично и предлагать решения.
* Примеры аналитических вопросов:
  * Data Insights: «У вас есть лог взаимодействий user-item (просмотры, покупки). Как бы вы определили, насколько "здоров" каталог? (например, % товаров, которые вообще не просматривают)». Это тест понимания coverage. Отвечаем: «Посчитал бы распределение просмотров по товарам (лонгтейл график), определил долю товаров с >0 просмотров, может, сделал вывод о необходимости улучшить рекомендации для хвоста».
  * Cold Start analysis: «Видим, что новые пользователи проводят мало времени. Как понять, проблема ли в холодном старте рекомендаций?». Предложить: «Разбить пользователей по когортам (новые vs старые) и сравнить их CTR на рекомендации. Если у новых значительно ниже – явно проблема. Дальше A/B: показывать новым или опросник, или популярные – посмотреть, улучшится ли удержание».
  * Metric deep-dive: «Precision@5 вырос, а Recall@5 упал после изменения алгоритма. Что это может значить?». Ответ: «Алгоритм стал рекомендовать более узко: то, что точно зайдёт (precision↑), но меньше разнообразия (упал recall). Может, слишком много популярного, или модель переобучилась на видимые взаимодействия». Решение: «может, чуть ослабить порог уверенности, добавить diversity».
  * User behavior anomaly: «Как бы вы обнаружили, что рекоммендер загнал пользователя в петлю (он смотрит одно и то же)?». Возможно: «Посмотреть entropy его прослушиваний/просмотров по времени: если сильно упало – сигнал. Или сравнить долю нового контента в его логе».
  * AB test analysis: «Вы провели A/B, новая модель увеличила клики, но продажи не выросли. Ваши действия?». Ответ: «Сегментировать по этапам воронки: возможно, новые рекомендации – кликабельный но менее конвертабельный контент. Нужно либо поменять оптимизируемую метрику модели (учитывать конверсию), либо применить послепроцессинг (фильтровать "пустые" клики). В краткосрок – откатить модель, т.к. бизнес-метрика не выросла».
  * Logging issue: «Представьте, обнаружили, что 5% логов рекомендаций теряются. Как это повлияет на обучение модели?». Рассуждение: «Если данные теряются неbiased (случайно) – не сильно, просто меньше данных. Но если систематически (например, теряются длинные сессии) – может сместить модель. Решение: улучшить логирование, может заполнить пробелы данными мониторинга».
* Подготовьте общие подходы: любые непонятки – смотреть на данные, сравнивать сегменты, строить графики. На интервью покажите структурность: *«я бы сначала проверил технические сбои (метрики, логи), потом посмотрел распределения, потом проверил гипотезы путем сегментации...».
* Продуктовые вопросы: часто будут: «Какую метрику вы бы выбрали в качестве ключевой для X?». Например: для YouTube – время просмотра (watch time) или удовлетворенность (survey). Для e-commerce – конверсия на покупку. Объясните выбор. Или: «Как убедиться, что рекомендации не вредят другим продуктовым метрикам (например, поиску)?» – ответ: мониторить влияние, возможно проводить комплексные эксперименты.
* SQL/псевдо-код запросы: может попросить: «У вас таблица просмотров (user,item,timestamp). Напишите запрос, чтобы для каждого пользователя получить топ-3 наиболее часто просматриваемых категории товаров». Будьте готовы к простым агрегациям, JOIN с таблицей item->category и оконным функциям (ROW_NUMBER).
* Математические задачки: редки, но могут. Например, «Есть 1000 пользователей, 100 товаров, построить user-user CF, сколько теоретически подобий надо вычислить?». Это комбинаторика: (1000 choose 2). Или «Как оценить память под хранение полной матрицы взаимодействий 1e6x1e5?» – посчитать 1e11 элементов, сколько bytes.
* Неструктурированное: «Как бы вы рекомендовали друзьям ресторан, в котором они не были, используя только их историю посещений?» – Здесь хотят услышать про collaborative filtering по пользователям со схожими вкусами.
### Рекомендуемые материалы:
* devinterview.io и mlstack.cafe – списки вопросов (прочитайте и попытайтесь ответить без подглядывания).
* InterviewQuery – часто есть реальные примеры кейсов (может платно).
* Статья «50 Must-Know RecSys Questions»
devinterview.io
careereducation.rochester.edu
 – обзор вопросов (англ).
* Практикум SQL/Hadoop: если есть время, освежите GROUP BY, JOIN, WINDOW – вдруг спросят вытянуть какую-то статистику из логов.
### Практика:
* Напишите небольшой список Q&A для себя: например, 10 вопросов, как выше, и краткие ответы. Потренируйтесь отвечать вслух, следя за структурой (сначала описание проблемы, затем предложение решения, упомянуть альтернативы, сделать вывод).
* Проведите мок-интервью: попросите знакомого задать вам вопросы из этой области или сами запишите вопрос на карточке и отвечайте, засеките время. Цель – чтобы ответы были 2-3 минутные, чёткие.
* Решите пару задач на метрики: «В тестовой выборке у пользователя 10 релевантных товаров, мы рекомендовали 6 из них в топ-10.
* Посчитайте Recall@10 и Precision@10.» (Ответ: Recall=0.6, Precision=0.6 если рекомендуем ровно 10 товаров и 6 из них релевантны).
* Потренируйтесь объяснять простым языком сложные вещи: например, «Что такое матричная факторизация?». Представьте, что интервьюер – не специалист (на самом деле может быть HR раунд). Объяснение: «Это метод, который выявляет скрытые особенности пользователей и товаров. Представьте, каждому пользователю и фильму соответствует вектор – список чисел, отражающий предпочтения или характеристики. Матричная факторизация – способ найти такие числа, чтобы прогнозировать оценки. Это как сжать большую таблицу предпочтений в две меньшие, но информативные». Если вы сможете так донести, значит сами хорошо понимаете.
* Если вам могут дать кейс: «Как предложить систему рекомендаций для нового музыкального сервиса?» – структуруйте ответ: 
  * какие данные будут (прослушивания, лайки), 
  * сначала простое решение (топ-чарты, жанровые подборки – холодный старт), 
  * потом collaborative filtering по юзерам, 
  * потом доработка контентом (жанры, артисты) – гибрид, 
  * инфраструктура: оффлайн модель + онлайн сервис, 
  * метрики успеха: увеличение прослушиваний/skip rate уменьшить. Лучше сказать сначала простое, затем усложнять – показывает здравомыслие.
* Отдых: не забудьте перед самым интервью дать мозгу отдохнуть. Последние дни лучше повторять написанные конспекты и хорошо выспаться.

## День 18: Проектирование архитектуры (системный дизайн) – практика кейсов
### Темы и задачи:
* Целенаправленно отработать System Design кейсы по рекоммендателям. Это сложный тип вопросов: мало времени, широкий охват. Ключ – показывать, что вы можете распланировать систему под требования.
* Типичный кейс: «Design a recommendation system for X (Netflix/Spotify/Amazon)». Интервьюер ждет: сбор требований (real-time или batched? сколько пользователей? какие бизнес-цели?), потом перечисления компонентов (data ingestion, storage, offline, online), затем фокус на 1-2 интересных момента (например, "как обеспечить <50ms latency").
  * Шаг 1: Уточнение требований. Пример: для Netflix – требование: персонализированные топ-10 видео для каждого, обновление ежедневно, latency не критично (плейсмент на главной). Для Twitter – рекомендации должны быть real-time, latency <100ms, объем ~100M в день. Выясните: какие данные входные (просмотры, лайки, тексты?), какие метрики (вовлеченность vs свежесть), должны ли быть разные виды рекомендаций (например, тематические карусели).
  * Шаг 2: High-level design. Нарисуйте 3 больших блока: Offline, Serving, Client. Offline: включает сбор данных (Kafka -> Data Lake -> ETL -> Model Training), Serving: pipeline retrieval + ranking + rules, Client: UI + feedback. Проговорите взаимодействие.
  * Шаг 3: Component detail. Углубитесь, например, в Retrieval: «Будем поддерживать индекс фичей товаров в Elasticsearch для контентного поиска, и отдельный ANN (Faiss) для эмбеддингов. При запросе будем обращаться туда параллельно.». Или модель ранжирования: «Развернём LightGBM модель в виде сервиса (можно использовать Treelite/ONNX для C++), latency ~1ms на запрос. Feature store – Redis кластер, обеспечит <5ms чтение».. Цифры впечатляют.
  * Шаг 4: Bottlenecks & Scaling. Упомяните: «Если пользователей x10, придётся шардинг: например, хранить ANN индекс по категориям на разных серверах. Feature store нужно масштабировать – возможно, тоже шардинг по ключу user_id. Модель, если ML heavy – возможно, использовать GPU batch inference.».
  * Шаг 5: Monitoring & Maintenance. Скажите: «Будем логировать время ответа каждого этапа (retrieval, ranking) в Prometheus, алертить если >X. Метрики качества – CTR, конверсия – A/B тестами. Резервные системы: если ML недоступен, фолбэк на популярное. Регулярный retrain модели (MLOps pipeline).».
* Уделите внимание конкретным технологиям: если интервьюер технарь, любит услышать: Kafka, Spark, Faiss, ScaNN, Redis, TensorFlow Serving, Kubernetes, etc. Но не перегибайте – называйте то, что к месту.
* Пример структурированного ответа (очень кратко): *«We need to design a movie recommender for Netflix-scale (~200M users, ~50k content). I'll split into offline training and online serving. Offline: user events (views, ratings) flow through Kafka to a Data Lake (S3/HDFS). Daily Spark jobs aggregate features (e.g. watch history vectors), and retrain collaborative models (matrix factorization or LightGCN for embeddings) and a ranking model (LightGBM) using labels from user ratings. Embeddings for users/items and the LightGBM model are exported to serving layer. Online: We have a multi-stage pipeline – a candidate generator service and a ranking service. The candidate service maintains an ANN index of item embeddings (using Faiss) for quick nearest-neighbor search. It also can fetch top popular in genres user likes (precomputed daily). Given a user request, it retrieves e.g. 200 candidates in <20ms. Then the ranking service fetches real-time features (user profile from a feature store – perhaps Redis, item metadata, context like time of day) and scores candidates using the LightGBM model (via an optimized C++ predict or TF Serving). That takes <30ms for 200 items. We then apply business rules (no R-rated for kids, ensure mix of genres – possibly at ranking or post-step). Finally, 10 recommendations returned to client. Scaling: The Faiss index is sharded by item ID to distribute memory; we run multiple instances of each service behind a load balancer (could use AWS autoscaling). We cache results for less active users to reduce load. Latency: total ~50ms meets requirement. Monitoring: Track p95 latency, errors for each service, and track CTR or completion rate. Use A/B testing platform to evaluate new models gradually. Cold start: new user gets popular content (maybe region-specific). New items get initial embedding from content (e.g. description via NLP) and some exploration traffic via bandit approach.*». Такой ответ покрывает многое – его, конечно, лучше адаптировать под конкретный кейс и говорить постепенно, проверяя реакцию интервьюера.
* Проработайте отдельно кастомные кейсы:
  * Рекомендации друзей (social graph + общие интересы).
  * Рекомендации для маркетплейса (две стороны: пользователи и продавцы – fairness важен).
  * Онлайн реклама (реальный time constraints, аукцион – но это отдельная тема).
  * Интересный: «новостная лента, где нужно мешать персональное и свежие новости» – тут расскажите про blending candidate sources: trending (fresh) + personalized (past behavior) + diversity.
* Не забудьте про правильные акронимы: SLA, QPS, P99 latency, etc. Это делает ответ профессиональным.
### Рекомендуемые материалы:
* Grokking the System Design Interview (разделы, может не быть конкретно про RecSys, но общие идеи полезны).
* SystemDesignPrimer (GitHub) – читать особенно про scaling terms.
* Лекции Сбер/ШАД по системному дизайну – возможно, есть конспекты на рус.
* YouTube: Exponent system design recsys – они разбирают “Spotify recommendations” и т.п. Послушайте, как эксперт отвечает.
### Практика:
* Напишите шаблон своего ответа на «Design Netflix recommender». Пусть будет 5-6 пунктов (offline data, candidate gen, ranking, scaling, etc.) со словами, которые вы хотите употребить. Затем попробуйте рассказать его, не глядя, за ~8-10 минут. Это долго, но system design интервью часто 30-40 мин, так что 10 мин монолога + 20 мин вопросов – нормально.
* Нарисуйте архитектуру на бумаге и убедитесь, что не забыли важных частей. Представьте, что у вас доска.
* Попросите друга задать уточняющие вопросы: например, «А как быстро узнаем о новом фильме?» или «Что если Redis упадёт?». Потренируйтесь отвечать на такие.
* Проработайте 2 варианта масштаба: один – “стартап версия” (мало пользователей, можно проще), другой – “enterprise версия” (миллионы, нужно все навороты). Если на интервью дадут смутный масштаб, уточните и выберите. Например: для малого – можно без сложного ANN, просто precompute topN for each user. Для большого – по полной. Это покажет гибкость.
* Обязательно упомяните тестирование: «Перед деплоем новой системы мы прогоняем оффлайн тесты, затем запускаем A/B». И этапы разработки: «Сначала сделаем простой популярные-темы, соберём данные, затем добавим персонализацию, потом улучшим модель…». Интервьюеры любят, когда вы видите поэтапно.
* Keep in mind trade-offs: например, precompute vs real-time (свежесть vs латентность), NCF deep model vs LightGBM (качество vs сложность), monolithic vs microservices (простота разработки vs масштабирование командой). Упомянуть 1-2 таких – плюс.
* В конце дня: поздравьте себя 🎉 – вы прошли огромный путь! Пересмотрите самые сложные моменты и отдохните. Завтра – финальное повторение и mock-interview.

## День 19: Финальное повторение и общий обзор
### Темы и задачи:
* Этот день – резервный/свободный. Используйте его, чтобы повторить всё пройденное, закрыть оставшиеся пробелы и настроиться психологически.
* Пробегитесь по конспектам дней 1–18. Особое внимание на определения и термины: collaborative filtering, matrix factorization, recall@K, exploration, feature store, latency, etc. Убедитесь, что можете их объяснить.
* Сделайте mind-map или шпаргалку: например, дерево тем – от классических алгоритмов до продакшн. Это поможет структурировать знания.
* Ответьте письменно (или в голове) на большой вопрос: «Что делает рекомендательную систему успешной?». Скомбинируйте: качество рекомендаций (релевантность, новизна), производительность (быстрота), способность адаптироваться (обучение на новых данных, bandits), бизнес-метрики (удержание, конверсия), ответственность (fairness, отсутствие нежелательного контента). Такой обобщающий ответ – хорошая подготовка к открытому вопросу типа «Расскажите о вашем понимании рекомендательных систем».
* Проведите mock-interview целиком: пусть знакомый/коллега спросит:
  * ваш опыт (рассказать о проекте, если есть – упомяните всё, что сделали, применяя выученные термины), 
  * теоретические вопросы (можно из тех, что готовили),
  * system design задачу (вместе прорепетировать рисование и рассказ). Получите обратную связь. Лучше несколько разных людей спросить, если возможно.
* Подготовьте истории на случай поведенческих вопросов: иногда в ML-интервью спрашивают: «Расскажите о сложном баге, который вы исправили», «Когда вы приняли неправильное решение в проекте и что сделали?». Можно связать с рекомендациями: например, «Запустили модель, а она повысила клики но вызвала жалобы – мы быстро откатили и добавили фильтры». Или «Сталкивался с непониманием между data science и продакт-менеджером – решили через A/B-тест компромиссного варианта». Подумайте над 1-2 такими историями.
* Отдых и уверенность: Вы проделали огромный труд, систематизировали знания. Ваша задача – донести их уверенно и логично. Перед самим интервью (будь то завтра или после) – хорошо выспитесь и верьте в свои силы. Даже если спросят что-то неожиданное, у вас теперь есть база, чтобы рассуждать и предлагать решения.

В этот день материалов нет – используйте собственные записи, возможно, пересмотрите любимые ресурсы или просто дайте голове отдохнуть.
На этом интенсив завершён. Вы познакомились с классическими и современными методами рекомендаций, поняли, как строится полный ML pipeline – от сбора данных до развёртывания сервиса, научились метрикам и экспериментам, проработали вопросы интервью. Теперь вы готовы уверенно обсуждать дизайн рекомендательной системы на уровне middle/senior и показывать глубокое понимание предмета. Удачи на интервью и в дальнейших проектах! 🚀


## Материалы
* [Рекомендательные системы в современном мире / Хабр](https://habr.com/ru/companies/otus/articles/950650/)
* [Шпаргалка по рекомендательным системам / Хабр](https://habr.com/ru/articles/792994/)
* [Рекомендательные системы](https://intsystems.github.io/ru/course/recommender_systems/index.html)
* [Метрики оценки для рекомендательных систем / Хабр](https://habr.com/ru/companies/otus/articles/732842/)
* [Building a Modern Recipe Recommender with LightGCN: A Case Study | by Marcin Hałupka | Medium](https://medium.com/@marcinhaupka/building-a-modern-recipe-recommender-with-lightgcn-a-case-study-7940636308d9)
* [Better recommender systems with LightGCN | by Jn2279@student.uni-lj.si | Medium](https://medium.com/@jn2279/better-recommender-systems-with-lightgcn-a0e764af14f9)
* [LightGCN — RecBole 1.2.1 documentation](https://recbole.io/docs/user_guide/model/general/lightgcn.html)
* [Как мы используем item2vec для рекомендаций похожих товаров / Хабр](https://habr.com/ru/companies/avito/articles/491942/)
* [Расчёт вкусов пользователя для ленты рекомендаций с ... - Хабр](https://habr.com/ru/companies/odnoklassniki/articles/800141/)
* [Personalised Search Recommendation Engines with Huggingface and Keras | by Richard Bownes | Medium](https://machinelearned.medium.com/personalised-search-recommendation-engines-with-huggingface-and-keras-6c220551b17a)
* [Recommendation Systems: An Architect's Playbook (Part 1) - The ML Architect](https://themlarchitect.com/blog/recommendation-systems-an-architects-playbook-part-1/)
* [Cross-Encoders, ColBERT, and LLM-Based Re-Rankers: A Practical Guide | by Michael Ryaboy | Medium](https://medium.com/@aimichael/cross-encoders-colbert-and-llm-based-re-rankers-a-practical-guide-a23570d88548)
* [Многорукие бандиты в рекомендациях / Хабр](https://habr.com/ru/companies/avito/articles/417571/)
* [Объяснение реранкера: Улучшение результатов поиска | Ultralytics](https://www.ultralytics.com/ru/glossary/reranker)
* [Lightgbm parameters tuning - My deep learning](https://konstantinklepikov.github.io/myknowlegebase/notes/lightgbm-parameters-tuning.html)
* [Evaluation Metrics for Search and Recommendation Systems | Weaviate](https://weaviate.io/blog/retrieval-evaluation-metrics)
* [Feature Store 101: Build, Serve, and Scale ML Features - Aerospike](https://aerospike.com/blog/feature-store/)
* [A Comprehensive Guide to Open Source Feature Stores](https://mohamed-elrefaey-77102.medium.com/unlocking-ai-ml-potential-a-comprehensive-guide-to-open-source-feature-stores-6b3fcdb5d0a5)
* [10 Recommender Systems Interview Questions and Answers for ML Engineers](https://www.remoterocketship.com/advice/guide/ml-engineer/recommender-systems-interview-questions-and-answers/)
* [Как оценить качество системы A/B-тестирования / Хабр](https://habr.com/ru/companies/hh/articles/321386/)
* [Как использовать A/B-тесты для приоритизации гипотез - VC.ru](https://vc.ru/marketing/2297982-kak-ab-testyi-pomogayut-prioritizirovat-gipotezy)
* [Калькулятор A/B-тестов Mindbox: выборка и анализ результатов](https://mindbox.ru/tools/ab-test-calculator/)
* [Контекстные многорукие бандиты для рекомендации контента ...](https://habr.com/ru/companies/vk/articles/673914/)
* [Exploration vs. Exploitation in Recommendation Systems - Shaped.ai](https://www.shaped.ai/blog/explore-vs-exploit)
* [Рекомендательные системы: что это и как работает алгоритм ...](https://mindbox.ru/journal/education/rekomendatelnye-sistemy/)
* [When You Hear “Filter Bubble”, “Echo Chamber”, or “Rabbit Hole”](https://medium.com/understanding-recommenders/when-you-hear-filter-bubble-echo-chamber-or-rabbit-hole-think-feedback-loop-7d1c8733d5c)
* [Data Pipeline Architecture Patterns - by AI Agents](https://systemdr.substack.com/p/data-pipeline-architecture-patterns)
* [Justin Basilico - Google Scholar](https://scholar.google.com/citations?user=fOBsTmAAAAAJ&hl=en)
* [50 Must-Know Recommendation Systems Interview Questions and ...](https://devinterview.io/questions/machine-learning-and-data-science/recommendation-systems-interview-questions/)
* [51 Essential Machine Learning Interview Questions and Answers](https://careereducation.rochester.edu/blog/2022/01/03/51-essential-machine-learning-interview-questions-and-answers/)
* [Recommendation Systems: Cracking the Interview Code](https://www.interviewnode.com/post/recommendation-systems-cracking-the-interview-code)
