# Recommender Systems Basics

## 1. Taxonomy

### 1.1 Неперсонализированные (non-personalized)
Используются как baseline, фолбэк, cold-start, либо как один из источников кандидатов.

- **Popularity-based**: рекомендовать «самое популярное» (global / по категории / по региону / по времени суток).
- **Trending / Recent-popularity**: популярность с убыванием по времени (устраняет «вечные хиты»).
- **New items / Freshness boost**: новинки с контролируемой долей (для экспозиции).
- **Editorial / Rules**: ручные подборки, бизнес-правила.

**Плюсы:** простота, стабильность, работает без истории пользователя.  
**Минусы:** слабая персонализация, усиливает popularity bias.

### 1.2 Персонализированные (personalized)

#### 1.2.1 Content-based
Рекомендуем **похожие по контенту** на то, что пользователю нравилось.

- Фичи айтема: текст/жанры/теги/атрибуты/эмбеддинги.
- Скоринг: cosine / dot / модель над фичами.

**Плюс:** решает cold-start для новых айтемов (если есть контент).  
**Минус:** “more of the same”, хуже ловит коллективные паттерны.

#### 1.2.2 Collaborative Filtering (CF)
Рекомендуем по **поведению пользователей** (матрица взаимодействий).

**Memory-based (kNN):**
- **User-based**: похожие пользователи → что им понравилось.
- **Item-based**: похожие айтемы → что похоже на уже понравившееся.
- Сходство: cosine, Pearson (для рейтингов), Jaccard (для бинарного implicit).

**Model-based:**
- **Matrix Factorization** (SVD/ALS/SGD-MF): латентные факторы пользователей и айтемов.
- Для implicit часто: **ALS (implicit)** и **BPR** (pairwise-ранжирование “позитив выше негатива”).

**Плюсы:** сильная персонализация, ловит скрытые факторы.  
**Минусы:** cold-start (новый user/item без истории), чувствительность к смещениям логов.

#### 1.2.3 Hybrid
Комбинация content + CF + popularity + rules.

- Гибрид на этапе кандидатов: объединяем источники.
- Гибрид на этапе ранжирования: фичи из разных моделей.
- Post-processing: diversity/novelty/freshness/constraints.

#### 1.2.4 Часто выделяют отдельно (полезно знать)
- **Session-based / Sequential**: учитывает порядок действий в сессии (RNN/Transformer/Markov).
- **Graph-based**: user–item граф (LightGCN и т.п.).
- **Context-aware**: время/гео/девайс/сезонность как вход.

---

## 2. Problem Statement

Дано:
- $U=\{u_j\},\; j=1,\dots,n_{users}$ — множество пользователей
- $I=\{i_j\},\; j=1,\dots,n_{items}$ — множество объектов (items)
- $R=\|r_{ui}\|$ — матрица взаимодействий размера $n_{users}\times n_{items}$

Типы фидбэка:
- **Implicit**: обычно $r_{ui}\in\{0,1\}$ (клик/просмотр/покупка). Важно: “0” чаще означает **не наблюдали**, а не “не нравится”.
- **Explicit**: обычно $r_{ui}\in\{1,2,3,4,5\}$ (рейтинг).

Типовые задачи:

### 2.1 Rating prediction
Предсказать неизвестный $r_{ui}$ как регрессию/классификацию.

### 2.2 Top-K ranking (главная промышленная постановка)
Дать пользователю **ранжированный список** из $K$ айтемов.
- **item2item**: похожие на айтем (например, «похожие товары»).
- **user2item / item2user**: что показать пользователю.

---

## 3 Метрики (offline, top-K)

Нотация: для пользователя $u$ модель выдаёт список $\pi_u=[i_1,\dots,i_k]$.

### 3.1 DCG@k → IDCG@k → nDCG@k

#### Формулы
**Gain (выигрыш) от релевантности $rel$:**
$$
 g(rel)=2^{rel}-1
$$

**Discount (штраф за позицию):**
$$
 disc(pos)=\frac{1}{\log_2(pos+1)}
$$

**DCG (Discounted Cumulative Gain):**
$$
 DCG@k(u)=\sum_{pos=1}^{k}\frac{2^{rel_{u,i_{pos}}}-1}{\log_2(pos+1)}
$$

**IDCG** — DCG для идеального порядка (сортируем релевантности по убыванию).

Нормировка:
$$
 nDCG@k(u)=\frac{DCG@k(u)}{IDCG@k(u)},\qquad
 nDCG@k=\frac{1}{|U|}\sum_u nDCG@k(u)
$$

#### Смысл
- $2^{rel}-1$: делает “очень релевантное” **намного важнее** “чуть релевантного”.
- Дисконт: ошибки вверху списка больнее, чем внизу.
- **Explicit**: $rel$ градуирован (0..5) → nDCG хорошо подходит.
- **Implicit**: $rel\in\{0,1\}$ → gain бинарный, но позиционный дисконт остаётся полезным.

#### Мини-пример (explicit, k=5)
Релевантности по позициям: $[3,2,0,1,2]$.

DCG:
- pos1: $(2^3-1)/\log_2(2)=7/1=7$
- pos2: $(2^2-1)/\log_2(3)=3/1.585\approx1.893$
- pos3: $(2^0-1)/\log_2(4)=0$
- pos4: $(2^1-1)/\log_2(5)=1/2.322\approx0.431$
- pos5: $(2^2-1)/\log_2(6)=3/2.585\approx1.161$

$DCG\approx10.484$.

IDCG: сортируем $[3,2,2,1,0]$:
$IDCG\approx 7+1.893+1.5+0.431+0=10.823$.

$nDCG\approx10.484/10.823\approx0.969$.

---

### 3.2 Precision@k(u) → AP@k(u) → MAP@k

(Классика для implicit / бинарной релевантности.)

#### Формулы
Пусть $Rel(u)$ — множество релевантных айтемов в тесте для пользователя $u$.

$$
Precision@k(u)=\frac{|\{i_{pos}\in Rel(u): pos\le k\}|}{k}
$$

Это "сколько из топ-k оказалось релевантным".  
**Минус:** позиции внутри топ-k не важны (попал на 1 или на 10 — всё равно).

**Average Precision@k(u):** среднее precision **в позициях, где встретили релевантный айтем**:
$$
AP@k(u)=\frac{1}{\min(k,|Rel(u)|)}\sum_{pos=1}^{k} Precision@pos(u)\cdot \mathbb{1}[i_{pos}\in Rel(u)]
$$

* Если релевантные стоят раньше, то соответствующие `Precision@i` выше ⇒ `AP` выше.
* Если релевантные стоят позже, то `Precision@i` ниже ⇒ `AP` ниже.
* Нерелевантные позиции в сумме не учитываются (потому что `rel_i=0`).

Нормировка через `min(|Rel(u)|, k)` нужна, чтобы максимум был 1 даже если релевантных больше, чем k.

**MAP@k:** усредняем по пользователям:

$$
MAP@k=\frac{1}{|U|}\sum_u AP@k(u)
$$

#### Смысл
- Precision@k: доля релевантных в топ-k.
- AP@k: дополнительно поощряет, когда релевантные стоят **раньше**.
- MAP@k: среднее по пользователям.

**Explicit vs implicit:**
- MAP обычно для **бинарной** релевантности (implicit). Для explicit либо биняризация (rating ≥ threshold), либо лучше nDCG.

#### Мини-пример (implicit, k=5)
Рекомендации: $[A,B,C,D,E]$

Релевантные в тесте: $\{B,D,E\}$.

Точки релевантности: pos2, pos4, pos5.
- $Precision@2=1/2=0.5$
- $Precision@4=2/4=0.5$
- $Precision@5=3/5=0.6$

**AP@5=(0.5+0.5+0.6)/3\approx0.533**

---

### 3.3 Recall@k

$$
Recall@k(u) = \frac{|\{i_{pos}\in Rel(u): pos\le k\}|}{|Rel(u)|}
$$

Агрегация по пользователям (обычно macro-average):

$$
Recall@k = \frac{1}{|U|}\sum_u Recall@k(u)
$$

#### Смысл
Метрика «не упустить»: насколько полно топ-k покрывает то, что для пользователя релевантно в тесте.

#### Мини-пример (implicit, k=5)
Рекомендации: $[A,B,C,D,E]$

Релевантные в тесте: $\{B, D, E, F\}$ (4 штуки, из них 3 попали в топ-5).

**Recall@5(u) = 3/4 = 0.75**.

#### Где Recall@k важнее всего
* Candidate generation / retrieval (первый этап): задача этапа — чтобы релевантное вообще попало в кандидаты для ранжировщика.
* Большие каталоги + разреженный implicit: промахнуться мимо нужного легко, поэтому «полнота» критична

#### Где Recall@k может быть шумным/обманчивым
* Где Recall@k может быть шумным/обманчивым. Когда у многих пользователей в тесте мало релевантных (|Rel(u)| = 1..2) — метрика становится «рваной».
* Exposure bias: если релевантность строится из кликов/покупок, то не показанное не могло стать релевантным.

---

### 3.4 MRR

Пусть $rank_u$ — позиция первого релевантного айтема (если нет — 0).

$$
RR(u)=\begin{cases}
\frac{1}{rank_u}, & \text{если есть релевантный}\\
0, & \text{иначе}
\end{cases}
$$

$$
MRR=\frac{1}{|U|}\sum_u RR(u)
$$

Смысл: модель хороша, если она поднимает **первое попадание** максимально вверх.

Мини-пример:
- первый релевантный на позиции 4 → $RR=0.25$
- на позиции 1 → $RR=1$

---

### 3.5 Когда какую метрику использовать

- **Rating prediction** (явные рейтинги, нужен прогноз числа): обычно **RMSE/MAE** (это отдельная ветка от top-K).
- **Top-K ranking, graded relevance (explicit)**: **nDCG@K** *(учитывает и порядок, и “силу” релевантности; де-факто стандарт)*.
- **Top-K ranking, бинарная релевантность (implicit)**:
  - “качество всего топа + ранние позиции важнее” → **MAP@K** *(штрафует за поздние попадания, чувствителен ко всему топу)* или **nDCG (binary rel)** - допустимая замена.
  - “Важно хотя бы одно раннее попадание” → **MRR** *(поиск, next-item, похожие объекты)*.
  - “чистота топ-k” → **Precision@K** *(простая, интерпретируемая, но не видит порядок внутри топа)*.
- **Candidate generation / retrieval** (первый этап пайплайна): **Recall@K (K ≫ финального, например 50/100/200)**, т. к. важно не потерять релевантное. Здесь recall — ключевая метрика, потому что ранжировщик не сможет поднять релевантный объект, если его не принесли кандидаты

Почему оффлайн-метрика может расходиться с продом:
- **позиционный bias** (клики зависят от позиции, а не только от релевантности),
- **feedback loop** (показываем → кликают → считаем релевантным → показываем ещё),
- **negative sampling** (оффлайн-распределение ≠ реальное пользовательское),
- **суррогатная оптимизация** (оффлайн оптимизируем nDCG/MAP, а бизнес-цель — CTR, конверсия, retention).

---

## 4. Overall evaluation framework

Цель: не просто «посчитать метрику», а спроектировать корректную оценку (offline / user study / online).

### 4.1 Evaluation Objectives
1) **Overall goal**: что оптимизируем (CTR, конверсия, watch time, retention, прибыль).
2) **Stakeholders**: пользователь / бизнес / контент-провайдеры / модерация / юристы.
3) **Properties**: обязательные свойства: персонализация, разнообразие, новизна, безопасность, стабильность по времени.

### 4.2 Evaluation Principles
- **Hypothesis / Research question**: формулируем проверяемую гипотезу.
- **Control variables**: что фиксируем (candidate set, UI, фильтры, ограничения).
- **Generalization power**: переносимость результатов (новые когорты, регионы, сезоны).
- **Reliability**: устойчивость (повторяемость, интервалы, защита от утечек).

### 4.3 Experiment Type
- **Offline evaluation**: быстро/дёшево; риск смещений.
- **User study**: качественные инсайты «почему»; малый масштаб.
- **Online evaluation (A/B)**: реальный эффект; дороже и рискованнее.

### 4.4 Evaluation Aspects
- **Types of data**: explicit/implicit, сессии, контент, контекст.
- **Data collection**: что логируем (показы, позиции, клики, dwell time), связь с вариантами эксперимента.
- **Data quality & biases**: пропуски, позиционный bias, feedback loop, popularity bias.
- **Evaluation metrics**: целевая метрика + guardrails (чтобы рост CTR не ломал качество/безопасность).
- **Evaluation system**: воспроизводимость (сплиты, версии данных, одинаковые candidate sets, отчётность).

### 4.5 Мини-чеклист
1) Зафиксировать цель и целевое действие.
2) Зафиксировать протокол offline:
   - time-split, фильтры, дедупликация,
   - негативы/сэмплинг (если нужен),
   - метрики и $K$.
3) Оценить устойчивость (бутстрэп по пользователям, интервалы).
4) Sanity-checks (включая popularity baseline).
5) Потом A/B:
   - primary metric + guardrails,
   - SRM check,
   - анализ по сегментам,
   - план отката.