# Типичные модели коллаборативной фильтрации

## 0. Постановка и нотация

* Пользователи: $u \in U$, айтемы: $i \in I$.
* Матрица взаимодействий/рейтингов: $R \in \mathbb{R}^{|U|\times|I|}$, элемент $r_{ui}$.
* $W$ — item × item матрица похожестей / весов.
* $I_u$ — айтемы, с которыми взаимодействовал пользователь $u$. 
* $U_i$ — пользователи, взаимодействовавшие с $i$.

Заполнение пропусков нулями:

* Это естественно для **implicit-feedback** (клики/просмотры): отсутствие взаимодействия трактуется как 0-сигнал (неявно).
* Для **explicit ratings** (1–5) ноль обычно **не** означает «пропуск»; тогда используют центрирование/остатки (например, вычитание $\bar r_u$) или отдельные маски.

---

## 1. ItemKNN (neighbourhood-based)

### 1.1. Идея

Предсказываем $\hat r_{ui}$ через **агрегацию** оценок пользователя $u$ по айтемам, похожим на целевой $i$. В отличие от «наивного» item-based CF, берём **не все** $I_u$, а только ближайших соседей.

Определим:

* $\mathrm{sim}(i,j)$ — похожесть айтемов (cosine/Pearson/и т.п.).
* $J_u(i) = \mathrm{TopK}_{j\in I_u}\ \mathrm{sim}(i,j)$ — множество $k$ самых похожих на $i$ айтемов среди тех, что известны для пользователя $u$.

### 1.2. Формула предсказания

Классическая форма (взвешенное среднее):

$$
\hat r_{ui} = \frac{\sum\limits_{j\in J_u(i)} \mathrm{sim}(i,j)\, r_{uj}}{\sum\limits_{j\in J_u(i)} |\mathrm{sim}(i,j)|}.
$$

Замечания:

* В знаменателе часто берут $\sum |\mathrm{sim}|$, чтобы корректно нормировать при возможных отрицательных похожестях.
* Если $\mathrm{sim}\ge 0$ гарантируется (например, неотрицательные веса), можно писать без модуля.

### 1.3. Матричная интерпретация

Можно мыслить как $\hat R = R W$, где столбец $W_{:,i}$ содержит веса соседних айтемов для предсказания $i$ (обычно **разреженный top-k** по каждому $i$).

---

## 2. UserKNN (симметрично ItemKNN)

### 2.1. Определения

* $\mathrm{sim}(u,v)$ — похожесть пользователей.
* $N_i(u) = \mathrm{TopK}_{v\in U_i}\ \mathrm{sim}(u,v)$ — множество $k$ ближайших пользователей, которые взаимодействовали с $i$.

### 2.2. Формула (базовая)

$$
\hat r_{ui} = \frac{\sum\limits_{v\in N_i(u)} \mathrm{sim}(u,v)\, r_{vi}}{\sum\limits_{v\in N_i(u)} |\mathrm{sim}(u,v)|}.
$$

### 2.3. Формула с нормировкой по пользователю (bias)

Для explicit-rating часто используют центрирование:
$$
\hat r_{ui} = \bar r_u + \frac{\sum\limits_{v\in N_i(u)} \mathrm{sim}(u,v)\, (r_{vi}-\bar r_v)}{\sum\limits_{v\in N_i(u)} |\mathrm{sim}(u,v)|}.
$$

Практически userKNN обычно менее стабильна (похожие пользователи «уплывают» со временем) и хуже масштабируется.

---

## 3. SLIM (Sparse Linear Methods)

### 3.1. Модель

SLIM строит предсказания как **линейную комбинацию** айтемов, с которыми пользователь уже взаимодействовал:

$$
\hat r_{ui} = R_{u,:}\, W_{:,i}, \quad \text{то есть}\quad \hat R = R W,
$$

где $W \in \mathbb{R}^{|I|\times|I|}$ — матрица item-item весов.

Интуиция: вместо фиксированного $\mathrm{sim}(i,j)$ мы **учим веса** $w_{ji}$ так, чтобы они лучше восстанавливали наблюдаемые взаимодействия.

### 3.2. Оптимизационная задача

Классическая постановка SLIM:

$$
\min_W \ |R - R W|_F^2 + \frac{\beta}{2}|W|_F^2 + \lambda|W|_1
$$

при ограничениях
$$
W \ge 0,\qquad \mathrm{diag}(W)=0.
$$

Расшифровка членов:

* $|R-RW|_F^2$: хотим, чтобы $RW$ хорошо **восстанавливал** наблюдаемую матрицу.
* $\frac{\beta}{2}|W|_F^2$ — $L2$-регуляризация: shrinkage весов, борьба с мультиколлинеарностью.
* $\lambda|W|_1$ — $L1$-регуляризация: **разреженность** $W$ → меньше памяти/вычислений и меньше переобучения, особенно при разреженном $R$.

### 3.3. Зачем $\mathrm{diag}(W)=0$
Условие $\mathrm{diag}(W)=0$ означает: айтем нельзя использовать для предсказания самого себя.

Если диагональ разрешить, возникает вырожденность — для любого пользователя $u$: 
$$
\hat{r}_{ui} = (RW)_{ui} = \sum_j{r_{uj}\cdot w_{ji}}
$$
Если поставить $w_{ii}$ большим, то доминирует слагаемое $r_{ui}\cdot w_{ii}$, и модель фактически копирует вход:
$$
\hat{r}_{ui}\approx r_{ui}\cdot w_{ii}
$$
Реконструкция становится “слишком лёгкой” и перестаёт отражать реальную идею item-kNN (использовать соседей).

**Почему это плохо:**
* **Концептуально:** мы хотим предсказывать $i$ из похожих айтемов, а не “из него самого”.
* **Практически:** при обучении на реконструкцию $R$ модель получает тривиальный путь минимизировать лосс (почти единичная $W$), что даёт дегенерацию и ухудшает обобщение.
* **Утечки:** если в расчёте участвуют $r_{ui}$, то это прямое подглядывание в таргет для пары $(u,i)$.

### 3.4. Нужна ли симметрия $W$

**Типичная ошибка:** считать, что $W$ обязана быть симметричной.

В SLIM симметрия **не требуется** и обычно **не выполняется**:

* $w_{ji}$ интерпретируется как «насколько наличие $j$ помогает предсказать $i$» — это направленная зависимость.
* симметрия возможна как пост-обработка (например, $(W+W^T)/2$), но это меняет смысл и может ухудшить метрики.

### 3.5. Как решают задачу на практике

Оптимизация по столбцам:

* каждый столбец $w_i = W_{:,i}$ — это линейная модель, которая объясняет взаимодействия с айтемом $i$ через взаимодействия с другими айтемами; регуляризация $L1+L2$ делает соседей редкими и устойчивыми.
* обучается **независимо для каждого i**, что удобно для параллелизации.

Типичные решатели:

* coordinate descent (классика для Lasso/ElasticNet + ограничения),
* proximal gradient (ISTA/FISTA),
* специализированные sparse solvers.

После $N$ итераций получаем приближённо оптимальный $W$ (по критерию задачи и допускам решателя).

---

## 4. EASE (Embarrassingly Shallow AutoEncoder)

### 4.1. Идея

EASE оставляет ту же линейную форму $\hat R = R W$, но упрощает обучение так, чтобы получить **закрытую формулу**.

### 4.2. Оптимизационная задача

$$
\min_W \ |R - R W|_F^2 + \lambda|W|_F^2
\quad \text{s.t.}\quad \mathrm{diag}(W)=0.
$$

Отличия от SLIM:

* **нет L1** → веса, как правило, **неразреженные**,
* **нет ограничения $W\ge 0$** (в базовой EASE),
* зато есть **closed-form** решение (через обращение матрицы размера $|I|\times|I|$).

### 4.3. Почему diag(W)=0 «критично»

По той же причине, что и в SLIM: без нулевой диагонали задача склонна к тривиальному самокопированию.

### 4.4. Closed-form (в правильной компактной форме)

Обозначим:
$$
G = R^T R + \lambda I, \qquad P = G^{-1}.
$$

Тогда веса EASE:
$$
W_{ij} =
\begin{cases}
0, & i=j,\\
-\dfrac{P_{ij}}{P_{jj}}, & i\ne j.
\end{cases}
$$

Интерпретация:

* $R^T R$ — item-item грамм-матрица (со-встречаемость в пользовательских профилях),
* $\lambda I$ — ridge-регуляризация,
* обращение даёт «декоррелированные» веса; нормировка на $P_{jj}$ возникает из условия на диагональ.

### 4.5. Практика применения

* Хотя $W$ получается плотной, на инференсе обычно берут **top-k** по столбцу $i$ (pruning) — это резко ускоряет ранжирование.
* Обучение требует обращения $|I|\times|I|$ матрицы: работает хорошо при умеренном числе айтемов; для больших каталогов нужны оптимизации (Cholesky, блочные/приближённые методы).

---

## 5. Связь моделей между собой

### 5.1. ItemKNN как частный случай линейной модели

Если построить $W$ так, чтобы:

* в каждом столбце $i$ были ненулевые веса только у top-k соседей,
* веса были пропорциональны $\mathrm{sim}(i,j)$ (и нормированы),

то $\hat R = R W$ совпадает с ItemKNN.

### 5.2. SLIM/EASE как «обучаемая похожесть»

* ItemKNN: $\mathrm{sim}(i,j)$ задаётся эвристикой (cosine/Pearson/…)
* SLIM/EASE: веса $w_{ji}$ **обучаются**, чтобы оптимально реконструировать $R$ при регуляризации.

Разница в контроле структуры:

* **SLIM** делает $W$ разреженной $L1$ и часто неотрицательной.
* **EASE** делает $W$ плотной, но даёт closed-form и часто сильный бейзлайн.

---

## 6. Мини-памятка по “что выбирать”

* Нужна простота и интерпретируемость → **ItemKNN**.
* Нужен сильный линейный бейзлайн с разреженностью → **SLIM**.
* Нужен очень сильный линейный бейзлайн с простым обучением → **EASE** (потом pruning).