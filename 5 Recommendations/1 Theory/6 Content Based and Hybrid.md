# Content‑based и Hybrid рекомендательные системы

## 0) Зачем это нужно: когда коллаборативки недостаточно
Коллаборативные модели (MF/ALS/BPR, kNN по взаимодействиям) сильны, когда:
- есть много исторических взаимодействий,
- айтемы и вкусы относительно стабильны,
- хватает времени “набрать” фидбек.

Но есть домены, где это ломается:
- **быстро меняющийся каталог** (fashion: коллекции обновляются несколько раз в год),
- **очень короткий жизненный цикл контента** (соцсети/новости/твиты),
- **холодный старт** (новые айтемы, новые пользователи),
- **нишевые предпочтения** (мало пересечений по аудитории).

Тогда становятся важны **контентные и контекстные признаки**: модель должна понимать, *что это за айтем* и *в каком контексте пользователь принимает решение*.

---

## 1) Определения
### 1.1 Content‑based рекомендации
**Content‑based** система использует **описательные признаки айтемов** (и иногда пользователя), чтобы рекомендовать объекты, похожие на то, что пользователю уже понравилось.

- “Похоже на то, что ты смотрел/покупал” по содержанию: жанр, теги, текст, изображение, бренд, стиль.
- Основной источник сигнала — **сходство айтемов в пространстве признаков**.

### 1.2 Hybrid системы
**Hybrid** — это любые системы, комбинирующие разные источники и подходы:
- коллаборативные (user×item взаимодействия),
- контентные фичи айтемов,
- фичи пользователя,
- контекстные фичи (время, устройство, погода),
- правила/бизнес‑логика.

Грубо: hybrid = “всё, что помогает”, если это корректно встроено в моделирование и прод‑пайплайн.

---

## 2) Что такое фичи в рекомендациях
Обозначим: хотим предсказывать скор релевантности `s(u, i, c)` для пользователя `u`, айтема `i` и контекста `c`.

Типы сигналов:

### 2.1 Interaction features (из взаимодействий)
- implicit: клики, просмотры, покупки, добавления в избранное, dwell time,
- explicit: рейтинги.

Это самая “чистая” персонализация, но она страдает от **MNAR/exposure bias** и cold start.

### 2.2 User features
Примеры: возраст, пол (если есть), регион, язык, устройство, сегмент, подписки, корзина/история, предпочтения из анкеты.

### 2.3 Item features (контент)
Примеры: цена, бренд, категория, размер/цвет/ткань, стиль, теги, текстовое описание, изображения, видео.

### 2.4 Context features
Примеры: время суток/день недели, сезонность, погода, гео/магазин, текущая страница (карточка товара, поиск), источник трафика.

### 2.5 Почему домен важен
У разных доменов:
- разные паттерны потребления,
- разная скорость устаревания.

Примеры:
- **Одежда**: каталоги быстро меняются → контент и контекст часто критичнее “старого рейтинга”.
- **Книги**: есть “вечная классика” + новые бестселлеры → сочетание CF + контента.
- **Соцсети/короткие тексты**: актуальность минут‑часов + огромный поток → ждать фидбек долго, поэтому контентные эмбеддинги и контекст обычно важнее.

---

## 3) Преобразование фичей (feature engineering для RecSys)
### 3.1 Числовые
- напрямую,
- часто нужна нормализация/лог‑скейл (цены, счётчики),
- иногда биннинг (ценовые корзины).

### 3.2 Категориальные
- one‑hot (малые кардинальности),
- hashing trick (большие кардинальности),
- multi‑hot для тегов,
- target/frequency encoding (осторожно, утечки!).

### 3.3 Текст и изображения
Нужны вектора фиксированной длины:
- TF‑IDF / BM25 (простые базлайны),
- эмбеддинги (BERT/CLIP/мультимодальные),
- агрегаты по текстовым полям/тегам.

### 3.4 Время в рекомендациях
Время обычно используют двумя способами:

1) **Календарные признаки**: час, день недели, месяц, сезон, праздники.
2) **Recency признаки**: “сколько времени прошло”:

$$
\Delta t = t_{now} - t_{event}
$$

и дальше:
- скармливать как числовую фичу,
- делать decay‑веса: $w = exp(-\Delta t / \tau)$.

Особенно важно для последовательных рекомендаций и динамических лент.

---

## 4) Content‑based similarity models (item‑to‑item по признакам)
Идея: в отличие от CF‑подходов, где сходство строится из взаимодействий, здесь мы строим сходство айтемов **в feature‑space**.

### 4.1 Представление айтема
Пусть у каждого айтема есть вектор признаков:

$$
\phi(i) \in \mathbb{R}^d
$$

где `φ(i)` может включать:
- числовые/категориальные фичи,
- text/image эмбеддинги,
- multi‑hot теги.

### 4.2 Мера сходства

- cosine:
$$
\mathrm{sim}(i,j)=\frac{\phi(i)\cdot \phi(j)}{\|\phi(i)\|\,\|\phi(j)\|}
$$

- dot product: `sim(i,j)=φ(i)·φ(j)` (часто после нормировки),
- Jaccard для множеств тегов `A,B`:
$$
J(A,B)=\frac{|A\cap B|}{|A\cup B|}
$$

### 4.3 Оценка релевантности кандидата по истории пользователя
Пусть `I_u` — множество айтемов в истории пользователя (например, купленные/лайкнутые).

Вариант “взвешенное среднее” (аналог item‑kNN, но sim берётся по контенту):

$$
\hat{r}_{ui}=\frac{\sum\limits_{j\in I_u} \mathrm{sim}(i,j)\,r_{uj}}{\sum\limits_{j\in I_u}|\mathrm{sim}(i,j)|}
$$

Вариант “максимум по похожему хорошему айтему”:

$$
\hat{r}_{ui}=\max_{j\in I_u,\; r_{uj}\ge \alpha}\; \mathrm{sim}(i,j)\,r_{uj}
$$

где:
- `r_{uj}` — “оценка/сила интереса” к айтему `j` (в implicit можно считать `r_{uj}=1` или вес по типу события),
- `α` — порог “достаточно хорошего” взаимодействия.

### 4.4 Плюсы
- **Cold item**: новый айтем можно рекомендовать сразу — у него есть контент.
- Хорошо для **короткой истории** пользователя.
- Помогает с **нишевыми интересами** (не нужен “похожий пользователь”, достаточно похожего контента).

### 4.5 Минусы
- Склонность к “однообразию”: рекомендует всё время *похожее на уже виденное*.
- “Похожесть по признакам ≠ похожесть по сути” (особенно если признаки бедные).
- Не использует коллаборативные закономерности (“люди, похожие на тебя, любят X”).

### 4.6 Коротко про cold start (пользователь)
Если у нового пользователя есть признаки (анкета, демография, источник, экосистемные данные), то можно:
- сделать начальную персонализацию по сегменту (например, “мужчины <30”),
- делать **user‑to‑user** в feature space,
- использовать эти признаки в гибридной модели (FM/LightFM/NN).

---

## 5) Почему “просто линейная модель” часто недостаточна
Пусть мы хотим предсказать `\hat{y}(u,i)` по признакам `x(u,i)`.

Линейная модель:

$$
\hat{y}(x)=w_0+\sum_{t=1}^n w_t x_t
$$

Проблема для рекомендаций:
- если признаки пользователя входят аддитивно, то для фиксированного пользователя `u` вклад user‑фич — **константа**, одинаковая для всех айтемов.
- ранжирование внутри пользователя будет в основном определяться item‑фичами, а **взаимодействия вида “этот пользователь × этот айтем”** напрямую не моделируются.

То есть линейка плохо ловит **кросс‑эффекты** (feature interactions), которые в рекомендациях ключевые.

Решение: явно моделировать попарные взаимодействия признаков.

---

## 6) Factorization Machines (FM)
FM — способ эффективно учитывать **попарные взаимодействия** признаков без явного обучения `O(n^2)` весов.

### 6.1 Данные
Пусть `x ∈ ℝ^n` — разреженный вектор признаков для пары (user, item, context).
Пример компонент:
- one‑hot userID,
- one‑hot itemID,
- one‑hot brand,
- one‑hot category,
- время,
- устройство,
- …

### 6.2 Модель FM (степень 2)

$$
\hat{y}(x)= w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n\sum_{j=i+1}^n \langle v_i, v_j\rangle x_i x_j
$$

где:
- `w_0` — bias,
- `w_i` — линейные веса,
- `v_i ∈ ℝ^k` — латентный вектор (эмбеддинг) для *i‑й фичи*,
- `<v_i, v_j>` — моделирует вклад пары признаков `(i,j)`.

### 6.3 Почему это эффективно
Если бы мы делали “полином 2‑й степени” напрямую, нам нужен вес для каждой пары фичей:
- количество пар ≈ `n(n-1)/2`.

FM заменяет вес пары на скалярное произведение двух векторов:
- вместо `O(n^2)` параметров мы учим `O(nk)`.

### 6.4 Вычисление только по ненулевым
На практике `x` разреженный (в одной строке активны десятки фич, не миллионы), и мы считаем взаимодействия **только для активных компонент**, что делает модель применимой.

### 6.5 Вариации
Базовая FM хранит один вектор на фичу. Иногда этого мало (разные “роли” сущностей). Есть расширения (Field‑aware FM и др.), но базовую идею важно знать.

---

## 7) Плюсы и минусы FM
### Плюсы
- Умеет сочетать user/item/context фичи.
- Поддерживает cold start (есть фичи — есть предсказания).
- Можно обучать под разные задачи и разные лоссы.
- Лучше, чем явные полиномиальные фичи, по памяти.

### Минусы
- Работа с непрерывными числовыми фичами не всегда тривиальна:
  - в линейной части они влияют линейно,
  - в парной части участвуют как `x_i x_j` — масштаб важен → нужны нормализации/биннинги.
- Иногда хуже выжимает чисто коллаборативный сигнал, чем специализированные CF‑модели.
- В проде сегодня часто уступает более современным нейросетевым гибридам.

---

## 8) LightFM: практичный гибрид CF + фичи
LightFM можно воспринимать как “MF, где эмбеддинги пользователя и айтема — это сумма эмбеддингов их признаков”.

### 8.1 Скоринг
Базовая форма:

$$
\hat{r}_{ui} = f( q_u^\top p_i + b_u + b_i )
$$

где:
- `q_u` — вектор пользователя,
- `p_i` — вектор айтема,
- `b_u, b_i` — bias,
- `f` — функция (часто identity для рейтингов или sigmoid для вероятностей).

### 8.2 Как формируются `q_u` и `p_i` из фич
Пусть:
- `f_u` — множество активных признаков пользователя,
- `f_i` — множество активных признаков айтема.

Тогда:

$$
q_u = \sum_{j\in f_u} w_j^{(u)}\, e_j,
\qquad
p_i = \sum_{j\in f_i} w_j^{(i)}\, e_j
$$

где:
- `e_j ∈ ℝ^k` — обучаемый эмбеддинг *признака* `j`,
- `w_j` — вес/скейл фичи (например 1 для one‑hot, либо нормировка),
- bias также можно суммировать по фичам:

$$
 b_u = \sum_{j\in f_u} w_j^{(u)}\, b_j,
 \qquad
 b_i = \sum_{j\in f_i} w_j^{(i)}\, b_j.
$$

Практический смысл:
- userID и itemID — это тоже “фичи” (через one‑hot),
- поэтому LightFM умеет работать **и без дополнительных фичей** (тогда это почти MF),
- и с фичами (тогда это гибрид).

### 8.3 Почему нормировка важна
Если у сущности много активных фич, простая сумма может раздувать норму. Поэтому часто делают нормировку весов, например:

$$
 w_j^{(u)} = \frac{1}{|f_u|}
$$

или $\dfrac{1}{\sqrt{|f_u|}}$ — чтобы “богатые по фичам” пользователи/айтемы не доминировали.

### 8.4 Прод‑инференс
После обучения:
- получаем `q_u`, `p_i` как суммы эмбеддингов → это быстро,
- далее скоринг — dot product,
- можно использовать ANN для retrieval.

---

## 9) Преимущества и недостатки LightFM
### Плюсы
- Простая формула инференса (dot product) → дружит с ANN.
- Работает с cold start users/items (если есть фичи).
- Может работать и без фичей.
- Хороший мостик к современным нейросетевым моделям (идея “эмбеддинг фич → агрегировать → скор”).

### Минусы
- Непрерывные числовые фичи всё ещё требуют аккуратной обработки (скейл, лог, биннинг).
- Иногда чистая CF‑модель без фичей может быть сильнее (если фичи шумные/устаревшие).
- Линейная агрегация фич не ловит сложные нелинейные зависимости (для этого нужны более мощные гибриды/NN).

---

## 10) Мини‑примеры на Python
### 10.1 Content‑based item2item по cosine
```python
import numpy as np

# item feature matrix: (n_items, d)
X = np.array([
    [1, 0, 0],
    [1, 1, 0],
    [0, 1, 0],
    [0, 1, 1],
], dtype=float)

# cosine similarity matrix
Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)
S = Xn @ Xn.T

item = 0
sim_ids = np.argsort(-S[item])
print('most similar to', item, sim_ids[:3], S[item, sim_ids[:3]])
```

### 10.2 Оценка $\hat{r}_{ui}$ по истории (взвешенное среднее)
```python
def score_by_history(target_item, history_items, history_strength, sim_matrix, eps=1e-12):
    sims = np.array([sim_matrix[target_item, j] for j in history_items])
    num = (sims * history_strength).sum()
    den = np.abs(sims).sum() + eps
    return num / den

# user history
Iu = [1, 2]
ru = np.array([1.0, 1.0])
print(score_by_history(3, Iu, ru, S))
```

---

## 11) TL;DR
- Content‑based: рекомендуем по сходству айтемов в пространстве признаков → отлично для cold items и короткой истории.
- Hybrid: комбинируем CF + контент + user/context фичи.
- Линейные модели плохо ловят взаимодействия фич → нужны feature interactions.
- FM: добавляет попарные взаимодействия через эмбеддинги фич (эффективно по памяти).
- LightFM: пользователь/айтем = сумма эмбеддингов их фич → быстрый инференс и cold start.
