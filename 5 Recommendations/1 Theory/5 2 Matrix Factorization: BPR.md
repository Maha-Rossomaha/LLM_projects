# BPR (Bayesian Personalized Ranking): pairwise‑оптимизация для implicit рекомендаций

## 0) Контекст: зачем нужен BPR
В implicit‑сценариях (клики, просмотры, покупки) мы чаще знаем только **позитивные события**:
- пользователь **взаимодействовал** с айтемом `i` → есть сигнал,
- пользователь **не взаимодействовал** с айтемом `j` → это не обязательно “не нравится”, чаще это **unknown** (не видел / не дошёл / не было exposure).

Поэтому минимизация MSE по “0/1” или по рейтингам часто даёт не то, что нужно:
- модель учится подгонять абсолютные значения,
- а продукту обычно нужен **ранжированный список**.

**BPR** — классический подход: учим модель так, чтобы для каждого пользователя айтемы, с которыми он взаимодействовал, имели **более высокий скор**, чем невзаимодействованные.

---

## 1) Интуиция: учим относительные предпочтения
Для пользователя `u`:
- `i ∈ I_u` — айтем, с которым был позитивный сигнал,
- `j ∉ I_u` — айтем без взаимодействия (кандидат‑негатив).

Мы хотим:

$$
\hat{s}(u,i) > \hat{s}(u,j)
$$

То есть задача — не предсказать “рейтинг”, а **упорядочить**.

---

## 2) Модель скоринга (обычно MF)
Чаще всего BPR используют поверх матричной факторизации:

- эмбеддинг пользователя: `p_u ∈ ℝ^k`,
- эмбеддинг айтема: `q_i ∈ ℝ^k`.

Скор:

$$
\hat{s}(u,i) = p_u^\top q_i
$$

(Можно добавлять bias, MLP, контекст, но BPR‑идея от этого не меняется.)

---

## 3) Постановка BPR: вероятность предпочтения
BPR формулирует вероятность того, что пользователь `u` предпочитает `i` перед `j`:

$$
P(i \succ_u j) = \sigma\big(\hat{s}(u,i) - \hat{s}(u,j)\big)
$$

где `σ` — сигмоида:

$$
\sigma(x)=\frac{1}{1+e^{-x}}.
$$

Обозначим разницу скоров:

$$
\hat{\Delta}_{uij} = \hat{s}(u,i) - \hat{s}(u,j).
$$

Если $\hat{\Delta}_{uij}$ большая положительная → $P(i \succ j)$ близко к 1.

---

## 4) BPR‑loss (лог‑правдоподобие + регуляризация)
Мы хотим максимизировать вероятность правильного порядка для множества триплетов `(u,i,j)`:

- `u` — пользователь,
- `i` — позитивный айтем (из истории),
- `j` — негативный (не взаимодействовал).

### 4.1 Максимизация лог‑правдоподобия

$$
\max_{\Theta}\; \sum_{(u,i,j)\in \mathcal{D}} \ln\sigma\big(\hat{\Delta}_{uij}\big)
$$

Эквивалентно минимизации отрицательного лог‑правдоподобия (то, что обычно называют BPR‑loss):

$$
\mathcal{L}_{BPR} = -\sum_{(u,i,j)\in \mathcal{D}} \ln\sigma\big(\hat{\Delta}_{uij}\big)
$$

### 4.2 Регуляризация
Добавляем L2, чтобы эмбеддинги не раздувались:

$$
\mathcal{L} = -\sum_{(u,i,j)\in \mathcal{D}} \ln\sigma(\hat{\Delta}_{uij})
+ \lambda\,\|\Theta\|_2^2
$$

Где `Θ` — все параметры модели (например, `P,Q` в MF).

**Почему без регуляризации опасно**: можно делать `p_u` и `q_i` огромными, чтобы разницы скоров стали гигантскими и сигмоида “насытилась”.

---

## 5) Градиенты (для MF‑скоринга)
Для матричной факторизации:

$$
\hat{\Delta}_{uij} = p_u^\top q_i - p_u^\top q_j = p_u^\top (q_i-q_j)
$$

Удобно помнить производную:

$$
\frac{d}{dx}\big(-\ln\sigma(x)\big)= \sigma(-x)=1-\sigma(x).
$$

Обозначим:

$$
\gamma = \sigma(-\hat{\Delta}_{uij}).
$$

Тогда (без учёта регуляризации) градиенты:

$$
\frac{\partial \mathcal{L}}{\partial p_u} = \gamma\,(q_j-q_i),
$$

$$
\frac{\partial \mathcal{L}}{\partial q_i} = -\gamma\,p_u,
$$

$$
\frac{\partial \mathcal{L}}{\partial q_j} = \gamma\,p_u.
$$

Интуиция обновлений:
- если `i` недостаточно выше `j` (Δ маленькая) → `γ` большая → сильнее тянем `p_u` к `q_i` и отталкиваем от `q_j`.

С регуляризацией добавляются члены `+2λ p_u`, `+2λ q_i`, `+2λ q_j` в соответствующие градиенты.

---

## 6) Как собрать обучающую выборку триплетов (u,i,j)
### 6.1 Позитивы
`i` берём из истории пользователя:
- клики/покупки,
- просмотры (иногда с весами),
- любые события “пользователь проявил интерес”.

### 6.2 Негативы = negative sampling
Ключевой момент: `j` — это **не истинный негатив**, а “не было взаимодействия”.

Популярные стратегии:

1) **Uniform negative sampling**: `j` равновероятно из всех айтемов.
   - просто, но часто даёт слишком лёгкие негативы (явно нерелевантные).

2) **Popularity‑based sampling**: `P(j) ∝ pop(j)^α`.
   - ближе к реальности exposure (популярное чаще показывают),
   - делает задачу сложнее и часто улучшает метрики.

3) **In‑batch negatives** (для нейросетевых моделей/батчей):
   - позитивы других пользователей в батче считаются негативами для данного пользователя.
   - очень эффективно вычислительно.

4) **Hard negative mining**:
   - выбираем `j`, который модель уже считает высоким (сложный негатив).
   - полезно, но требует аккуратности (можно ловить “ложные негативы” — реально релевантные айтемы).

Практическое правило: качество BPR сильно зависит от того, **какие негативы** мы сэмплируем.

---

## 7) Связь с ranking‑метриками и что оптимизирует BPR
BPR оптимизирует вероятность корректного pairwise порядка. Это ближе к ранжированию, чем MSE.

- BPR не оптимизирует напрямую NDCG/Recall@K, но часто коррелирует.
- Особенно хорош в implicit, где абсолютный “рейтинг” не определён.

Интуиция: BPR “поднимает вверх” позитивы относительно отрицательных, что ведёт к росту качества top‑K.

---

## 8) Ограничения и подводные камни
1) **Exposure/MNAR остаётся**: “не кликнул” может значить “не видел”.
   - Популярити‑семплинг иногда частично приближает exposure, но не решает полностью.

2) **Ложные негативы**: среди `j` могут быть реально релевантные, просто не показанные.
   - усиливается при hard negatives.

3) **Popularity bias**:
   - MF+dot product и популярити‑семплинг могут ещё сильнее толкать модель в популярное.
   - помогают нормализация, регуляризация, дебайс‑семплинг, корректировка кандидатов.

4) **Стабильность обучения**:
   - сигмоида может насыщаться при больших Δ → градиенты маленькие.
   - лечится регуляризацией, правильным `lr`, инициализацией, контролем норм.

---

## 9) Мини‑пример на Python: BPR для MF (SGD)
Ниже учебная реализация: работает на маленьких данных, показывает механику.

```python
import numpy as np

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))


def train_bpr_mf(user_pos_items, n_users, n_items, k=32,
                 lr=0.05, reg=0.01, epochs=10, n_samples_per_user=50, seed=42,
                 neg_sampling="uniform", item_pop=None, pop_alpha=0.75):
    """
    user_pos_items: list of sets, user_pos_items[u] = {i1, i2, ...}
    neg_sampling: 'uniform' or 'pop'
    item_pop: array-like, popularity of items (needed for 'pop')

    обучаем MF с BPR-loss:
      L = -log(sigmoid(s(u,i)-s(u,j))) + reg*(||p_u||^2+||q_i||^2+||q_j||^2)
    """
    rng = np.random.default_rng(seed)
    P = 0.01 * rng.standard_normal((n_users, k))
    Q = 0.01 * rng.standard_normal((n_items, k))

    if neg_sampling == "pop":
        if item_pop is None:
            raise ValueError("item_pop is required for popularity sampling")
        probs = np.asarray(item_pop, dtype=float) ** pop_alpha
        probs = probs / probs.sum()

    all_items = np.arange(n_items)

    for _ in range(epochs):
        for u in range(n_users):
            pos = list(user_pos_items[u])
            if not pos:
                continue

            for _ in range(n_samples_per_user):
                i = pos[rng.integers(0, len(pos))]

                # sample negative j
                if neg_sampling == "uniform":
                    j = rng.integers(0, n_items)
                    while j in user_pos_items[u]:
                        j = rng.integers(0, n_items)
                else:  # pop
                    j = rng.choice(all_items, p=probs)
                    while j in user_pos_items[u]:
                        j = rng.choice(all_items, p=probs)

                # scores and delta
                x_ui = P[u] @ Q[i]
                x_uj = P[u] @ Q[j]
                delta = x_ui - x_uj

                # gamma = sigmoid(-delta) = 1 - sigmoid(delta)
                gamma = sigmoid(-delta)

                pu = P[u].copy()
                qi = Q[i].copy()
                qj = Q[j].copy()

                # gradient step (SGD)
                P[u] += lr * ( gamma * (qi - qj) - reg * pu )
                Q[i] += lr * ( gamma * pu        - reg * qi )
                Q[j] += lr * ( -gamma * pu       - reg * qj )

    return P, Q


# toy example
n_users, n_items = 3, 6
user_pos_items = [
    {1, 2},
    {2, 3, 4},
    {0}
]
P, Q = train_bpr_mf(user_pos_items, n_users, n_items, k=8, epochs=30)

# recommend for user 0
scores = Q @ P[0]
print('user0 top:', np.argsort(-scores)[:3], scores[np.argsort(-scores)[:3]])
```

Что проверить на игрушке:
- у пользователя `u` его позитивы должны быть выше случайных айтемов,
- top‑K должен вытягивать элементы из `I_u`.

---

## 10) Практические рекомендации
- Начни с **MF+BPR**, это сильный baseline для implicit.
- Негатив‑семплинг — ключевой рычаг качества.
  - попробуй uniform vs popularity,
  - затем in‑batch или hard negatives.
- Следи за **нормами эмбеддингов** и стабильностью (регуляризация, lr).
- Для retrieval‑стека сочетай BPR‑эмбеддинги с **ANN**.

---

## 11) TL;DR
- Implicit = много “unknown”, мало истинных негативов.
- BPR учит **порядок**, а не абсолютные рейтинги.
- Оптимизирует `-log σ(s(u,i)-s(u,j))` по триплетам `(u,i,j)`.
- Сильный baseline для top‑K ранжирования.
- Качество сильно зависит от **negative sampling** и контроля bias.