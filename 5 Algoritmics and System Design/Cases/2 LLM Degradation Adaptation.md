# Цикл переобучения и адаптации LLM в продакшене

## 1. Сбор сигналов деградации

### Метрики производительности:

* **Latency (p50/p95)** — рост задержки может указывать на рост сложности запросов или деградацию модели.
* **Throughput** — если падает, возможны проблемы с batching или рост вычислений на запрос.
* **Memory** — утечки, фрагментация, рост потребления.
* **Retrieval Hit Rate** — % запросов, где найдено хотя бы одно релевантное.
* **Embedding norm drift** — средняя длина эмбеддинга по времени.
* **Reranker Score Drift** — сдвиг в распределении оценок.

### Поведенческие сигналы (implicit feedback):

* **CTR**, scroll depth, time-on-answer — сигнал вовлечённости.
* **Повторные запросы** — сильный сигнал нерелевантности.

### Explicit feedback:

* Лайки / дизлайки / флажки «плохой ответ».
* Пользовательские жалобы и отзывы.
* A/B сравнение ответов (для сбора данных под DPO/KTO).

---

## 2. Обнаружение дрифта

### Embedding drift:

* `mean(cosine_similarity(q_emb, hist_q_emb))`
* Сдвиг центроидов кластеров
* PCA/UMAP визуализации

### Behavior drift:

* Расхождение с golden QA (e.g. BLEU, nDCG, ROUGE)
* Повторяющиеся жалобы по темам → кластеризация тем
* Увеличение числа hallucinations / unsafe ответов

---

## 3. Формирование новых выборок для обучения

### Источники:

* Логи: (query, clicked, skipped)
* Reranker: топовые и провальные примеры по score
* Ответы с фидбэком (positive/negative)
* A/B эксперименты и human annotation (если бюджет есть)

### Подготовка:

* Удаление дубликатов, нерелевантных / off-topic
* Балансировка классов и тематик
* Нормализация текста

---

## 4. Этапы и методы дообучения

| Компонент | Метод                | Данные                       |
| --------- | -------------------- | ---------------------------- |
| Retriever | LoRA / Adapter       | (query, pos\_doc, neg\_doc)  |
| Generator | LoRA-SFT / CPT       | (prompt, answer)             |
| Alignment | DPO / KTO / PPO      | (prompt, good vs bad answer) |
| Reranker  | Supervised fine-tune | (query, doc, rel\_score)     |

**Важно:**

* Везде, где возможно — использовать PEFT.
* Разделить retriever / генератор / reranker, дообучать независимо.

---

## 5. Триггеры и частота переобучения

### Запуск:

* Раз в неделю / месяц — планово
* При превышении порога: падение CTR, shift embedding norms, жалобы
* По ивенту: запуск нового домена, рост ошибок

### Безопасность:

* Canary prompts для контроля регрессий
* Shadow-модель до релиза
* Rollback механизм
* Cross-eval на golden сетах

---

## 6. Автоматизация и CI/CD

* Автоматический drift detection → создание новой ветки PEFT
* Shadow evaluation → если метрики > baseline → promote
* Ветка retraining может собираться и откатываться автоматически
* Использовать regression guardrails (например, MRR ↓ → rollback)

---

## Заключение

Ключ к устойчивости LLM в продакшене — это не редкие retrain’ы, а системный monitoring + PEFT loop + eval guardrails. Вся адаптация должна быть модульной, дешёвой и откатимой. В противном случае — ты кормишь чёрный ящик и надеешься, что он не захлебнётся.
