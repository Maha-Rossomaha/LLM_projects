# 7. Метрики и оценка качества uplift‑моделей

## 1. Зачем отдельные метрики для uplift

Классические метрики (AUC, logloss, accuracy) измеряют качество **предсказания исхода** $Y$, но uplift‑модель предсказывает **разность исходов** между treatment и control:

$$
\tau(x) = \mathbb E[Y(1) - Y(0) \mid X=x].
$$

Нас интересует не столько точность по $Y$, сколько:

- насколько хорошо модель **ранжирует** клиентов по полезности воздействия;
- какой **дополнительный эффект** (конверсии/доход/прибыль) она даёт по сравнению с:
  - рандомным таргетингом;
  - простыми Heuristic‑стратегиями (например, таргетинг по score вероятности $Y=1$).

Отсюда появляются uplift‑кривые, Qini, AUUC, policy value и т.п.

---

## 2. Базовая схема офлайн‑оценки uplift

Предположим, у нас есть **RCT‑эксперимент**:

- treatment $W \in \{0,1\}$ назначен случайно;
- есть исход $Y$ (обычно бинарный, но может быть и регрессионный);
- есть признаки $X$;
- есть предсказанный uplift‑скор $s_i = s(X_i)$ (чем больше, тем больше ожидаемый эффект).

Общая схема для большинства uplift‑метрик:

1. Посчитать $s_i$ на **отложенной выборке** (test) с уже известными $W,Y$.
2. Отсортировать объекты по $s_i$ по убыванию.
3. По префиксам (top‑q% или по бинам) сравнивать исходы между treated и control.
4. Строить кривые (uplift / Qini) и интегралы под ними.

Ключевой принцип: **никакого data leakage** – пороги и модели обучаем только на train, метрики считаем на holdout.

---

## 3. Uplift curve и AUUC (Area Under Uplift Curve)

### 3.1. Как строится uplift‑кривая

1. На тесте для каждого объекта есть $(X_i, W_i, Y_i, s_i)$.

2. Сортируем по $s_i$ по убыванию.

3. Делим отсортированный список на K бинов (например, K=10 – deciles; можно и по каждой точке).

4. Для каждого бина $k$:

   - $n_{1k}$: количество treated;
   - $n_{0k}$: количество control;
   - $\bar Y_{1k}$: средний исход среди treated;
   - $\bar Y_{0k}$: средний исход среди control.

5. Оценка uplift в бине:

   $$
   \hat u_k = \bar Y_{1k} - \bar Y_{0k}.  
   $$

   (иногда используют скорректированные формулы с масштабированием по размеру групп, см. Qini ниже).

6. Строим кумулятивную кривую: по оси X – доля таргетируемых клиентов ($q$), по оси Y – суммарный инкрементальный эффект в top‑q:

   $$
   U(q_m) = \sum_{k=1}^m \Delta_k,
   $$

   где $q_m$ – доля объектов в первых $m$ бинах, $\Delta_k$ – оценка дополнительного числа откликов в бине k.

**Важно:** кривую обычно строят не по $\hat u_k$, а по **кумулятивному инкременту** (см. Qini‑конструкцию), но интуиция везде одна и та же: чем выше кривая – тем лучше ранжирование.

---

### 3.2. AUUC – интеграл под uplift‑кривой

**AUUC (Area Under the Uplift Curve)** – аналог AUC, но для uplift:

- берём uplift‑кривую $U(q)$;
- численно интегрируем по $q \in [0,1]$ (например, методом трапеций);
- чем больше AUUC – тем лучше модель.

Проблема: абсолютное значение AUUC зависит от:

- масштаба исхода $Y$;
- размеров treatment/control;
- общей конверсии.

Поэтому AUUC часто используют **в относительном сравнении** (модель A vs B, baseline vs модель) или нормируют (см. Qini coefficient).

---

## 4. Qini curve и Qini coefficient

### 4.1. Инкрементальный отклик в бине

Классическая Qini‑логика: в каждой группе сравнивать treated с **перемасштабированным control**, чтобы учесть разные размеры групп.

Пусть в бине k:

- $n_{1k}$, $n_{0k}$ – размеры treated/control;
- $y_{1k}$, $y_{0k}$ – число исходов (например, конверсий) в каждой группе.

Тогда оценка **инкрементального числа откликов** в бине:

$$
\Delta_k = y_{1k} - y_{0k} \cdot \frac{n_{1k}}{n_{0k}}.
$$

Интерпретация:

- $y_{0k} (n_{1k}/n_{0k})$ – ожидаемое число откликов в treated, если бы в бине действовал **control‑мир**;
- разность – сколько откликов мы получили «сверх» за счёт treatment.

### 4.2. Qini‑кривая

1. Сортируем по uplift‑скору $s_i$ по убыванию.
2. Разбиваем на бины, как выше.
3. Для каждого бина считаем $\Delta_k$.
4. Строим **кумулятивную Qini‑кривую**:
   $$
   Q(m) = \sum_{k=1}^m \Delta_k,
   $$
   по оси X – доля охвата (сколько клиентов мы таргетируем, взяв первые m бинов), по оси Y – суммарный инкрементальный отклик.

### 4.3. Qini coefficient

**Qini coefficient** – это, грубо, "площадь между кривыми":

1. Строим Qini‑кривую нашей модели $Q_{model}(q)$.
2. Строим Qini‑кривую для **рандомного таргетинга** (baseline). У рандома инкремент распределён более‑менее линейно, кривая ближе к прямой.
3. Qini‑коэффициент:
   $$
   Qini = \int_0^1 \big( Q_{model}(q) - Q_{random}(q) \big) \, dq.
   $$

На практике:

- считаем дискретный аналог (сумма по бинам);
- иногда дополнительно делят на Qini "идеального" оракула и получают **нормированный Qini** $\in [0,1]$.

**Чем больше Qini – тем сильнее модель выигрывает у рандома по инкрементальному отклику.**

---

### 4.4. Мини‑пример (табличный)

Воображаемый пример: 3 бина (по 100 клиентов), конверсии:

| Бин | n1 | y1 | n0 | y0 |
| --- | -- | -- | -- | -- |
| 1   | 60 | 18 | 40 | 4  |
| 2   | 50 | 10 | 50 | 5  |
| 3   | 40 | 4  | 60 | 3  |

Посчитаем $\Delta_k$:

- Бин 1: $\Delta_1 = 18 - 4 \cdot (60/40) = 18 - 6 = 12$;
- Бин 2: $\Delta_2 = 10 - 5 \cdot (50/50) = 10 - 5 = 5$;
- Бин 3: $\Delta_3 = 4 - 3 \cdot (40/60) = 4 - 2 = 2$ (округляя).

Кумулятивный Qini:

- Top 1 бин: $Q(1) = 12$;
- Top 2 бина: $Q(2) = 12+5 = 17$;
- Top 3 бина: $Q(3) = 19$.

Сравниваем это с рандомом (примерно линейный рост от 0 до 19). Qini‑коэффициент – площадь между этими кривыми.

---

**$\Delta CR$** – самая простая метрика для интерпретации:

1. Выбираем top‑q% клиентов по uplift‑скору.

2. Смотрим **фактическую** конверсию среди treated и control в этом сегменте:

   $$
   CR_1(q) = \frac{\sum_{i \in Top(q), W_i=1} Y_i}{\sum_{i \in Top(q)} \mathbb I\{W_i=1\}}, \quad
   CR_0(q) = \frac{\sum_{i \in Top(q), W_i=0} Y_i}{\sum_{i \in Top(q)} \mathbb I\{W_i=0\}}.
   $$

3. Разность:

   $$
   \Delta CR(q) = CR_1(q) - CR_0(q).
   $$

Интерпретация: «Если мы таргетируем только top‑q% по uplift‑скору, насколько treatment повышает конверсию в этом сегменте?»

Можно строить график $\Delta CR(q)$ по $q$ и смотреть, как ведёт себя локальный uplift.

Плюсы:

- очень понятная бизнесу метрика;
- удобно смотреть локальный эффект при разных охватах.

Минусы:

- учитывает только выбранный $q$, не всю кривую;
- чувствительна к шуму при маленьких выборках.

---

## 6. Policy Value / Expected Profit

Uplift‑модель – это не самоцель, а способ выбрать **оптимальную политику таргетинга** $\pi(x)$:

- $\pi(x)=1$ – таргетируем;
- $\pi(x)=0$ – не трогаем.

**Policy value** – ожидаемое значение целевой функции (конверсии / прибыли / NPV) при применении политики $\pi$ на популяции.

### 6.1. Определение

Пусть $Y(w)$ – исход при treatment $w$, и есть cost/доходы:

- $Gain(Y,w,x)$ – прибыль от исхода $Y$ при режиме $w$ и признаках $x$ (сюда можно включить cost контакта, скидку и маржу).

Policy value:

$$
V(\pi) = \mathbb E\big[ Gain(Y(\pi(X)), \pi(X), X) \big].
$$

В RCT‑данных можно оценить $V(\pi)$ офлайн.

### 6.2. Оценка policy value в RCT

Если treatment рандомизирован, можно использовать **inverse propensity weighting**. Пусть $p = P(W=1)$ – доля treated.

Оценка:

$$
\hat V(\pi) = \frac{1}{n} \sum_{i=1}^n \left[ \frac{\mathbb I\{\pi(X_i)=1, W_i=1\}}{p} Gain(Y_i,1,X_i)
+ \frac{\mathbb I\{\pi(X_i)=0, W_i=0\}}{1-p} Gain(Y_i,0,X_i) \right].
$$

Интуиция:

- мы перескладываем наблюдательные исходы так, как если бы использовали политику $\pi$;
- веса $1/p$ и $1/(1-p)$ компенсируют рандомизацию treatment.

На практике:

1. Фиксируем порог по uplift‑скор $t$: $\pi_t(x) = \mathbb I\{s(x) \ge t\}$.
2. Для разных $t$ считаем $\hat V(\pi_t)$.
3. Строим график "Expected profit vs охват" и выбираем $t$, максимизирующий прибыль.

---

## 7. Split‑based evaluation: holdouts и cross‑validation

### 7.1. Treatment / control holdouts

Классическая схема:

1. Есть один эксперимент (RCT) с treatment/control.
2. Делим клиентов на train/test (например, 70/30) **до** любой обработки.
3. На train:
   - обучаем uplift‑модель;
   - подбираем гиперпараметры;
   - можем настраивать порог по policy value/$\Delta CR$.
4. На test:
   - фиксируем модель и все пороги;
   - считаем uplift‑кривые, AUUC, Qini, policy value;
   - сравниваем разные модели.

Важно:

- сплит должен быть **по юнитам**, не по treatment/control отдельно;
- распределение $W$ и $Y$ на test должно отражать реальную популяцию.

### 7.2. Cross‑validation в uplift‑задачах

CV для uplift похож на обычный, но есть нюансы:

1. Разбиваем данные на K фолдов по юнитам.
2. Для каждого фолда k:
   - обучаем модель на остальных K-1 фолдах;
   - считаем uplift‑метрики (Qini/AUUC/policy value) на фолде k.
3. Усредняем метрики по фолдам.

Особенности:

- **внутри каждого фолда** должны быть и treated, и control;
- маятник разреженности: если фолды маленькие, Qini может быть очень шумным;
- часто практичнее делать не K=10, а 3–5 фолдов или 2–3 большие bootstrap‑репликации.

**Ключевое:** не смешивать train и test при расчёте uplift‑метрик.

---

## 8. Симуляции (synthetic uplift data)

Симуляции полезны, когда:

- нужно проверить корректность реализации метрик и пайплайнов;
- сравниваем несколько архитектур uplift и хотим знать, какая лучше при заданной "истинной" модели;
- хотим изучить поведение метрик при разных уровнях шума, дисбаланса и т.п.

### 8.1. Схема генерации синтетических данных

Простейшая симуляция:

1. Генерируем признаки $X$:

   $$
   X \sim f_X, \quad X \in \mathbb R^d.
   $$

2. Задаём истинный uplift $\tau^*(x)$, например:

   $$
   \tau^*(x) = 0.4 \cdot \mathbb I\{x_1 > 0\} - 0.2 \cdot \mathbb I\{x_2 < -1\}.
   $$

3. Задаём базовый отклик в control:

   $$
   \mu_0(x) = \sigma(\beta^T x), \quad \sigma(z) = \frac{1}{1+e^{-z}}.
   $$

4. Определяем отклик в treatment:

   $$
   \mu_1(x) = \sigma( \beta^T x + g(x)),
   $$

   где $g(x)$ выбираем так, чтобы $\mu_1(x) - \mu_0(x)$ согласовывался с $\tau^*(x)$.

5. Treatment (для RCT):

   $$
   W \sim \mathrm{Bernoulli}(p).
   $$

6. Исход:

   $$
   Y \sim \mathrm{Bernoulli}(\mu_W(X)).
   $$

В таком мире мы знаем истинный $\tau^*(x)$ и можем:

- обучать различные uplift‑модели;
- сравнивать **MSE(****$\hat \tau$****)** с истинной $\tau^*$;
- сравнивать Qini/AUUC/policy value;
- смотреть, какие метрики лучше коррелируют с реальным качеством.

### 8.2. Мини‑скелет симуляции на Python

```python
import numpy as np

np.random.seed(0)

n = 10000
X = np.random.normal(size=(n, 2))

# истинный uplift
true_tau = 0.4 * (X[:, 0] > 0) - 0.2 * (X[:, 1] < -1)

# базовый логит
beta = np.array([0.5, -0.3])
logit_mu0 = X @ beta

# treatment логит (добавляем uplift как сдвиг)
logit_mu1 = logit_mu0 + true_tau

mu0 = 1 / (1 + np.exp(-logit_mu0))
mu1 = 1 / (1 + np.exp(-logit_mu1))

p_treat = 0.5
W = np.random.binomial(1, p_treat, size=n)

Y = np.where(W == 1,
             np.random.binomial(1, mu1),
             np.random.binomial(1, mu0))

# Здесь можно обучать uplift-модель, считать Qini, AUUC и сравнивать с true_tau
```

---

## 9. Практические замечания и типичные ошибки

1. **Оценивать uplift по train** – грубый leak. Всегда нужен отдельный holdout или честный CV.
2. **Сравнивать модели по AUC/ROC вместо uplift‑метрик.** Модель, хорошо предсказывающая $Y$, может быть плохой uplift‑моделью, если она одинаково классифицирует и treated, и control.
3. **Игнорировать дисбаланс treatment/control** при расчёте Qini/uplift‑кривых.
4. **Неправильно построенный baseline (random)** – приводит к некорректной интерпретации AUUC/Qini.
5. **Малые выборки/сегменты** – сильно шумные Qini‑кривые. Желательно иметь достаточно наблюдений в каждом бине.

---

## 10. Краткий чек‑лист по оценке uplift‑модели

1. Есть ли **корректный RCT** или хорошо скорректированные наблюдательные данные?
2. Разделены ли **train/test** (или CV‑фолды) по юнитам?
3. Для каждой модели посчитаны:
   - Qini‑кривая и Qini‑коэффициент;
   - AUUC;
   - $\Delta CR(q)$ для релевантных уровней охвата;
   - policy value (ожидаемая прибыль) при оптимальном или фиксированном пороге.
4. Есть ли **baseline** (random, score‑based таргетинг), с которым сравниваем модель?
5. Проведены ли **симуляции** или хотя бы sanity‑checks (например, модель без X даёт почти плоский uplift)?

Если на все эти пункты можно честно ответить "да", то оценка качества uplift‑модели, скорее всего, достаточно надёжна и интерпретируема для практического использования.

