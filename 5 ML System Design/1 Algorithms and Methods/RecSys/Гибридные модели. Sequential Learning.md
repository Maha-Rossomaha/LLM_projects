---
Owner: IIvan Vashchenko
---
> [!important] 
> В основном про гибридные модели рекомендаций - SVDFeature, связь с LightFM. Также более подробно про факторизационные машины.

# Гибридные модели рекомендаций

Проблема редких взаимодействий - когда нет пересечений у похожих в реальности клиентов.

Три ключевых сценария “холодного старта”:

- по пользователям
- по товарам
- системный холодный старт

Решение - использовать признаковое описание (в любом виде) товара и пользователя.

**Гибридные рекомендательные системы** - комбинация content-based и collaborative models. Контент = признаки.

![[image 16.png|image 16.png]]

Стоит понимать, что при линейной регрессии фичи пользователя не влияют на задачу _top-n._

Лучше декартово произведение $z = x \times y$, значит $r = x^TWy$, $\large W \in R^{m_x \times n_y}$

![[image 1 8.png|image 1 8.png]]

Проблема поставленной задачи - разные товары и пользователи могут содержать одинаковые факторы. Решение: использование поведенческой информации.

## **SVDFeature = LightFM**

**SVDFeature** — это расширение классической матричной факторизации, которое учитывает не только латентные факторы пользователей и объектов, но и разнообразные дополнительные признаки (фичи), включая excplicit и implicit характеристики, что улучшает качество рекомендаций за счёт более богатого описания контекста. В **SVDFeature** можно рассматривать как мощное обобщение матричной факторизации, объединяющее идею латентных факторов с богатым набором признаков, что значительно улучшает гибкость и качество модели при условии грамотной обработки данных.

### **Предпосылки и мотивация**

Классические рекомендательные модели, такие как простая матричная факторизация (MF), представляют пользователя и объект (товар, статью, фильм) в виде векторов латентных факторов. Тем не менее, такие модели часто слишком «голые»: они не учитывают полноту контекстной информации, такую как дополнительные признаки пользователей (например, демография), объекты (например, жанр фильма, цена товара), а также признаки взаимодействия (например, временной контекст, расположение).

SVDFeature была предложена как метод расширения факторизации матрицы, который позволяет представить любую информацию о пользователях, объектах и их взаимодействиях в формате признаков и включить их в факторизационную модель. Это особенно полезно в ситуациях, когда нужно интегрировать разнообразные источники данных помимо простого «пользователь-объект» рейтинга.

### **Основная идея SVDFeature**

В традиционной матричной факторизации мы пытаемся аппроксимировать рейтинг $r_{u,i}$ пользователя $u$ к объекту $i$ через произведение латентных векторов:

$r_{u,i} \approx b_0 + g_u + f_i + x_u^T P Q^T y_i$

где:

- $b_0$ — глобальный сдвиг (bias);
- $g_u$ — вектор факторов или смещений, относящийся к пользователю $u$;
- $f_i$ — вектор факторов или смещений, относящийся к объекту $i$;
- $x_u$, $y_i$ — латентные представления пользователя и объекта;
- $P, Q$ — матрицы преобразований, связывающие признаковые пространства.

В SVDFeature же мы рассматриваем более общий формат:

$r = b_0 + g^T x + f^T y + x^T P Q^T y$

но при этом вектора $x$ и $y$ могут содержать не только латентные факторы, но и реальные заранее вычисленные признаки, например:

- Признаки пользователя: возраст, пол, история просмотров, результаты поисковых запросов и пр.
- Признаки объекта: категория, метаданные, текстовые описания, визуальные признаки (если речь о картинках) и др.
- Признаки взаимодействия: время суток, устройство, контекст использования.

Таким образом, $x$ и $y$ в SVDFeature — это не просто скрытые факторы, а конкатенация или иная форма представления множества явно заданных признаков, преобразованных при помощи факторизации.

**Формула в терминах признаков**:

Пусть у нас есть вектор признаков для пользователя (или сессии) $X = [X_1, X_2, \ldots, X_m]$ и для объекта $Y = [Y_1, Y_2, \ldots, Y_n]$. Тогда:

$r = b_0 + g^T X + f^T Y + X^T P Q^T Y.$

- $b_0$ — глобальный бейзлайн, вычисленный по всему датасету.
- $g$ и $f$ — вектора параметров (модельных коэффициентов), соответствующих признакам пользователя и объекта соответственно.
- $P$ и $Q$ — матрицы перехода, которые позволяют улавливать взаимодействия между признаками пользователей и объектов.

Если вы разложите $X$ и $Y$ на компонентные признаки (например, один признак может быть «пользователь: женский пол», другой — «объект: жанр боевик»), тогда матрицы $P$ и $Q$ позволяют моделировать, как различные сочетания признаков влияют на итоговый рейтинг.

**Оптимизация и обучение модели**:

Параметры $\Theta = \{g, f, P, Q\}$ можно обучать с помощью различных методов: ALS (Alternating Least Squares), SGD (Stochastic Gradient Descent) или методов, основанных на байесовском персонализированном ранжировании (BPR), если задача ранжирования важнее, чем точная предсказательная оценка.

Перед обучением можно предварительно вычислить некоторые сдвиги, например $b_0$ (средний рейтинг), чтобы упростить задачу для основной факторизации.

### **Плюсы SVDFeature**

- **Гибкость**: Позволяет использовать разнообразные признаки пользователей и объектов, а также контекстные признаки.
- **Улучшение качества рекомендаций**: За счёт более богатого описания каждый рейтинг не сводится только к взаимодействию «пользователь-объект», а учитывает дополнительные сигналы.
- **Универсальность**: Модель можно адаптировать под разные типы данных и задач: рекомендации товаров, фильмов, объявлений, новостей и т.д.

### **Минусы SVDFeature**

- **Сложность моделирования**: Добавление множества признаков увеличивает размерность и сложность модели, что может привести к росту вычислительных затрат.
- **Необходимость в инженерии признаков**: Требуется осмысленно выбирать и конструировать признаки. Если признаки нерелевантны или шумны, модель может не достичь улучшения качества.
- **Переобучение**: При большом количестве параметров и признаков возрастает риск переобучения, требуется аккуратная регуляризация.

### Аналогия с LightFM

**LightFM** не является прямой реализацией **SVDFeature**, но концептуально очень близок: он также моделирует взаимодействия между пользователями и объектами с учётом дополнительных признаков и оптимизирует параметры через SGD. Однако LightFM имеет свои отличия в реализации потерь, функциональных форм и механизмов регуляризации.

**LightFM** — это библиотека и модель, которая сочетает в себе преимущества классических факторизационных моделей с возможностью использовать дополнительные признаки пользователей и объектов, подобно **SVDFeature**. Основная идея **LightFM** состоит в том, чтобы уметь учитывать любые бинарные признаки (например, «_пользователь: принадлежит к группе X_», «_объект: относится к категории Y_») в дополнение к традиционной парадигме «_пользователь—объект_».

**SVDFeature** предложен как более общий фреймворк для факторизации, где входные данные выражаются в форме признаков, и модель учится отображать эти признаки в совместное латентное пространство. LightFM концептуально очень близок к этой идее:

- Он принимает на вход признаки пользователей и объектов.
- Он оптимизируется с помощью стохастического градиентного спуска (SGD).
- Он может работать со специфичными функциями потерь (логистическая, BPR, WARP), что отражает похожую гибкость, как и у SVDFeature, где можно было применять ALS, SGD или BPR.

При этом LightFM не заявлен как прямая или полная реализация SVDFeature. Он был создан с учётом идей гибридных рекомендательных моделей, опираясь на различные источники, включая идеи MF с признаками, но с акцентом на простоту использования в прикладных задачах. В частности:

- **LightFM** часто применяется для задач ранжирования и рекомендаций списков топ-N.
- В **SVDFeature** формула, упомянутая ранее, детализирует явный вид параметров (например, $x^T P Q^T y$) и структуры признаков. **LightFM** же более абстрактен: он «вклеивает» бинарные признаки в вектора эмбеддингов, оптимизируя их совместно с параметрами модели.
- В **LightFM** акцент сделан на гибких функциях потерь, предназначенных для оптимизации качества ранжирования (BPR, WARP), в то время как SVDFeature изначально презентуется как общий фреймворк без упора на конкретную задачу оптимизации.

**Вывод**: Хотя LightFM и SVDFeature очень похожи по философии — это использование факторизации с дополнительными признаками для улучшения рекомендательных моделей — назвать LightFM точной реализацией SVDFeature некорректно. Скорее, LightFM можно считать одним из фреймворков, вдохновлённых идеями SVDFeature, но имеющим свою специфику в плане реализации функций потерь и некоторых деталей обучения.

---

## Factorization Machines

![[image 2 7.png|image 2 7.png]]

Факторизационные машины (Factorization Machines, FM) — это обобщённая модель, способная учитывать взаимодействия между множеством разнообразных признаков (фич) пользователей, объектов и контекста. Они особенно хорошо работают с разреженными данными, умеют моделировать нелинейные взаимоотношения без ручной инженерии фич и удобны для гибридных рекомендаций.

### **Идея и мотивация**

![[image 3 7.png|image 3 7.png]]

Часто в рекомендательных задачах (и не только) у нас есть много различных признаков:

- Характеристики пользователя (возраст, пол, предпочтения, история кликов)
- Атрибуты объекта (категория товара, жанр фильма, цена)
- Контекстные факторы (время суток, устройство, геолокация)

Факторизационные машины призваны упростить учет всех этих факторов одновременно, в том числе их взаимодействий. В отличие от линейных моделей, где взаимодействия между признаками нужно задавать вручную, FM «учатся» выявлять их сами.

Представьте, что у нас есть признак «пользователь — любит комедии» и признак «фильм — это комедия». Интуитивно их совместное наличие влияет на рейтинг или вероятность просмотра. В простой линейной модели нам пришлось бы явно добавить фичу «пользователь_любит_комедии × фильм_комедия», чтобы модель учитывала эту связку. Факторизационные машины делают это автоматически, вычленяя взаимодействия из скрытых факторов.

FM вводят для каждого признака не просто один вес (как в линейной модели), а скрытый вектор факторов. Этот вектор можно понимать как «описание» признака в некотором латентном пространстве. Когда мы рассматриваем пару признаков, FM вычисляет их взаимодействие через «скалярные произведения» этих латентных векторов. Таким образом, не нужно вручную перебирать все пары признаков, модель сама выявляет важные взаимодействия.

![[image 4 7.png|image 4 7.png]]

### **Преимущества FM**:

- **Обобщаемость**: Работают не только для задач рекомендаций, но и для любых предиктивных задач с разреженными, категориальными или гибридными данными.
- **Автоматическое выявление взаимодействий**: Нет необходимости вручную генерировать новые признаки-произведения, FM сами учатся, какие взаимодействия важны.
- **Подходят для разреженных данных**: Часто в рекомендациях у нас много признаков, но для конкретного пользователя-объекта большинство из них отсутствуют. FM хорошо справляются с этой разреженностью.
- **Гибкость**: Можно использовать разные функции потерь (логистическая, регрессионная, ранжирующая) и расширять FM до более сложных архитектур.

### **Связь с матричной факторизацией и применение в рекомендациях**

![[image 5 7.png|image 5 7.png]]

В LightFM отстустует $V_x V_{fx}^T$ и $V_y V_{fy}^T$, то есть нам незачем иметь информацию о том как фичи выражают товар, так как они уже его выражают. В FM можно добавить “контекст”, в LightFM непонятно куда. Можно с помощью бинарных индикаторов игнорировать некоторые interactions.

Factorization Machines (FM) представлены как общая модель, которая учитывает взаимодействия между всеми признаками (пользовательскими, объектными и дополнительными) через матрицу факторов. LightFM использует похожую идею факторизации, но вместо того, чтобы явно представлять и перемножать все признаки в едином огромном пространстве, фактически складывает векторы признаков users и items (включая их атрибуты) и потом берёт скалярное произведение уже суммарных векторов. В итоге LightFM также реализует идею второго порядка, но делает это проще и более явно, чем классические FM.

**Различия и сходства с опорой на сккрин**

1. **FM**
    
    Видно, что Factorization Machines строят большую матрицу взаимодействий между всеми типами признаков (пользователями, объектами, их атрибутами, контекстными фичами). FM рассматривают все взаимодействия через общую матрицу $H$ или факторы $V$. Это значит, что FM изначально задумываются как общий подход, способный учесть любые взаимодействия между любыми признаками.
    
    Верхняя часть слайда (с надписью FM) показывает матрицу $H$ (или $VV^T$), в которой представлены взаимодействия между всеми наборами признаков (пользовательские, объектные, атрибутивные), обозначенные кружками разных цветов. Так FM моделируют любые пары признаков, включая «пользователь-пользовательские атрибуты», «пользователь-объект», «пользователь-объектные атрибуты» и так далее.
    
2. **LightFM**
    
    В нижней части слайда приводится формула LightFM, где мы видим выражение вида:
    
    $(V_x + V_{fx})^T (V_y + V_{fy})$
    
    Это означает, что в LightFM мы берём вектора пользователя ($V_x$) и его атрибутов ($V_{fx}$), складываем их в единый вектор, и делаем то же самое для объекта ($V_y$) и его атрибутов ($V_{fy}$). Затем просто считаем их скалярное произведение.
    
    Таким образом LightFM упрощает схему, превращая все признаки, связанные с пользователем, в один суммарный вектор, и все признаки, связанные с объектом, тоже в один суммарный вектор. Взаимодействия внутри этих групп признаков оказываются уже «заложенными» в результирующие вектора за счёт обучения этих эмбеддингов, а для вычисления итогового предсказания остаётся лишь пересечь эти «суммарные» представления.
    
3. **Сходство**
    - И FM, и LightFM используют факторизацию (латентные вектора) для моделирования взаимодействий между признаками. Они по сути опираются на одну и ту же идею — представить признаки в латентном пространстве.
    - Оба подхода не требуют ручного создания всех пар взаимодействующих признаков. Модель сама учится тому, какие взаимодействия важны.
4. **Различия**
    - **Явность представления взаимодействий**
        
        - FM в общем виде рассматривают полную матрицу взаимосвязей между признаками (как показано на слайде в виде матрицы $H$).
        - LightFM, в отличие от этого, просто складывает векторы относящихся к пользователю признаков и векторы относящихся к объекту признаков, а затем пересекает эти агрегированные вектора.
        
        Таким образом, LightFM более явно разделяет признаки на «пользовательские» и «объектные» и не пытается единым махом учесть все возможные пары, а фактически строит взаимодействие через сумму латентных представлений.
        
    - **Простота вычислений**: LightFM проще в реализации, потому что вместо перебора всех пар и формирования большой матрицы взаимодействий (как в FM), она фактически сводит задачу к простому сложению нескольких векторов и их скалярному произведению.
    - **Гранулярность**: FM формально может учесть взаимодействия между любыми фичами в одном шаге. LightFM упрощает этот процесс, но может быть немного более ограничен в том, как именно моделировать сложные взаимосвязи между любыми признаками.

  

---

## Field-aware FM

[![](https://habrastorage.org/r/w1560/getpro/habr/upload_files/2d3/343/e5d/2d3343e5d4fe666fa6bb45b469d82af0.png)](https://habrastorage.org/r/w1560/getpro/habr/upload_files/2d3/343/e5d/2d3343e5d4fe666fa6bb45b469d82af0.png)

Идея [FFM - Field-aware Factorization Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf): разбить признаки из матрицы  на группы **:**Тогда модель FFM выглядит как:

![[image 6 7.png|image 6 7.png]]

В остальном, как и в FM.

---

## Аналитическое решение гибридных моделей рекомендаций

![[image 7 6.png|image 7 6.png]]

---

## Эволюция алгоритмов матричной факторизации

Коллаборативная фильтрация:

- Weighted MF (2007)
- PureSVD (2010)
- SLIM (2011)
- Closed-form (2019) - EASER

**Гибридные подходы:**

- Coupled Factorization( 2008)
- Factorization Machines (2010)
- HybridSVD (2018)
- Closed-from (2020)

---

  

# Sequence-aware Learning

Задача - предсказать действия пользователя на основании предыдущих действий. Порядок важен.

Типы моделей:

- **Sequential** - набор независимых товаров
- **Session-based** - последовательность корзин, но сессии анонимные
- **Session-aware** - последовательность корзин, сессии не анонимные

![[image 8 5.png|image 8 5.png]]

Вообще порядок можно начинать учитывать и в базовых моделях (pairCount, Sequential kNN).

[https://github.com/rn5l/session-rec/tree/master/algorithms](https://github.com/rn5l/session-rec/tree/master/algorithms) - simple sequential baselines (session-based). Есть датасеты, на которых базовые sequential подходы обгоняют нейросетевые.