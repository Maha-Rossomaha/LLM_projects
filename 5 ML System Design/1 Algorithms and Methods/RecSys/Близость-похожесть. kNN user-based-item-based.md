---
Owner: IIvan Vashchenko
---
> [!important] 
> Модели на основе понятия близости/похожести объектов. Продвинутые модели на основе гибридных подходов. Холодный старт. Модели с учетом контекста и тензорные методы.

# Nearest Neighbors Models

![[image 12.png|image 12.png]]

## kNN-based approach

**Ключевые преимущества**

1. **Легкость реализации**
    - Алгоритмы kNN интуитивно понятны и не требуют сложных моделей.
    - Не нужны предварительные этапы обучения — используется непосредственное вычисление сходств.
2. **Интуитивные объяснения**
    - Результаты kNN легко интерпретировать: рекомендации строятся на основе похожих пользователей (user-based) или объектов (item-based).
3. **Хорошая базовая модель**
    - kNN часто используется как отправная точка (baseline) в рекомендательных системах для сравнения с более сложными методами, такими как матричная факторизация или нейронные сети.
4. **Подходит для разреженных данных**
    
    - kNN хорошо справляется с разреженностью матрицы взаимодействий, так как обрабатывает только пересекающиеся оценки.
    
      
    

**Масштабируемость**

1. **Временная сложность в худшем случае**
    - Для **user-based kNN**: $O(MN^2)$ где $M$ — количество пользователей, $N$ — количество объектов.
        - Вычисление сходства между всеми пользователями требует сравнения каждой пары.
    - Для **item-based kNN**: $O(M^2N)$ где вычисляются сходства между всеми парами объектов.
2. **Сложность хранения в худшем случае**
    - В обоих подходах $O(M^2)$ или $O(N^2)$:
        - Нужно хранить либо матрицу схожести пользователей (user-user similarity), либо объектов (item-item similarity).
3. **Разреженность снижает реальную сложность**
    - Благодаря разреженности матрицы взаимодействий фактическая сложность ниже, так как рассматриваются только пары с пересекающимися оценками.
4. **Можно ограничить количество соседей**
    - Для ускорения и снижения сложности можно ограничить число соседей ($k$) для каждого пользователя или объекта (например, учитывать только $k$-ближайших).
5. **Инкрементальные обновления иногда возможны**
    - После добавления новых данных не всегда нужно пересчитывать все сходства с нуля — можно обновить только затронутые части.

---

### Items-based approach

Аггрегированная информация от похожих товаров (_**легче объяснять**_)

$$\Large\text{score}_{iKNN}(u,i) = \text{agg}_{j \in N_u(i)} a_{uj}$$

где $N_u(i)$ - все товары, которые оценил пользователь $u$ (как и $i$).

  

### User-based approach

Аггрегированная информация от похожих пользователей _**(повышает serendipity)**_

$$\Large\text{score}_{uKNN}(u,i) = \text{agg}_{v \in N_i(u)} a_{vi}$$

где $N_i(u)$ - все users, кто оценил товар $i$ (как и $u$).

**Simple user-based kNN:**

![[image 1 5.png|image 1 5.png]]

---

### **Improved user-based kNN**

Ищем некоторую меру “похожести” - пересечения по интересам, чтобы взвешивать оценки разных пользователей. Все еще остается большое окружение = высокая вычислительная сложность .

![[image 2 4.png|image 2 4.png]]

А почему выч сложность? Потому что например нам нужно хранить матрицу схожести $\text{sim}(u, v)$. Решения:

- можно вычислять “на лету”, тогда функция похожести должна быть простой
- “агрессивное сэмплирование” - например если соседей больше N, то ограничиваем
    - рандомно
    - фильтруем по времени (most recent only)
        
        ![[image 3 4.png|image 3 4.png]]
        
    - most ratings in common
- Приближенные методы ближайших соседей ([NMSLib](https://github.com/nmslib/nmslib), [[Приближенный поиск ближайших соседей]], [Faiss](https://github.com/facebookresearch/faiss), [Annoy](https://github.com/spotify/annoy))
- Методы сжатия пространства

---

### **Centered kNN**

![[image 4 4.png|image 4 4.png]]

На практике может оказаться бесполезным.

---

### kNN matrix form

![[image 5 4.png|image 5 4.png]]

|   |   |   |
|---|---|---|
|**Параметр**|**Element-wise weighting**|**Row-wise/no weighting**|
|**Сфокусированность**|Фильтрация известных данных|Предполагается $0$-замещение|
|**Лучшее применение**|Прогнозирование оценок (rating-based)|Топ-$N$ рекомендации|
|**Точность обработки данных**|Учитываются только известные оценки|Упрощенная нормализация|

**Общее понимание:**

- **User-based**: Схожесть пользователей определяет рекомендации.
- **Item-based**: Схожесть объектов определяет рекомендации.
- **Element-wise**: Используется для прогноза точных значений оценок (на ранжирование не влияет, так как взвешивание за суммой).
- **Row-wise**: Используется для ранжирования рекомендаций.

  

**Element-wise weighting for user-based kNN**

- Фильтруются только **известные оценки** (учитываются реальные данные).
- Это лучше для **прогнозирования конкретных оценок**.

$$\large r_{ui} = \frac{1}{z_{ui}} \sum_{v \in N_i(u)}{\text{sim}(u,v)a_{vi}}$$

где:

- $N_i(u) = U_i \setminus \{u\}$ - пользователи, провзаимодействовашие с товаром $i$, кроме $u$
- $\large z_{ui} = \sum_{v \in N_i(u)}{\text{sim}(u,v)}$

Пусть $K \in \mathbb{R}^{M \times M}$ - матрица похожести ($M$ - num of users), $[K]_{u,v} = \text{sim}(u,v)$.

$k_u = [\text{sim}(u,v_1), ..., \text{sim}(u, v_M)]^T$ - строка матрицы $K$.

$A = [a_{ui}]_1^{M,N}$, $\overline{a_i} = [a_1, ..., a_{M_i}]^T$

$$B = [b_{ij}], \quad b_{ij} =  
\begin{cases}  
1, & \text{если } a_{ij} > 0, \\  
0, & \text{если } a_{ij} = 0,  
\end{cases}$$

$\huge r_{ui} = \frac{{k}_u^\top {\bar{a}}_i}{{k}_u^\top {\bar{B}}_i}$

  

_Решение user-based:_

> ${R} = {K}{A} \oslash {K} {B}$
> 
> **Идея:** Пользователи похожи друг на друга. Для предсказания оценки пользователя $u$ объекту $i$ берутся взвешенные оценки соседей.
> 
> - $KA$: Сумма оценок соседей пользователя, взвешенная сходством пользователей.
> - $KB$: Взвешивающий множитель, чтобы учитывать, какие оценки известны (через $b_{ui}$).
> - $b_{ui}$**:** Маска, которая учитывает, известна оценка $a_{ui}$ или нет.

_Решение item-based:_

> ${R} = {A}^{T} \oslash ({B}{S}^T)$
> 
> **Идея**: Сходство между объектами используется для предсказания. Оцениваются объекты, на которые пользователь ещё не оставил оценку, основываясь на схожести объектов, которые он оценил.
> 
> - $A^T$: Транспонированные оценки объектов пользователями.
> - $BS^T$: Сходство объектов, взвешенное на маску $b_{ui}$

  

**Row-wise weighting for user-based kNN**

- Считается, что неизвестные оценки заменены на ноль.
- Это лучше для задач типа **"топ-N рекомендации"**, где важны не оценки, а ранжирование.

Теперь $N_i(u) = U \setminus \{u\}$ - теперь все пользователи, кроме $u$

С вычислительной точки зрения мало что поменяется, потому что данные разрежены (базово).

Сумма матрично не меняется. Вводим новую матрицу $\large \frac{1}{z_{ui}} \sim D = [d_{uu}]_1^{M}$ - диагональная, где $d_{uu} = \sum_{v \in N_i(u)}{|\text{sim}(u,v)|}$.

  

_Решение user-based:_

> ${R} = {D}_K^{-1}{K}{A}$
> 
> **Идея**: Вместо поэлементного взвешивания используется нормализация строк (пользователей) для агрегирования оценок.
> 
> - $D_K=diag(Ke)$: Диагональная матрица, нормализующая вклад соседей (может быть единичной,$D_K = I$.
> - $e$: Вектор единиц.

_Решение item-based:_

> $R = A S^T D_S^{-1}$
> 
> **Идея**: Здесь нормализация проводится по строкам объектов, что упрощает расчет топ-N рекомендаций.
> 
> - $D_S=diag(Se)$: Нормализующая диагональная матрица сходства объектов.

---

### kNN with asymmetric similarity

![[image 6 4.png|image 6 4.png]]

  

## Меры похожести

В зависимости от имеющегося фидбека, похожесть будет считаться по-разному: в случае _explicit_ фидбека это может быть корреляция, а в случае _implicit_ - мера Жаккара.

### Корелляция Пирсона и adjusted cosine similarity

Основные идейные отличия между корреляцией Пирсона и Adjusted Cosine Similarity в контексте item-based/user-based сходства:

![[image 7 3.png|image 7 3.png]]

1. **Коррекция на среднее значение**:
    - **Корреляция Пирсона**: Вычитает среднее значение оценок для каждого пользователя (или объекта), чтобы учитывать смещение, вызванное различиями в уровнях оценивания (например, одни пользователи ставят в среднем высокие оценки, а другие — низкие).
    - **Adjusted Cosine Similarity**: Также учитывает смещение, вычитая среднюю оценку пользователя, но используется в основном для item-based рекомендаций, чтобы устранить эффект от различных пользовательских склонностей к оценкам.
2. **Применение к объектам/пользователям**:
    - **Корреляция Пирсона**: Может быть применена как к сходству между пользователями, так и к сходству между объектами.
    - **Adjusted Cosine Similarity**: Обычно применяется к item-based подходу, так как вычисляется через пользователей как ось.
3. **Масштабирование значений**:
    - **Корреляция Пирсона**: Результат лежит в диапазоне от -1 до 1, где отрицательные значения отражают обратную корреляцию.
    - **Adjusted Cosine Similarity**: Значение находится в диапазоне от -1 до 1, но интерпретация фокусируется на "угловой" схожести.
4. **Устойчивость к разреженности**:
    - **Корреляция Пирсона**: Чувствительна к разреженным данным, поскольку требует достаточного количества совпадающих оценок.
    - **Adjusted Cosine Similarity**: Также требует совпадений, но лучше обрабатывает разреженные матрицы, если пользователей много.

---

### **Jaccard Similarity**

Мера Жаккара используется для оценки схожести между двумя наборами, измеряя отношение размера их пересечения к размеру их объединения.

**Формула**:

$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$

где:

- $A$ и $B$ — два множества;
- $| A \cap B |$ — количество общих элементов;
- $|A \cup B|$ — общее количество уникальных элементов в обоих множествах.

**Применение**:

- Сравнение бинарных данных (например, покупки: куплен/не куплен).
- Оценка схожести текстов (анализ шинглов).

**Преимущества**:

- Простота и понятная интерпретация.
- Работает для разреженных данных, так как учитываются только непустые пересечения.

**Недостатки**:

- Не учитывает вес элементов в множествах (влияние значимости каждого элемента одинаково).

---

### **Weighted Jaccard Similarity**

![[image 8 2.png|image 8 2.png]]

Расширяет меру Жаккара, учитывая веса элементов. Вместо простого подсчёта общих и объединённых элементов, она сравнивает взвешенные вклады.

$$J_w(A, B) = \frac{\sum_i \min(w_i^A, w_i^B)}{\sum_i \max(w_i^A, w_i^B)}$$

где:

- $w_i^A$ и $w_i^B$ — вес элемента i в множествах $A$ и $B$;
- $⁡\min$ и $\max$ используются для оценки пересечения и объединения соответственно.

**Пример весов**:

- Частота слова в тексте.
- Количество раз, когда пользователь взаимодействовал с элементом.
- Обратный ранк порядка $w \sim \frac{1}{rank}$

**Применение**:

- Анализ текстов с учётом частоты слов (TF-IDF).
- Рекомендательные системы, где учитывается интенсивность взаимодействий (например, количество покупок).

**Преимущества**:

- Более точная оценка схожести для данных с весами.
- Учитывает значимость отдельных элементов.

**Недостатки**:

- Более сложна в реализации, чем стандартная мера Жаккара.
- Чувствительна к шкале весов.

---