# Архитектуры uplift‑моделей

## 1. Постановка задачи и общая картина

### 1.1. Объект интереса

Работает стандартная схема potential outcomes:

- бинарное воздействие $W \in \{0,1\}$;
- исход $Y$ (конверсия, доход, дефолт и т.п.);
- ковариаты $X$.

Идеальный объект интереса:

$$
\tau(x) = \mathbb E[Y(1) - Y(0) \mid X = x].
$$

В uplift‑терминах:

- $\tau(x) > 0$: целесообразно таргетировать (ожидаем прирост);
- $\tau(x) < 0$: лучше **не трогать** (или даже применять анти‑третмент);
- $\tau(x) \approx 0$: эффект слабый → приоритизация по другим метрикам.

---

### 1.2. Два подхода к оценке

1. **Indirect (T/S/X/R/DR‑learners)**

   Сначала строим модели для $\mathbb E[Y\mid X,W]$ и/или $P(W=1\mid X)$, потом из них вычисляем $\hat \tau(x)$.

2. **Direct uplift modeling**

   Сразу оптимизируем критерий, связанный с uplift (Qini, разность конверсий в сегментах, специально сконструированные таргеты), не пытаясь отдельно оценивать исходы под $W=0$ и $W=1$.

Большинство архитектур ниже можно рассматривать как вариации одной и той же идеи: сконструировать «правильный» псевдо‑таргет или задачу, в которой стандартные ML‑модели дают оценку $\tau(x)$.

---

## 2. S‑learner

### 2.1. Идея

Строим одну модель исхода $m(x,w)$, где treatment подаётся как ещё одна фича.

Обозначим:

$$
\hat m(x,w) \approx \mathbb E[Y \mid X = x, W = w].
$$

Оценка uplift:

$$
\hat \tau_S(x) = \hat m(x,1) - \hat m(x,0).
$$

### 2.2. Обучение

Строим один регрессор / классификатор на данных:

- вход: $(X_i, W_i)$;
- таргет: $Y_i$;
- обычная supervised‑задача.

### 2.3. Применение

Для нового объекта $x$:

1. Считаем $\hat y_1 = \hat m(x,1)$;
2. Считаем $\hat y_0 = \hat m(x,0)$;
3. Берём $\hat \tau_S(x) = \hat y_1 - \hat y_0$.

### 2.4. Плюсы и минусы

**Плюсы:**

- максимально простой пайплайн (одна модель);
- легко использовать любые ML‑алгоритмы (GBM, NN и т.п.).

**Минусы:**

- модель может «игнорировать» $W$ как фичу, если основной вклад в вариацию $Y$ даёт $X$;
- чувствителен к дисбалансу групп (если $W=1$ сильно реже $W=0$);
- при сильной неоднородности эффекта может «усреднять» uplift.

S‑learner часто работает как baseline, но для тонких uplift‑эффектов его обычно мало.

---

## 3. T‑learner

### 3.1. Идея

Строим две отдельные модели исхода для каждой группы:

$$
\hat m_1(x) \approx \mathbb E[Y \mid X=x, W=1], \quad
\hat m_0(x) \approx \mathbb E[Y \mid X=x, W=0].
$$

Оценка uplift:

$$
\hat \tau_T(x) = \hat m_1(x) - \hat m_0(x).
$$

### 3.2. Обучение

1. Модель $f_1$: обучаем на подвыборке с $W=1$, таргет $Y$;
2. Модель $f_0$: обучаем на подвыборке с $W=0$, таргет $Y$.

Обычно $f_0$ и $f_1$ – одинаковые по классу модели (например, два GBM, два Random Forest и т.п.), но могут быть и разными, если природа данных в группах другая.

### 3.3. Применение

Для нового объекта $x$:

1. $\hat y_1 = f_1(x)$;
2. $\hat y_0 = f_0(x)$;
3. $\hat \tau_T(x) = \hat y_1 - \hat y_0$.

### 3.4. Плюсы и минусы

**Плюсы:**

- даёт максимум гибкости: каждая модель адаптируется под свою группу;
- хорошо работает при достаточном объёме данных в обеих группах.

**Минусы:**

- при сильном дисбалансе ($n_1 \ll n_0$ или наоборот) одна из моделей недообучена;
- разность двух шумных прогнозов → высокая дисперсия оценки $\hat \tau(x)$;
- не использует никакой «общей структуры» между группами (если она есть).

T‑learner – базовая архитектура indirect uplift; многие другие варианты пытаются исправить его недостатки.

---

## 4. X‑learner

### 4.1. Интуиция

X‑learner (Künzel et al.) решает две проблемы T‑learner:

1. дисбаланс групп;
2. высокая дисперсия прямой разности $\hat m_1(x) - \hat m_0(x)$.

Идея: использовать данные каждой группы для построения **псевдо‑таргета эффекта** в другой группе, а затем усреднять.

### 4.2. Стандартная схема X‑learner

1. **Шаг 1. Оценка исхода в каждой группе (как в T‑learner)**

   $$
   \hat m_1(x) \approx \mathbb E[Y \mid X=x, W=1], \quad
   \hat m_0(x) \approx \mathbb E[Y \mid X=x, W=0].
   $$

2. **Шаг 2. Построение псевдо‑таргетов эффекта**

   - для treated наблюдений $i: W_i=1$:
     $$
     D_i^{(1)} = Y_i - \hat m_0(X_i) \approx Y_i(1) - Y_i(0) = \tau(X_i);
     $$
   - для control наблюдений $i: W_i=0$:
     $$
     D_i^{(0)} = \hat m_1(X_i) - Y_i \approx Y_i(1) - Y_i(0) = \tau(X_i).
     $$

3. **Шаг 3. Обучение двух моделей uplift**

   - $g_1(x)$ на паре $(X_i, D_i^{(1)})$ для $W_i=1$ → оценка эффекта в treated‑популяции;
   - $g_0(x)$ на паре $(X_i, D_i^{(0)})$ для $W_i=0$ → оценка эффекта в control‑популяции.

4. **Шаг 4. Смешивание оценок**

   Итоговая оценка:

   $$
   \hat \tau_X(x) = \alpha(x) \cdot g_1(x) + (1-\alpha(x)) \cdot g_0(x),
   $$

   где $\alpha(x)$ – вес, часто зависящий от propensity или от плотности treated/control в окрестности $x$. Простейший вариант – константа, пропорциональная долям групп.

### 4.3. Плюсы и минусы

**Плюсы:**

- лучше использует данные при сильном дисбалансе групп;
- снижаем дисперсию за счёт регрессии псевдо‑таргетов;
- при хороших $\hat m_w$ даёт приличную устойчивость.

**Минусы:**

- два уровня моделей ($\hat m_w$ и $g_w$) → сложнее настройка и интерпретация;
- качество сильно зависит от первой стадии оценок $\hat m_w$.

X‑learner – один из наиболее популярных и практически работающих компромиссов для CATE/uplift.

---

## 5. R‑learner

### 5.1. Robinson‑декомпозиция

R‑learner (Nie & Wager) основан на ортогонализации (Robinson decomposition):

$$
Y - m(X) = \tau(X) \cdot (W - e(X)) + \varepsilon,
$$

где

- $m(X) = \mathbb E[Y \mid X]$;
- $e(X) = P(W=1\mid X)$ – propensity.

Идея: вычесть «главные эффекты» $m(X)$ и $e(X)$, чтобы получить задачу для $\tau(X)$, слабо чувствительную к ошибкам этих моделей.

### 5.2. Обучение R‑learner

1. Оценить $\hat m(X)$ (outcome‑модель без W) на всех данных.
2. Оценить $\hat e(X)$ (propensity) на всех данных.
3. Решать задачу минимизации по $\tau(\cdot)$:
   $$
   \min_{\tau \in \mathcal H} \frac{1}{n} \sum_{i=1}^n \Big( (Y_i - \hat m(X_i)) - \tau(X_i) (W_i - \hat e(X_i)) \Big)^2 + \Lambda(\tau),
   $$
   где $\Lambda(\tau)$ – регуляризатор (например, штраф за сложность модели $\tau$).

На практике $\tau(x)$ аппроксимируют любым регрессором, подавая на вход $X$, а в качестве таргета используют «псевдо‑резидуалы» и веса:

- таргет: $R_i = Y_i - \hat m(X_i)$;
- фича‑вес: $T_i = W_i - \hat e(X_i)$.

Многие реализации сводят это к взвешенной регрессии $R_i / T_i$ на $X_i$ с отсечением по $|T_i|$ и регуляризацией.

### 5.3. Свойства и связь с Double ML

- R‑learner использует ортогональное условие момента → малая чувствительность к первой стадии $\hat m, \hat e$ (если они достаточно хороши);
- естественно дружит с cross‑fitting: первые стадии оцениваются на одном фолде, $\tau$ – на другом;
- это прототип того, что в Double ML называют «локально ортогональным» подходом.

R‑learner больше про «чистую статистику» CATE и часто рассматривается как теоретический референс.

---

## 6. DR‑learner (Doubly Robust learner)

### 6.1. Путаница в терминологии

Под "DR‑learner" в литературе называют несколько очень похожих вещей. Общая идея:

- использовать **AIPW / doubly‑robust псевдо‑таргет**;
- обучать на нём обычную регрессию для получения $\tau(x)$.

### 6.2. Базовая конструкция DR‑learner

1. Оценить:

   - $\hat m_1(x) = \mathbb E[Y \mid X=x, W=1]$;
   - $\hat m_0(x) = \mathbb E[Y \mid X=x, W=0]$;
   - $\hat e(x) = P(W=1\mid X=x)$.

2. Построить для каждого наблюдения DR‑псевдо‑таргет эффекта $Z_i$:

   $$
   Z_i = \big(\hat m_1(X_i) - \hat m_0(X_i)\big)
   + \frac{W_i}{\hat e(X_i)} (Y_i - \hat m_1(X_i))
   - \frac{1-W_i}{1-\hat e(X_i)} (Y_i - \hat m_0(X_i)).
   $$

3. Обучить регрессию $h(X)$ на $(X_i, Z_i)$. Итоговая оценка:

   $$
   \hat \tau_{DR}(x) = h(x).
   $$

Свойство: если хотя бы $\hat m_w$ или $\hat e$ примерно корректны, получаем хорошую оценку $\tau(x)$; если корректны обе стадии, достигается эффективная оценка.

### 6.3. Связь с R‑learner и X‑learner

- DR‑learner можно рассматривать как "AIPW‑версию" X‑learner / T‑learner;
- R‑learner использует похожую ортогонализацию, но оптимизирует $\tau$ через другую цель (Robinson‑форму);
- в Double ML DR‑псевдо‑таргеты используются как вход для конечной регрессии параметров.

---

## 7. Direct Uplift Modeling

### 7.1. Общая идея

Indirect‑подходы сначала оценивают исходы / вероятности, потом разность. Direct‑подходы пытаются **сразу оптимизировать uplift**:

- либо через специально сконструированные таргеты;
- либо через модифицированные функции потерь;
- либо через разбиения на деревьях, максимизирующие разность конверсий.

### 7.2. Two‑model approach (классический indirect uplift)

Исторически в маркетинге под "two‑model uplift" часто понимают следующее:

1. Модель $p_1(x)$ для $P(Y=1 \mid X=x, W=1)$;
2. Модель $p_0(x)$ для $P(Y=1 \mid X=x, W=0)$;
3. Оценка uplift:
   $$
   \hat u(x) = p_1(x) - p_0(x).
   $$

Формально это T‑learner для бинарного исхода. Но в практической литературе это часто выделяют как "two‑model uplift".

### 7.3. Single‑model direct подходы

Есть несколько классов:

1. **Модифицированный таргет** (transformed outcome)

   Например, для RCT с пропорциями групп можно задать:

   $$
   Z = \frac{W Y}{p} - \frac{(1-W) Y}{1-p},
   $$

   где $p = P(W=1)$ – доля treated. Тогда при некоторых допущениях

   $$
   \mathbb E[Z \mid X=x] = \tau(x).
   $$

   Остаётся обучить одну регрессию $\mathbb E[Z \mid X]$.

2. **4‑классовая классификация**

   Классы: $(W,Y) \in \{(0,0),(0,1),(1,0),(1,1)\}$. Модель учится предсказывать распределение по этим 4 классам, а uplift для $x$ считается как разность конверсий:

   $$
   \hat u(x) = \hat P(Y=1 \mid X=x, W=1) - \hat P(Y=1 \mid X=x, W=0).
   $$

   Фактически это более богатый S‑learner.

3. **Прямые uplift‑лоссы**

   В некоторых моделях (особенно в деревьях) лосс формулируется прямо через uplift (например, прирост Qini‑коэффициента на сплите). Тогда дерево растёт так, чтобы максимально разделить сегменты по uplift.

Direct‑подходы привлекательны тем, что сразу оптимизируют то, что реально нужно, но часто требуют аккуратной настройки и специализированных реализаций.

---

## 8. Методы регуляризации uplift

Регуляризация критична: мы оцениваем разность / псевдо‑таргеты, часто на дисбалансных данных. Переобучиться $\hat \tau(x)$ крайне легко.

Основные приёмы:

### 8.1. Стандартная ML‑регуляризация

- L1/L2‑штрафы, ограничение глубины деревьев;
- shrinkage / learning rate в бустинге;
- ранняя остановка по валидации;
- bagging / random forest как способ снизить дисперсию.

### 8.2. Специальные ограничения на uplift

1. **Монотонность по признакам**

   Если известно, что эффект не должен уменьшаться/увеличиваться по какому‑то признаку (например, по score риска) → вводим monotonic constraints (в GBM / деревьях).

2. **Shrinkage uplift к нулю**

   В регионах с маленьким overlap или малым числом наблюдений добавляют штраф, тянущий $\hat \tau(x)$ к 0. Это можно реализовать:

   - как явный регуляризатор в функционале;
   - через Bayesian shrinkage (partial pooling);
   - через пост‑процессинг (обрезка $\hat \tau$ по доверительному интервалу).

3. **Наказание за несбалансированные сплиты** в деревьях

   При построении сплита можно штрафовать варианты, которые сильно нарушают баланс treatment/control в дочерних узлах.

4. **Ограничение области действия модели**

   В зонах крайне низкого пропенсити / отсутствия overlap вообще не делаем индивидуальные решения ("no decision" / fallback‑стратегия).

---

## 9. Tree‑based uplift models

### 9.1. Uplift trees

Идея: строить дерево, где каждый сплит выбирается так, чтобы **максимизировать разницу эффектов** между дочерними узлами.

Для узла $A$:

- средний исход в treatment: $\bar Y_{1,A}$;
- средний исход в control: $\bar Y_{0,A}$;
- uplift в узле: $u_A = \bar Y_{1,A} - \bar Y_{0,A}$.

При кандидате сплита $A \to (L,R)$ хотим максимизировать критерий типа:

$$
\Delta = |u_L - u_R| \times f(\text{размеры и баланс}),
$$

где $f$ штрафует за маленькие и несбалансированные узлы.

Есть множество вариантов критериев:

- разность uplift;
- прирост Qini / AUUC на узле;
- статистические критерии ($\chi^2$ для разницы эффектов).

### 9.2. Uplift Random Forest

Прямое обобщение:

- строим много uplift‑деревьев на бутстрэп‑выборках и подмножествах признаков;
- усредняем оценку uplift по деревьям:
  $$
  \hat \tau_{RF}(x) = \frac{1}{B} \sum_{b=1}^B \hat \tau^{(b)}(x).
  $$

Достоинства:

- сильно снижает дисперсию по сравнению с одиночным деревом;
- даёт устойчивые сегментации по uplift.

### 9.3. Causal Tree (Athey & Imbens)

Causal Tree – дерево, специально настроенное на гетерогенный treatment effect:

1. Данные делятся на "honest" части: одна часть используется для построения структуры дерева, другая – для оценки эффектов в листах.
2. Сплиты выбираются так, чтобы максимизировать дисперсию оценённого эффекта между листьями (при этом контролируя размер).
3. В листе оценивается $\hat \tau$ как разность средних (с корректировкой по ковариатам при необходимости).

"Honest" подход снижает оптимистичный bias оценок эффекта в маленьких листьях.

### 9.4. Causal Forest / GRF (Generalized Random Forest)

Causal Forest (Wager & Athey) и обобщения (GRF) – ансамбль causal‑trees с дополнительной ортогонализацией:

- используют $\hat m(X)$ и $\hat e(X)$ для предварительной центровки (как в R‑learner);
- строят деревья на резидуалах, а не на сырых $Y$;
- итоговая оценка $\hat \tau(x)$ интерпретируется как локально взвешенная регрессия псевдо‑таргетов по соседям $x$, определяемым структурой леса.

Плюсы:

- теория: состоятельность, доверительные интервалы для $\tau(x)$ под разумными допущениями;
- практика: устойчивость к высоким размерностям и сложным нелинейностям.

Минусы:

- сложнее в настройке и интерпретации, чем простые uplift‑деревья;
- требовательны к объёму данных.

---

## 10. Пример схемы обучения и предсказания

### 10.1. Пайплайн X‑learner + Gradient Boosting (RCT‑кейс)

Предположим, есть RCT с рандомизацией $P(W=1)=p$.

**Шаг 1. Подготовка данных**

- очистка, feature engineering, разделение на train/valid;
- опционально: стратификация по крупным сегментам.

**Шаг 2. Оценка исходов $\hat m_1, \hat m_0$**

- обучаем GBM на treated: $f_1(x) \to \hat m_1(x)$;
- обучаем GBM на control: $f_0(x) \to \hat m_0(x)$.

**Шаг 3. Псевдо‑таргеты эффекта**

- для treated: $D_i^{(1)} = Y_i - \hat m_0(X_i)$;
- для control: $D_i^{(0)} = \hat m_1(X_i) - Y_i$.

**Шаг 4. Модели эффекта $g_1, g_0$**

- обучаем $g_1$ (GBM / RF) на $(X_i, D_i^{(1)})$ по treated;
- обучаем $g_0$ на $(X_i, D_i^{(0)})$ по control.

**Шаг 5. Смешивание**

Для RCT можно взять $\alpha(x) = p$ или оценить $\alpha(x)$ как функцию $X$ (например, через локальную плотность treated).

Итог:

$$
\hat \tau(x) = \alpha(x) g_1(x) + (1-\alpha(x)) g_0(x).
$$

**Шаг 6. Использование модели**

- сортируем клиентов по $\hat \tau(x)$ от большего к меньшему;
- выбираем top‑q% для таргетинга в зависимости от бюджета и unit economics;
- строим uplift‑кривые (Qini/AUUC) для оценки качества модели.

### 10.2. Пайплайн DR‑learner + любой регрессор (наблюдательные данные)

1. Оценить $\hat e(X)$ (propensity, см. конспект по методам оценки эффекта).
2. Оценить $\hat m_1(X), \hat m_0(X)$ с корректировкой по $\hat e(X)$ (IPTW / DR‑подходы).
3. Построить DR‑псевдо‑таргеты $Z_i$ как выше.
4. Разбить данные на фолды и обучить регрессор $h(X)$ на $(X_i, Z_i)$ с cross‑fitting.
5. $\hat \tau(x) = h(x)$ используется как uplift‑скор.

---

## 11. Краткое сравнение архитектур

| Архитектура         | Основная идея                                       | Требует $e(X)$? | Устойчивость к дисбалансу                  | Сложность       |
| ------------------- | --------------------------------------------------- | ----------------- | ------------------------------------------ | --------------- |
| S‑learner           | Одна модель $m(x,w)$, uplift как разность         | нет               | низкая                                     | низкая          |
| T‑learner           | Две модели $m_1, m_0$, uplift как разность        | нет               | средняя, страдает при сильном дисбалансе   | низкая–средняя  |
| X‑learner           | Псевдо‑таргеты эффекта в обеих группах + смешивание | опционально       | хорошая                                    | средняя         |
| R‑learner           | Robinson‑формула, ортогонализация по $m,e$        | да                | хорошая (при нормальных $\hat m,\hat e$) | высокая         |
| DR‑learner          | AIPW‑псевдо‑таргеты + регрессия $\tau(x)$         | да                | высокая (двойная робастность)              | средняя–высокая |
| Uplift trees / RF   | Сплиты, максимизирующие разность эффектов           | опционально       | зависит от критерия и штрафов              | средняя         |
| Causal Forest / GRF | Лес causal‑деревьев с ортогонализацией              | да                | высокая                                    | высокая         |

---

## 12. Выводы

- S‑ и T‑learners – простые базовые архитектуры; T‑learner обычно лучше при достаточных данных.
- X‑learner – рабочая лошадка для CATE/uplift, особенно при дисбалансе групп.
- R‑ и DR‑learners привносят ортогонализацию и двойную робастность, важны для корректного анализа причинных эффектов в сложных наблюдательных данных.
- Direct uplift‑модели и uplift‑деревья оптимизируют uplift более напрямую и удобны для интерпретируемой сегментации.
- Causal Tree / Forest (GRF) – теоретически обоснованные методы с возможностью получать доверительные интервалы для $\tau(x)$.

Понимая, как эти архитектуры связаны между собой (через псевдо‑таргеты, ортогонализацию и использование propensity), можно сознательно выбирать подход под конкретную задачу, данные и требования к интерпретируемости и статистической строгости.

