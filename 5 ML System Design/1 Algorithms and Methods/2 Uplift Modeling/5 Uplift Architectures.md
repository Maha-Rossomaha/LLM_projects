# Архитектуры uplift‑моделей

## 1. Постановка задачи и общая картина

### 1.1. Объект интереса

Работает стандартная схема potential outcomes:

- бинарное воздействие $W \in \{0,1\}$;
- исход $Y$ (конверсия, доход, дефолт и т.п.);
- ковариаты $X$;
- базовый уровень исхода без учёта treatment $m(x)$.

Идеальный объект интереса:

$$
\tau(x) = \mathbb E[Y(1) - Y(0) \mid X = x].
$$

В uplift‑терминах:

- $\tau(x) > 0$: целесообразно таргетировать (ожидаем прирост);
- $\tau(x) < 0$: лучше **не трогать** (или даже применять анти‑тритмент);
- $\tau(x) \approx 0$: эффект слабый → приоритизация по другим метрикам.

Общая формула:
$$
   Y = m(x) + \tau(x)\cdot W + \text{шум}
$$

---

### 1.2. Два подхода к оценке

1. **Indirect (T/S/X/R/DR‑learners)**

   Сначала строим модели для $\mathbb E[Y\mid X,W]$ и/или $P(W=1\mid X)$, потом из них вычисляем $\hat \tau(x)$.

2. **Direct uplift modeling**

   Сразу оптимизируем критерий, связанный с uplift (Qini, разность конверсий в сегментах, специально сконструированные таргеты), не пытаясь отдельно оценивать исходы под $W=0$ и $W=1$.

Большинство архитектур ниже можно рассматривать как вариации одной и той же идеи: сконструировать «правильный» псевдо‑таргет или задачу, в которой стандартные ML‑модели дают оценку $\tau(x)$.

---

## 2. S‑learner

### 2.1. Идея

Строим одну модель исхода $m(x,w)$, где treatment подаётся как ещё одна фича.

Обозначим:

$$
\hat m(x,w) \approx \mathbb E[Y \mid X = x, W = w].
$$

Оценка uplift:

$$
\hat \tau_S(x) = \hat m(x,1) - \hat m(x,0).
$$

### 2.2. Обучение

Строим один регрессор / классификатор на данных:

- вход: $(X_i, W_i)$;
- таргет: $Y_i$;
- обычная supervised‑задача.

### 2.3. Применение

Для нового объекта $x$:

1. Считаем $\hat y_1 = \hat m(x,1)$;
2. Считаем $\hat y_0 = \hat m(x,0)$;
3. Берём $\hat \tau_S(x) = \hat y_1 - \hat y_0$.

### 2.4. Плюсы и минусы

**Плюсы:**

- максимально простой пайплайн (одна модель);
- легко использовать любые ML‑алгоритмы (GBM, NN и т.п.).

**Минусы:**

- модель может «игнорировать» $W$ как фичу, если основной вклад в вариацию $Y$ даёт $X$;
- чувствителен к дисбалансу групп (если $W=1$ сильно реже $W=0$);
- при сильной неоднородности эффекта может «усреднять» uplift.

S‑learner часто работает как baseline, но для тонких uplift‑эффектов его обычно мало.

---

## 3. T‑learner

### 3.1. Идея

Строим две отдельные модели исхода для каждой группы:

$$
\hat m_1(x) \approx \mathbb E[Y \mid X=x, W=1], \quad
\hat m_0(x) \approx \mathbb E[Y \mid X=x, W=0].
$$

Оценка uplift:

$$
\hat \tau_T(x) = \hat m_1(x) - \hat m_0(x).
$$

### 3.2. Обучение

1. Модель $f_1$: обучаем на подвыборке с $W=1$, таргет $Y$;
2. Модель $f_0$: обучаем на подвыборке с $W=0$, таргет $Y$.

Обычно $f_0$ и $f_1$ – одинаковые по классу модели (например, два GBM, два Random Forest и т.п.), но могут быть и разными, если природа данных в группах другая.

### 3.3. Применение

Для нового объекта $x$:

1. $\hat y_1 = f_1(x)$;
2. $\hat y_0 = f_0(x)$;
3. $\hat \tau_T(x) = \hat y_1 - \hat y_0$.

### 3.4. Плюсы и минусы

**Плюсы:**

- даёт максимум гибкости: каждая модель адаптируется под свою группу;
- хорошо работает при достаточном объёме данных в обеих группах.

**Минусы:**

- при сильном дисбалансе ($n_1 \ll n_0$ или наоборот) одна из моделей недообучена;
- разность двух шумных прогнозов → высокая дисперсия оценки $\hat \tau(x)$;
- не использует никакой «общей структуры» между группами (если она есть).

T‑learner – базовая архитектура indirect uplift; многие другие варианты пытаются исправить его недостатки.

---

## 4. X‑learner

### 4.1. Интуиция

X‑learner решает две проблемы T‑learner:

1. дисбаланс групп;
2. высокая дисперсия прямой разности $\hat m_1(x) - \hat m_0(x)$.

Идея: использовать данные каждой группы для построения **псевдо‑таргета эффекта** в другой группе, а затем усреднять.

### 4.2. Стандартная схема X‑learner

1. **Шаг 1. Оценка исхода в каждой группе (как в T‑learner)**

   $$
   \hat m_1(x) \approx \mathbb E[Y \mid X=x, W=1], \quad
   \hat m_0(x) \approx \mathbb E[Y \mid X=x, W=0].
   $$

2. **Шаг 2. Построение псевдо‑таргетов эффекта**

   - для treated наблюдений $i: W_i=1$:
     $$
     D_i^{(1)} = Y_i - \hat m_0(X_i) \approx Y_i(1) - Y_i(0) = \tau(X_i);
     $$
   - для control наблюдений $i: W_i=0$:
     $$
     D_i^{(0)} = \hat m_1(X_i) - Y_i \approx Y_i(1) - Y_i(0) = \tau(X_i).
     $$
   > **Важно:** $D_i^{(0)}\ \text{и}\ D_i^{(1)} \in [-1, 1]$

3. **Шаг 3. Обучение двух моделей uplift**

   - $g_1(x)$ на паре $(X_i, D_i^{(1)})$ для $W_i=1$ → оценка эффекта в treated‑популяции;
   - $g_0(x)$ на паре $(X_i, D_i^{(0)})$ для $W_i=0$ → оценка эффекта в control‑популяции.

4. **Шаг 4. Смешивание оценок**

   Итоговая оценка:

   $$
   \hat \tau_X(x) = \alpha(x) \cdot g_1(x) + (1-\alpha(x)) \cdot g_0(x),
   $$

   где $\alpha(x)$ – вес, часто зависящий от propensity или от плотности treated/control в окрестности $x$. 

#### Основные варианты $\alpha(x)$:

1. **Константа:** пропорционально долям групп $\alpha=\frac{n_1}{n_0+n_1}$.
2. **Пропорционально propensity:** $\alpha(x) = e(x)$ - *самый популярный*
3. **Weighting propensity logits:** если $e(x)$ близок к 0 или 1: $\alpha(x) = \sigma(\beta\cdot \text{logit}(e(x)))$
   1. $\beta > 1$ сглаживает экстремальные веса,
   2. $\beta < 1$ усиливает разделение.
4. **Truncated propensity weighting:** цель - не давать огромный вес «почти 1» или «почти 0»
$$
e'(x)=\min(\max(e(x), \epsilon), 1-\epsilon), \quad \epsilon \in [0.01, 0.05]\\
\alpha(x) = e'(x)
$$
5. **Weighting через модель ошибки:** самый строгий и статистически верный вариант, *часто используется в банках*.  
**Мотивация:** 
   1. в области, где мало treated → $g_1(x)$ плохо обучен
   2. в области, где мало control → $g_0(x)$ менее устойчив  
поэтому хочется смешивать так, чтобы получать **больше веса** от более точной модели  
**Шаги для построения:**
      1. **Построение модели MSE** *(для получения численной ошибки)***:**  
         Для **treated:** $\quad\quad e_1 = D_1 - g_1(X_i) = Y_i - m_0(X_i) - g_1(X_i)$  
         Для **control:** $\quad\quad e_0 = D_0 - g_0(X_i) = m_1(X_i) - Y_i - g_0(X_i)$
      2. **Построение модели дисперсии:**
         $$
            \widehat{\text{Var}}_1(x) = h_1(x),\\
            \widehat{\text{Var}}_0(x) = h_0(x)
         $$  
         где $h_1,\;h_0$ нужны для оценки **дисперсии ошибки** моделей $g_1,\;g_0$ в каждой точке $x$.  
      **Это можно сделать через:**
            1. **modeling squared loss** - хороший вариант для регрессий (*обучаем регрессию, где таргет — квадрат ошибки*)
            $$
               e_1^2\rightarrow h_1(x),\\
               e_0^2\rightarrow h_0(x)
            $$
            2. **quantile loss** оценка распределения ошибки (*модель квантильной регрессии*)  
            $$
               h_1(x) = q_{1_{0.9}}(x)-q_{1_{0.1}}(x),\\
               h_0(x) = q_{0_{0.9}}(x)-q_{0_{0.1}}(x),\\
            $$
      3. **Задание веса через обратную дисперсию:** (*меньше дисперсия ошибки $\rightarrow$ надежднее оценка*)
      $$
         \alpha(x)=\frac{1/\widehat{\text{Var}}_1(x)}{1/\widehat{\text{Var}}_0(x)+1/\widehat{\text{Var}}_1(x)}
      $$
   

### 4.3. Плюсы и минусы

**Плюсы:**

- лучше использует данные при сильном дисбалансе групп;
- снижаем дисперсию за счёт регрессии псевдо‑таргетов;
- при хороших $\hat m_w$ даёт приличную устойчивость.

**Минусы:**

- два уровня моделей ($\hat m_w$ и $g_w$) → сложнее настройка и интерпретация;
- качество сильно зависит от первой стадии оценок $\hat m_w$.

X‑learner – один из наиболее популярных и практически работающих компромиссов для CATE/uplift.

---

## 5. R‑learner

> R‑learner больше про «чистую статистику» CATE и часто рассматривается как теоретический референс.

### 5.1. Robinson‑декомпозиция

R‑learner основан на ортогонализации (Robinson decomposition):

$$
Y - m(X) = \tau(X) \cdot (W - e(X)) + \varepsilon,
$$

где

- $m(X) = \mathbb E[Y \mid X]$;
- $e(X) = P(W=1\mid X)$ – propensity.

Идея: вычесть «главные эффекты» $m(X)$ и $e(X)$, чтобы получить задачу для $\tau(X)$, слабо чувствительную к ошибкам этих моделей.

#### Шаги ортогонализации

1. Изначальная формула - $Y = m(X) + \tau(X)\cdot W + \text{шум}$.
2. Определяем $m(X) = \mathbb E[Y \mid X]$. Тогда $Y = m(X) + (Y - m(X))$.  
*Здесь $Y - m(X)$ — это **остаток исхода**, то, что не объясняется голыми признаками $X$.*
3. Определяем $e(X) = \mathbb E[W \mid X] = P(W=1\mid X)$. Тогда $W = e(X) + (W - e(X))$.  
*$W - e(X)$ — это **остаток treatment**, т.е. насколько фактическое назначение treatment отличается от «ожидаемого по $X$».*


### 5.2. Обучение R‑learner

1. Оцениваем $\hat m(X) \approx \mathbb E[Y\mid X]$ (любая ML-модель без $W$).
2. Оцениваем $\hat e(X) \approx P(W=1\mid X)$.
3. Строим «псевдо-датасет»:
   * таргет: $R_i = Y_i - \hat m(X_i)$;
   * «инструмент»: $T_i = W_i - \hat e(X_i)$.
4. Решать задачу минимизации по $\tau(\cdot)$:
   $$
   \min_{\tau \in \mathcal H} \frac{1}{n} \sum_{i=1}^n \Big( (Y_i - \hat m(X_i)) - \tau(X_i) (W_i - \hat e(X_i)) \Big)^2 + \Lambda(\tau),
   $$
   где $\Lambda(\tau)$ – регуляризатор (например, штраф за сложность модели $\tau$).

На практике $\tau(x)$ аппроксимируют любым регрессором, подавая на вход $X$, а в качестве таргета используют «псевдо‑резидуалы» и веса.

Многие реализации сводят это к взвешенной регрессии $R_i / T_i$ на $X_i$ с отсечением по $|T_i|$ и регуляризацией.

### 5.3. Свойства и связь с Double ML

- R‑learner использует ортогональное условие момента → малая чувствительность к первой стадии $\hat m, \hat e$ (если они достаточно хороши);
- естественно дружит с cross‑fitting: первые стадии оцениваются на одном фолде, $\tau$ – на другом;
- это прототип того, что в Double ML называют «локально ортогональным» подходом.

---

## 6. DR‑learner (Doubly Robust learner)

### 6.1. Путаница в терминологии

Под "DR‑learner" в литературе называют несколько очень похожих вещей. Общая идея:

- использовать **AIPW / doubly‑robust псевдо‑таргет**;
- обучать на нём обычную регрессию для получения $\tau(x)$.

### 6.2. Базовая конструкция DR‑learner

1. Оценить:

   - $\hat m_1(x) = \mathbb E[Y \mid X=x, W=1]$;
   - $\hat m_0(x) = \mathbb E[Y \mid X=x, W=0]$;
   - $\hat e(x) = P(W=1\mid X=x)$.

2. Построить для каждого наблюдения DR‑псевдо‑таргет эффекта $Z_i$:

   $$
   Z_i = \big(\hat m_1(X_i) - \hat m_0(X_i)\big)
   + \frac{W_i}{\hat e(X_i)} (Y_i - \hat m_1(X_i))
   - \frac{1-W_i}{1-\hat e(X_i)} (Y_i - \hat m_0(X_i)).
   $$

3. Обучить регрессию $h(X)$ на $(X_i, Z_i)$. Итоговая оценка:

   $$
   \hat \tau_{DR}(x) = h(x).
   $$

> Свойство: если хотя бы $\hat m_w$ или $\hat e$ примерно корректны, получаем хорошую оценку $\tau(x)$; если корректны обе стадии, достигается эффективная оценка.

#### Разбор формулы для эффекта $Z_i$
1. **Базовый «регрессионный» эффект:**

   $$
   \underbrace{\hat m_1(X_i) - \hat m_0(X_i)}_{\text{импутированный uplift из outcome-моделей}}
   $$

   Это то, что дал бы обычный T-learner: чисто по моделям исхода.

2. **Коррекция для treated:**

   $$
      \frac{W_i}{\hat e(X_i)} (Y_i - \hat m_1(X_i)).
   $$

   * Если $W_i=1$, член превращается в $\frac{1}{\hat e(X_i)} (Y_i - \hat m_1(X_i))$.
   * Если $W_i=0$, он равен нулю.

   Смысл:

   * $Y_i - \hat m_1(X_i)$ — **ошибка модели m₁** на treated-наблюдении;
   * мы её добавляем с весом $1/\hat e(X_i)$ (IPW-фактор), чтобы компенсировать смещение регрессионной части в treated-подвыборке.

3. **Коррекция для control (со знаком минус):**

   $$
      -\frac{1-W_i}{1-\hat e(X_i)} (Y_i - \hat m_0(X_i)).
   $$

   * Если $W_i=0$, это $- \frac{1}{1-\hat e(X_i)} (Y_i - \hat m_0(X_i))$.

   * Если $W_i=1$, этот член = 0.

   Аналогично:

   * $Y_i - \hat m_0(X_i)$ — ошибка модели m₀ на контроле;
   * вычитаем её с IPW-весом, чтобы исправить смещение в control.

#### Интуиция за формулой

* Первая часть $\hat m_1 - \hat m_0$ даёт **гладкую, малодисперсную, но потенциально смещённую** оценку $\tau(x)$ (если модели исхода не идеальны).
* Вторые две части — это **IPTW-коррекция остатков**, которая:
  * «подтягивает» оценку к реальным наблюдённым $Y_i$;
  * исправляет смещение, если верно $\hat e(X)$.

Главное свойство (двойная робастность):

* если **хотя бы одна** из двух частей (outcome-модели $\hat m_0,\hat m_1$ **или** propensity $\hat e)$ специфицирована правильно, то
  $$
  \mathbb E[Z_i \mid X_i=x] = \tau(x).
  $$

То есть $Z_i$ — это **псевдо-наблюдение индивидуального эффекта** с хорошими свойствами.
Дальше DR-learner просто делает регрессию:

$$
\hat \tau(x) \approx \mathbb E[Z \mid X=x],
$$

обучая любой регрессор на паре $(X_i, Z_i)$.

### 6.3. Связь с R‑learner и X‑learner

- DR‑learner можно рассматривать как "AIPW‑версию" X‑learner / T‑learner;
- R‑learner использует похожую ортогонализацию, но оптимизирует $\tau$ через другую цель (Robinson‑форму);
- в Double ML DR‑псевдо‑таргеты используются как вход для конечной регрессии параметров.

### 6.4. Когда DR-learner выгоднее T/X-learner на практике

DR-learner сложнее и тяжелее в проде, поэтому он имеет смысл не всегда. Типичные ситуации, где он реально выигрывает у T/X-learner:

1. **Сильная несбалансированность treatment / control.**  
   Когда $e(X)$ сильно варьируется и много объектов с $e(X)\approx 0$ или $1$ (типичная оффлайн-оценка маркетинговых политик, прод-эксперименты с неравными долями), T/X-learner даёт сильно смещённый uplift на «тонких» слоях. DR-learner за счёт IPW-компоненты лучше корректирует этот перекос.

2. **Сильный конфаундинг и маленький uplift.**  
   В задачах, где:
   - эффект небольшой (типичный кредитный скоринг с промо-ставками, тонкие маркетинговые воздействия),
   - а базовый отклик сильно зависит от $X$,
   простое вычитание $m_1(x)-m_0(x)$ (T-learner) хорошо оценивает $m$, но *плохо* — разницу. DR-learner уменьшает смещение за счёт ортогонализации по остаткам.

3. **Неидеальные модели исходов или пропенсити (реалистичный прод).**  
   В реальных пайплайнах:
   - $m_w(x)$ и $e(x)$ часто обучены разными командами, на разных выборках, с разным качеством;
   - нет гарантии, что обе модели корректны.  
   Для T/X-learner ошибка любой модели напрямую бьёт по uplift. У DR-learner есть «страховка»: если хотя бы *что-то одно* смоделировано адекватно, итоговая оценка $\tau(x)$ остаётся приемлемой.

4. **Heavy ML: глубокие сети / очень гибкие модели на $X$.**  
   При использовании очень гибких моделей (GBM, RF, NN) для $m_w, e$ возрастает риск переобучения и лика. DR-learner в комбинации с cross-fitting (sample-splitting) снижает bias от оверфита ньюис-моделей, давая более стабильный uplift, чем банальный X-learner поверх тех же моделей.

5. **Сценарии «оценить существующую политику / таргетинг».**  
   Когда задача — не только «предсказать uplift», но и:
   - честно оценить эффект уже работающей политики (policy evaluation),
   - или сравнить несколько политик задним числом,  
   DR-learner ближе к инструментарию каузального inference и даёт более интерпретируемую и устойчивую к смещению оценку, чем T/X-learner, которые больше «про ML, а не про каузальность».

### 6.5. Когда можно обойтись X/T-learner

Если:
- **чистый A/B-тест** с рандомизацией и примерно равными долями групп,
- **слабый конфаундинг** (по сути его нет),
- разумный объём данных,

то выигрыш DR-learner над хорошо настроенным X-learner будет небольшим, а сложность пайплайна — выше. В таком случае проще и надёжнее использовать X-learner / T-learner + аккуратный feature-engineering и валидацию uplift-кривыми / Qini.


---

## 7. Direct Uplift Modeling

### 7.1. Общая идея

Indirect‑подходы сначала оценивают исходы / вероятности, потом разность. Direct‑подходы пытаются **сразу оптимизировать uplift**:

- либо через специально сконструированные таргеты;
- либо через модифицированные функции потерь;
- либо через разбиения на деревьях, максимизирующие разность конверсий.

### 7.2. Two‑model approach (классический indirect uplift)

Исторически в маркетинге под "two‑model uplift" часто понимают следующее:

1. Модель $p_1(x)$ для $P(Y=1 \mid X=x, W=1)$;
2. Модель $p_0(x)$ для $P(Y=1 \mid X=x, W=0)$;
3. Оценка uplift:
   $$
   \hat u(x) = p_1(x) - p_0(x).
   $$

Формально это T‑learner для бинарного исхода. Но в практической литературе это часто выделяют как "two‑model uplift".

### 7.3. Single‑model direct подходы

Есть несколько классов:

1. **Модифицированный таргет** (transformed outcome)

   Например, для RCT с пропорциями групп можно задать:

   $$
   Z = \frac{W Y}{p} - \frac{(1-W) Y}{1-p},
   $$

   где $p = P(W=1)$ – доля treated. Тогда при некоторых допущениях

   $$
   \mathbb E[Z \mid X=x] = \tau(x).
   $$

   Остаётся обучить одну регрессию $\mathbb E[Z \mid X]$.

2. **4‑классовая классификация**

   Классы: $(W,Y) \in \{(0,0),(0,1),(1,0),(1,1)\}$. Модель учится предсказывать распределение по этим 4 классам, а uplift для $x$ считается как разность конверсий:

   $$
   \hat u(x) = \hat P(Y=1 \mid X=x, W=1) - \hat P(Y=1 \mid X=x, W=0).
   $$

   Фактически это более богатый S‑learner.

3. **Прямые uplift‑лоссы**

   В некоторых моделях (особенно в деревьях) лосс формулируется прямо через uplift (например, прирост Qini‑коэффициента на сплите). Тогда дерево растёт так, чтобы максимально разделить сегменты по uplift.

Direct‑подходы привлекательны тем, что сразу оптимизируют то, что реально нужно, но часто требуют аккуратной настройки и специализированных реализаций.

---

## 8. Методы регуляризации uplift

Регуляризация критична: мы оцениваем разность / псевдо‑таргеты, часто на дисбалансных данных. Переобучиться $\hat \tau(x)$ крайне легко.

Основные приёмы:

### 8.1. Стандартная ML‑регуляризация

- L1/L2‑штрафы, ограничение глубины деревьев;
- shrinkage / learning rate в бустинге;
- ранняя остановка по валидации;
- bagging / random forest как способ снизить дисперсию.

### 8.2. Специальные ограничения на uplift

1. **Монотонность по признакам**

   Если известно, что эффект не должен уменьшаться/увеличиваться по какому‑то признаку (например, по score риска) → вводим monotonic constraints (в GBM / деревьях).

2. **Shrinkage uplift к нулю**

   В регионах с маленьким overlap или малым числом наблюдений добавляют штраф, тянущий $\hat \tau(x)$ к 0. Это можно реализовать:

   - как явный регуляризатор в функционале;
   - через Bayesian shrinkage (partial pooling);
   - через пост‑процессинг (обрезка $\hat \tau$ по доверительному интервалу).

3. **Наказание за несбалансированные сплиты** в деревьях

   При построении сплита можно штрафовать варианты, которые сильно нарушают баланс treatment/control в дочерних узлах.

4. **Ограничение области действия модели**

   В зонах крайне низкого пропенсити / отсутствия overlap вообще не делаем индивидуальные решения ("no decision" / fallback‑стратегия).

---

## 9. Tree‑based uplift models

### 9.1. Uplift trees

Идея: строить дерево, где каждый сплит выбирается так, чтобы **максимизировать разницу эффектов** между дочерними узлами.

Для узла $A$:

- средний исход в treatment: $\bar Y_{1,A}$;
- средний исход в control: $\bar Y_{0,A}$;
- uplift в узле: $u_A = \bar Y_{1,A} - \bar Y_{0,A}$.

При кандидате сплита $A \to (L,R)$ хотим максимизировать критерий типа:

$$
\Delta = |u_L - u_R| \times f(\text{размеры и баланс}),
$$

где $f$ штрафует за маленькие и несбалансированные узлы.

Есть множество вариантов критериев:

- разность uplift;
- прирост Qini / AUUC на узле;
- статистические критерии ($\chi^2$ для разницы эффектов).

### 9.2. Uplift Random Forest

Прямое обобщение:

- строим много uplift‑деревьев на бутстрэп‑выборках и подмножествах признаков;
- усредняем оценку uplift по деревьям:
  $$
  \hat \tau_{RF}(x) = \frac{1}{B} \sum_{b=1}^B \hat \tau^{(b)}(x).
  $$

Достоинства:

- сильно снижает дисперсию по сравнению с одиночным деревом;
- даёт устойчивые сегментации по uplift.

### 9.3. Causal Tree (Athey & Imbens)

Causal Tree – дерево, специально настроенное на гетерогенный treatment effect:

1. Данные делятся на "honest" части: одна часть используется для построения структуры дерева, другая – для оценки эффектов в листах.
2. Сплиты выбираются так, чтобы максимизировать дисперсию оценённого эффекта между листьями (при этом контролируя размер).
3. В листе оценивается $\hat \tau$ как разность средних (с корректировкой по ковариатам при необходимости).

"Honest" подход снижает оптимистичный bias оценок эффекта в маленьких листьях.

### 9.4. Causal Forest / GRF (Generalized Random Forest)

Causal Forest (Wager & Athey) и обобщения (GRF) – ансамбль causal‑trees с дополнительной ортогонализацией:

- используют $\hat m(X)$ и $\hat e(X)$ для предварительной центровки (как в R‑learner);
- строят деревья на резидуалах, а не на сырых $Y$;
- итоговая оценка $\hat \tau(x)$ интерпретируется как локально взвешенная регрессия псевдо‑таргетов по соседям $x$, определяемым структурой леса.

Плюсы:

- теория: состоятельность, доверительные интервалы для $\tau(x)$ под разумными допущениями;
- практика: устойчивость к высоким размерностям и сложным нелинейностям.

Минусы:

- сложнее в настройке и интерпретации, чем простые uplift‑деревья;
- требовательны к объёму данных.

---

## 10. Пример схемы обучения и предсказания

### 10.1. Пайплайн X‑learner + Gradient Boosting (RCT‑кейс)

Предположим, есть RCT с рандомизацией $P(W=1)=p$.

**Шаг 1. Подготовка данных**

- очистка, feature engineering, разделение на train/valid;
- опционально: стратификация по крупным сегментам.

**Шаг 2. Оценка исходов $\hat m_1, \hat m_0$**

- обучаем GBM на treated: $f_1(x) \to \hat m_1(x)$;
- обучаем GBM на control: $f_0(x) \to \hat m_0(x)$.

**Шаг 3. Псевдо‑таргеты эффекта**

- для treated: $D_i^{(1)} = Y_i - \hat m_0(X_i)$;
- для control: $D_i^{(0)} = \hat m_1(X_i) - Y_i$.

**Шаг 4. Модели эффекта $g_1, g_0$**

- обучаем $g_1$ (GBM / RF) на $(X_i, D_i^{(1)})$ по treated;
- обучаем $g_0$ на $(X_i, D_i^{(0)})$ по control.

**Шаг 5. Смешивание**

Для RCT можно взять $\alpha(x) = p$ или оценить $\alpha(x)$ как функцию $X$ (например, через локальную плотность treated).

Итог:

$$
\hat \tau(x) = \alpha(x) g_1(x) + (1-\alpha(x)) g_0(x).
$$

**Шаг 6. Использование модели**

- сортируем клиентов по $\hat \tau(x)$ от большего к меньшему;
- выбираем top‑q% для таргетинга в зависимости от бюджета и unit economics;
- строим uplift‑кривые (Qini/AUUC) для оценки качества модели.

### 10.2. Пайплайн DR‑learner + любой регрессор (наблюдательные данные)

1. Оценить $\hat e(X)$ (propensity, см. конспект по методам оценки эффекта).
2. Оценить $\hat m_1(X), \hat m_0(X)$ с корректировкой по $\hat e(X)$ (IPTW / DR‑подходы).
3. Построить DR‑псевдо‑таргеты $Z_i$ как выше.
4. Разбить данные на фолды и обучить регрессор $h(X)$ на $(X_i, Z_i)$ с cross‑fitting.
5. $\hat \tau(x) = h(x)$ используется как uplift‑скор.

---

## 11. Краткое сравнение архитектур

| Архитектура         | Основная идея                                       | Требует $e(X)$? | Устойчивость к дисбалансу                  | Сложность       |
| ------------------- | --------------------------------------------------- | ----------------- | ------------------------------------------ | --------------- |
| S‑learner           | Одна модель $m(x,w)$, uplift как разность         | нет               | низкая                                     | низкая          |
| T‑learner           | Две модели $m_1, m_0$, uplift как разность        | нет               | средняя, страдает при сильном дисбалансе   | низкая–средняя  |
| X‑learner           | Псевдо‑таргеты эффекта в обеих группах + смешивание | опционально       | хорошая                                    | средняя         |
| R‑learner           | Robinson‑формула, ортогонализация по $m,e$        | да                | хорошая (при нормальных $\hat m,\hat e$) | высокая         |
| DR‑learner          | AIPW‑псевдо‑таргеты + регрессия $\tau(x)$         | да                | высокая (двойная робастность)              | средняя–высокая |
| Uplift trees / RF   | Сплиты, максимизирующие разность эффектов           | опционально       | зависит от критерия и штрафов              | средняя         |
| Causal Forest / GRF | Лес causal‑деревьев с ортогонализацией              | да                | высокая                                    | высокая         |

---

## 12. Выводы

- S‑ и T‑learners – простые базовые архитектуры; T‑learner обычно лучше при достаточных данных.
- X‑learner – рабочая лошадка для CATE/uplift, особенно при дисбалансе групп.
- R‑ и DR‑learners привносят ортогонализацию и двойную робастность, важны для корректного анализа причинных эффектов в сложных наблюдательных данных.
- Direct uplift‑модели и uplift‑деревья оптимизируют uplift более напрямую и удобны для интерпретируемой сегментации.
- Causal Tree / Forest (GRF) – теоретически обоснованные методы с возможностью получать доверительные интервалы для $\tau(x)$.

Понимая, как эти архитектуры связаны между собой (через псевдо‑таргеты, ортогонализацию и использование propensity), можно сознательно выбирать подход под конкретную задачу, данные и требования к интерпретируемости и статистической строгости.

