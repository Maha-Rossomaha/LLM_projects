# Мульти- и континуальные варианты

## 1. Постановка задачи

До сих пор предполагалось:

- бинарное воздействие $W \in \{0,1\}$;
- бинарный исход $Y \in \{0,1\}$ (конверсия / отклик).

В реальных задачах чаще:

- **многоуровневый treatment**:
  - несколько типов оффера ($W \in \{0,1,2,3\}$);
  - несколько тарифов / продуктов;
  - ступенчатые уровни скидки (0 %, 5 %, 10 %, 20 %...).

- **непрерывный treatment**:
  - размер скидки в процентах;
  - уровень маркетинговых затрат на пользователя;
  - ставка по кредиту, лимит, объём выдачи.

- **небинарный target**:
  - доход на клиента (ARPU, LTV);
  - регрессионный отклик (сумма покупки);
  - многоклассовая разметка (нет реакции / дешёвая покупка / дорогая покупка).

Объект интереса в общем виде:

- для дискретного treatment $w$:

  $$
  \tau_{a,b}(x) = \mathbb E[Y(a) - Y(b) \mid X=x], \quad a,b \in \mathcal W,
  $$

  где $\mathcal W$ – множество уровней воздействия;

- для непрерывного treatment $T \in \mathbb R$:

  - **dose-response функция**:

    $$
    \mu(t,x) = \mathbb E[Y(t) \mid X=x],
    $$

  - **маржинальный эффект**:

    $$
    \theta(t,x) = \frac{\partial}{\partial t} \mu(t,x),
    $$

    или разность при изменении дозы: $\mu(t_2,x) - \mu(t_1,x)$.

---

## 2. Non-binary treatment (многоуровневое воздействие)

### 2.1. Потенциальные исходы и эффекты

Пусть $W \in \{0,1,\dots,K\}$ – уровни воздействия (например, $0$ – нет оффера, $1$ – лёгкий оффер, $2$ – более агрессивный и т.д.).

Для каждого уровня определены потенциальные исходы:

$$
Y(0), Y(1), \dots, Y(K).
$$

Тогда **локальный эффект** между уровнями $a,b$:

$$
\tau_{a,b}(x) = \mathbb E[Y(a) - Y(b) \mid X=x].
$$

Типичные интересующие эффекты:

- "эффект любого воздействия против отсутствия": $\tau_{k,0}(x)$;
- "эффект повышения интенсивности" (например, 10 % скидки против 5 %): $\tau_{2,1}(x)$.

---

### 2.2. Стратегии моделирования

#### 2.2.1. Generalized S-learner

Обобщение обычного S-learner:

- одна модель $m(x,w) \approx \mathbb E[Y \mid X=x, W=w]$;
- на вход подаём $X$ и категориальный/числовой $W$;
- uplift между $a$ и $b$:

  $$
  \hat \tau_{a,b}^S(x) = \hat m(x,a) - \hat m(x,b).
  $$

Реализация:

- если $W$ категориальный → one-hot / embeddings;
- если уровни имеют естественный порядок → можно использовать числовой признак $W$ и модель с нелинейностями.

Плюсы:

- одна модель, упрощённая реализация;
- автоматически использует общую структуру между уровнями.

Минусы:

- при больших различиях между режимами может сглаживать специфические эффекты;
- как и обычный S-learner, может частично игнорировать $W$.

#### 2.2.2. Generalized T-learner

Обобщение T-learner:

- для каждого уровня $w$ строим модель

  $$
  \hat m_w(x) \approx \mathbb E[Y \mid X=x, W=w];
  $$

- эффект между $a$ и $b$:

  $$
  \hat \tau_{a,b}^T(x) = \hat m_a(x) - \hat m_b(x).
  $$

Плюсы:

- максимальная гибкость: каждый режим получает собственную модель;
- можно использовать разные классы моделей под разные уровни.

Минусы:

- если уровней много, данных на уровень может быть мало → переобучение;
- нужна координация между моделями (одинаковые признаки и т.п.).

Частый компромисс:

- объединять редкие уровни в один "остальные";
- моделировать только ключевые контрасты (например, каждый $k$ против $0$).

#### 2.2.3. Мультиклассовый propensity и IPW

Обобщённый propensity score для $W \in \{0,\dots,K\}$:

$$
 e_w(X) = P(W=w \mid X), \quad w=0,\dots,K.
$$

Для оценки эффекта $a$ против $b$ можно использовать IPW по подвыборке $W \in \{a,b\}$:

- переопределить бинарный treatment: $\tilde W = 1$ для уровня $a$, $0$ – для $b$;
- использовать веса на основе $P(W=a \mid X)$ и $P(W=b \mid X)$;
- применить стандартные формулы ATE/ATT.

На практике чаще комбинируют:

- outcome-модели $\hat m_w(x)$ (T- / S-learner);
- мультиклассовый propensity $e_w(x)$;
- DR-конструкции для конкретных контрастов $\tau_{a,b}(x)$.

---

### 2.3. Пример мотивации

Пример: кредитный продукт с 3 уровнями ставки (низкая / средняя / высокая). Цель – подобрать ставку так, чтобы:

- максимизировать ожидаемую прибыль;
- при этом не разрушить спрос.

Нужны оценки $\tau_{\text{low,mid}}(x)$, $\tau_{\text{mid,high}}(x)$ по целевому $Y$ (например, NPV). Далее стратегия:

- если $\tau_{\text{low,mid}}(x) > 0$ и $\tau_{\text{mid,high}}(x) > 0$ → можно смещаться к более высокой ставке;
- если $\tau_{\text{low,mid}}(x) < 0$, но $\tau_{\text{mid,high}}(x) > 0$ → лучше остаться на средней и не падать до низкой и т.п.

---

## 3. Continuous treatment (непрерывное воздействие)

### 3.1. Объект: dose-response функция

Пусть $T \in \mathbb R$ – непрерывный treatment (доза, ставка, размер скидки, маркетинговый спенд).

Потенциальные исходы: $Y(t)$ – результат при дозе $t$.

Интересующие объекта:

- **dose-response:**

  $$
  \mu(t,x) = \mathbb E[Y(t) \mid X=x];
  $$

- **маржинальный эффект (градиент по дозе):**

  $$
  \theta(t,x) = \frac{\partial}{\partial t} \mu(t,x);
  $$

- **оптимальная доза:**

  $$
  t^*(x) = \arg\max_t \; \mu(t,x) - \text{Cost}(t,x).
  $$

### 3.2. Generalized Propensity Score (GPS)

Аналог propensity для непрерывного treatment – **generalized propensity score**:

$$
 r(t,x) = f_{T\mid X}(t \mid x),
$$

где $f_{T\mid X}$ – условная плотность дозы при заданных ковариатах.

Свойство: при некоторых допущениях

- условно на $r(T,X)$ treatment "почти случайный";
- можно стратифицировать / регрессировать по $T$ и $r(T,X)$, чтобы восстанавливать $\mu(t,x)$.

Практика:

1. Моделируем $f_{T\mid X}$ (например, нормальный условный шум или более сложную модель).
2. Вычисляем оценки $\hat r(T_i, X_i)$.
3. Используем их в регрессии исхода $Y$ на $T$ и $\hat r$ (или их функции) для восстановления dose-response.

### 3.3. Ортогонализация для непрерывного treatment

Аналог Robinson-декомпозиции:

1. Оценить

   - $m(X) = \mathbb E[Y \mid X]$;
   - $g(X) = \mathbb E[T \mid X]$.

2. Рассмотреть модель

   $$
   Y - m(X) = h(T,X) + \varepsilon,
   $$

   где $h(T,X)$ кодирует эффект изменения T вокруг $g(X)$. Для линейных эффектов можно рассматривать форму

   $$
   Y - m(X) = \theta(X) (T - g(X)) + \varepsilon,
   $$

   и оценивать $\theta(X)$ как в R-learner, только с непрерывным treatment.

3. Для нелинейных эффектов $h(T,X)$ можно аппроксимировать сплайнами / нейросетями и т.п., при этом сохранять идею "partialling out" $m(X), g(X)$.

### 3.4. Дискретизация дозы как приближение

Часто вместо полноценного continuous-treatment Causal ML используют грубую дискретизацию:

- разбивают $T$ на интервалы (например, $[0,5], (5,10], (10,20] $ процентов скидки);
- решают задачу как multi-treatment uplfit (см. раздел 2);
- для практики этого достаточно, если
  - бизнес реально оперирует только установленными уровнями дозы;
  - размер выборки не позволяет надёжно оценивать $\theta(t,x)$ как функцию от непрерывного $t$.

---

## 4. Non-binary target (регрессия и многоклассовый отклик)

### 4.1. Регрессионный target

Если $Y$ – непрерывная величина (доход, чек, NPV), uplift‑объект:

$$
\tau(x) = \mathbb E[Y(1) - Y(0) \mid X=x]
$$

остаётся тем же, только теперь:

- outcome-модели $m_w(x)$ – регрессия;
- метрики качества – MSE / MAE, но главное – качество по uplift-метрикам (incremental revenue);
- Qini/AUUC можно строить по инкрементальному доходу.

Всё, что описано для S/T/X/R/DR-learner, переносится напрямую, меняется только лосс на стадии обучения исходов.

### 4.2. Многоклассовый target

Если $Y$ принимает несколько категорий (например, $0$ – нет покупки, $1$ – дешёвая, $2$ – дорогая):

1. **Фокус на одном классе**

   Можно выбрать бизнес-значимый класс (например, $Y=2$) и работать с бинарным индикатором $\mathbb I\{Y=2\}$ как таргетом для uplift.

2. **Перевод в регрессию через utility**

   Задать ценность каждой категории:

   $$
   U(Y) = u_0 \mathbb I\{Y=0\} + u_1 \mathbb I\{Y=1\} + u_2 \mathbb I\{Y=2\},
   $$

   и рассматривать uplift по $U(Y)$ как регрессионный кейс.

3. **Векторный uplift по классам**

   Строить модель вероятностей классов:

   $$
   p_w(y \mid x) = P(Y=y \mid X=x, W=w), \quad y \in \{0,1,2\},
   $$

   и считать uplift по каждому классу или по функционалу от распределения (например, ожидаемый доход). Это по сути multi-output S-/T-learner.

---

## 5. Обобщённые meta-learners (S/T/X для мульти- и continuous treatment)

### 5.1. Generalized S-learner

Уже описан для multi-treatment. Для continuous treatment:

- строим $m(x,t) \approx \mathbb E[Y \mid X=x, T=t]$;
- используем регрессионную модель с входом $(X,T)$;
- оценка эффекта при переходе из $t_1$ в $t_2$:

  $$
  \hat \tau_S(x; t_2,t_1) = \hat m(x,t_2) - \hat m(x,t_1).
  $$

Если модель дифференцируема по $t$, можно оценивать $\partial \hat m / \partial t$ как локальный эффект.

Проблемы те же: модель может плохо использовать $T$, особенно если вариация $Y$ по $X$ сильнее, чем по $T$.

### 5.2. Generalized T/X-learners

Для multi-treatment:

- T-learner уже естественно обобщён (модель на каждый $w$);
- X-learner можно строить для пары уровней $a,b$:
  - шаг 1: оценить $m_a(x), m_b(x)$;
  - шаг 2: построить псевдо-эффект $D^{(a)}, D^{(b)}$ и регрессии по каждой группе.

Для continuous treatment возможно:

- дискретизировать $T$ на несколько уровней и применять X-learner как для мульти-treatment;
- или строить псевдо-таргеты для пар доз ($t_2$ vs $t_1$) для выборок, где реально наблюдались обе дозы (что на практике ограничено).

Чистые X-/T-подходы для строго непрерывного treatment обычно заменяют на GPS- и ортогональные методы (см. раздел 3 и 6).

### 5.3. Generalized R-/DR-learners

R-/DR-логика обобщается естественно:

- R-learner: центрируем и $Y$, и $T$ по $X$, затем учим $\theta(X)$ в

  $$
  Y - m(X) = \theta(X) (T - g(X)) + \varepsilon,
  $$

  что даёт местный эффект изменения $T$ вокруг $g(X)$;

- DR-learner дляmulti-treatment: используем AIPW-подобные псевдо-таргеты для $\tau_{a,b}(x)$, комбинируя $m_a, m_b$ и мультиклассовый $e_w(x)$.

Идея та же, что и при бинарном treatment: ортогонализация и двойная робастность, но теперь пространство treatment богаче.

---

## 6. Generalized Random Forests (GRF) и causal ML

GRF (Generalized Random Forests) – фреймворк, который:

- решает задачи оценки условных параметров (включая CATE) через ансамбль деревьев;
- поддерживает различные задачи: бинарный / continuous treatment, инструменты, квантили и т.д.

Для непрерывного treatment и CATE можно:

- использовать модификации causal forest / orthogonal random forest;
- задать задачу в виде ортогонального условия момента (как в R-/DR-learner);
- позволить лесу строить локальные веса соседей $x$ и решать локальную регрессию.

Интерпретация:

- деревья определяют "соседство" объектов по $X$;
- в каждом узле/листе решается локальная оценка эффекта изменения $T$;
- итоговая $\hat \theta(x)$ – взвешенное по лесу решение.

Плюсы:

- хорошо работает в high-dimensional задачах;
- есть теоретические результаты по состоятельности и доверительным интервалам;
- естественно поддерживает сложные зависимости между $X$, $T$, $Y$.

Минусы:

- сложность реализации и настройки;
- для production в бизнес-задачах часто избыточен, если достаточно дискретизировать treatment.

---

## 7. Пример схемы обучения (discount uplift)

Сценарий: e-commerce, где $T$ – размер скидки (в процентах), $Y$ – доход с клиента за период. Задача – подобрать скидку под пользователя.

**Шаг 1. Подготовка данных**

- собрать $(X_i, T_i, Y_i)$ за несколько кампаний;
- выбросить экстремальные $T$, где слишком мало наблюдений;
- при необходимости – дизбаланс по $T$ скорректировать (reweighting / subsampling).

**Шаг 2. Моделирование dose-response**

Вариант A (дискретизация):

- разбить $T$ на несколько уровней: 0 %, 5–10 %, 10–20 %, >20 %;
- применить multi-treatment T-/X-learner;
- получить оценки $\hat m_w(x)$ и выбирать $w$ с максимальным $\hat m_w(x) - Cost(w,x)$.

Вариант B (continuous treatment):

1. Оценить $m(X) = \mathbb E[Y \mid X]$ и $g(X) = \mathbb E[T \mid X]$.
2. Оценить $\theta(X)$ в модели

   $$
   Y - m(X) = \theta(X) (T - g(X)) + \varepsilon
   $$

   с помощью R-learner / orthogonal forest.

3. В окрестности текущего уровня скидки $T_0$ использовать $\hat \theta(X)$ как локальный градиент:

   - если $\hat \theta(X) > 0$ (дополнительная скидка повышает доход с учётом эластичности спроса) – можно увеличить $T$;
   - если $< 0$ – уменьшать $T$ или держать на месте.

**Шаг 3. Решение для нового клиента**

- для заданного $x$ оцениваем $\hat m(x,t)$ или $\hat \theta(x)$;
- выбираем $t$ (или ближайший уровень $w$), максимизирующий ожидаемый NPV / маржу;
- ограничиваем диапазон $t$ бизнес-ограничениями (минимальная маржа, регуляторные лимиты и т.п.).

---

## 8. Краткие выводы

- Мульти- и continuous treatment – не "другая" задача, а обобщение тех же идей CATE/uplift.
- Для многоуровневых режимов достаточно обобщить S/T/X-/DR-подходы и, при необходимости, использовать мультиклассовый propensity.
- Для непрерывного treatment ключевые объекты – dose-response $\mu(t,x)$ и её производная; инструменты: GPS, ортогонализация и GRF / orthogonal forests.
- Небинарный target естественно обрабатывается через регрессию или utility-функцию от классов.
- В прикладных проектах часто выигрывает гибридный подход: разумная дискретизация treatment + проверенные бинарные uplift-архитектуры, а continuous Causal ML используют там, где действительно критична точная оптимизация дозы / ставки.

