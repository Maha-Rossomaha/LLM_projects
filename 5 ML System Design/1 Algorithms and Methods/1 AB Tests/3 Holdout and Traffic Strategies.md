# A/B/n‑тесты, holdout и трафиковые стратегии

> **Идея:** когда вариантов больше, чем просто A и B, важным становится не только сам статистический тест, но и **стратегия разбиения трафика**: как делить пользователей, как держать «чистый» контроль, как выкатывать новые версии без риска.
>
> * **A/B/n‑тесты:** несколько тестовых вариантов против одного контроля.
> * **Holdout‑группа:** постоянный контроль, который не трогаем, чтобы оценивать долгосрочный эффект.
> * **Split‑трафик:** как именно делим трафик по группам (50/50, 90/10, динамика, bandits и т.п.).
> * **Champion/Challenger, shadow deploy:** стратегии безопасного выката и сравнения моделей/фич.

---

## 1. A/B vs A/B/n‑тесты

### 1.1. Базовые определения

* **A/B‑тест:**

  * 1 контрольный вариант (**A**), 1 тестовый (**B**).
  * Проверяем гипотезу вида: метрика у B лучше/не хуже, чем у A.

* **A/B/n‑тест:**

  * 1 контроль (**A**) и **несколько** тестовых вариантов: **B₁, B₂, …, Bₙ**.
  * Примеры:

    * 3 разных дизайна карточки товара,
    * несколько вариантов рекомендательной модели,
    * разные схемы скидок/промо.

Обозначения (для бинарной метрики, например, конверсии):

* $p_A$ — конверсия в контроле.
* $p_{B_1}, p_{B_2}, \dots, p_{B_n}$ — конверсии в тестовых вариантах.
* Эффект варианта $k$: $\Delta_k = p_{B_k} - p_A$ (абсолютный) или $\Delta_k^{rel} = \frac{p_{B_k} - p_A}{p_A}$.

### 1.2. Задачи A/B/n‑теста

Основные вопросы:

1. Есть ли **хотя бы один** вариант, который статистически отличается от контроля?
2. Какой **вариант лучший** с точки зрения метрики (и статистически, и практически)?
3. Не получаем ли мы «победителя по случайности» из‑за большого числа вариантов?

Для этого нужно учитывать:

* **множественные сравнения** (много гипотез A vs B₁, A vs B₂, …),
* **MDE** и мощность для каждого сравнения,
* **объём трафика**, который приходится на каждый вариант.

---

## 2. Контрольные группы и holdout

### 2.1. Обычный контроль в A/B(/n)

В классическом A/B(/n)‑тесте:

* Контрольная группа **A**:

  * получает текущую «промышленную» логику,
  * нужна для сравнения с тестовыми вариантами.
* После завершения эксперимента:

  * либо «победитель» выкатывается на весь трафик,
  * либо продолжаем эксперимент в другом дизайне.

Контроль в таком тесте **временный**, завязан на конкретную гипотезу.

### 2.2. Holdout‑группа

**Holdout** — это **постоянная контрольная группа**, которая **не получает никаких новых фич** (или получает их сильно позже).

Идея:

* Всегда держать X% пользователей (например, 1–10%) в «старом мире»,
* и сравнивать:

  * метрики основной массы пользователей,
  * с метриками holdout‑группы.

Используется для:

* **долгосрочного мониторинга**: как суммарное влияние всех выкатываемых изменений влияет на метрики,
* **оценки накопленного эффекта** (например, от десятков релизов за год),
* **обнаружения деградаций**: если метрики основной популяции просели относительно holdout.

Важно:

* Holdout **не равен** контролю из одного конкретного A/B‑теста.
* Это **структурная часть системы экспериментов**, которая всегда остаётся неизменной (или меняется крайне редко и осознанно).

---

## 3. Split‑трафик: как делим пользователей

### 3.1. Основные варианты разбиения

Пусть есть общий трафик (пользователи / сессии / запросы). Нужно разделить его:

* Между **контролем и тестом** (A/B);
* Между **несколькими тестовыми вариантами** (A/B/n);
* Между **продакшен‑моделью и экспериментальными моделями**.

Частые схемы:

* **Равномерный split:**

  * A/B: 50/50,
  * A/B/C: 33/33/34,
  * A/B/n: примерно 1/(n+1) на группу.
* **Неравномерный split:**

  * A/B: 90/10 или 80/20,
  * A/B/n: контроль крупнее, варианты меньше.

Решается задачами:

* Сколько риска мы готовы взять на себя (сколько трафика отдаём «сомнительным» фичам),
* Насколько быстро хотим собрать статистически значимый результат.

### 3.2. Пользовательский vs запросный split

* **По пользователям:**

  * один пользователь всегда попадает в одну и ту же группу по стабильному ключу (user_id, cookie и т.п.);
  * хорошо для фич, которые влияют на поведение пользователя во времени (UI, рекомендации, цены).

* **По запросам/сессиям:**

  * каждый запрос может уйти в любую группу;
  * используется, когда решение принимается «на запрос» и нет сильной зависимости по пользователю.

Важно обеспечить **стабильный, детерминированный split**, чтобы пользователь не «прыгал» между вариантами.

### 3.3. Динамические стратегии трафика

Помимо фиксированных долей (50/50 и т.п.), возможны динамические стратегии:

* Сначала 5–10% трафика на новый вариант,
* Постепенное увеличение доли при отсутствии деградаций,
* Остановка/откат, если метрики портятся.

Это уже ближе к **progressive rollout** и **bandit‑стратегиям**, но логика разбиения трафика по сути такая же.

---

## 4. Мультивариантные тесты (A/B/n)

### 4.1. Проблема множественных сравнений

В A/B/n‑тесте мы одновременно проверяем несколько гипотез:

* $H_{0,k}: p_{B_k} = p_A$ против $H_{1,k}: p_{B_k} \ne p_A$ (или «>»).

Если смотреть на p‑value каждого варианта **по отдельности** при уровне значимости $\alpha$:

* вероятность хотя бы одной ложной тревоги **растёт** с числом вариантов;
* можно «случайно» выбрать «победителя», который на самом деле не лучше.

Поэтому в A/B/n‑тестах часто используют:

* **Консервативные поправки**:

  * Bonferroni: тестируем на уровне $\alpha' = \alpha / m$, где $m$ — число сравнений;
  * Holm, Hochberg и другие step‑up/step‑down процедуры.
* **Контроль FDR (False Discovery Rate)**:

  * менее консервативный, но позволяет контролировать долю ложных «открытий».

### 4.2. Распределение трафика между вариантами

При A/B/n надо помнить, что:

* При фиксированном общем трафике на **каждый** вариант идёт меньше данных;
* MDE для каждого сравнения при этом **растёт** (или требуется дольше крутить эксперимент).

Типичная практика:

* Не делать A/B/n с десятком вариантов на реальном трафике;
* Сначала сузить множество кандидатов оффлайн или на синтетике;
* В онлайн выносить 2–3 наиболее перспективных challengers.

---

## 5. Champion / Challenger

### 5.1. Определения

* **Champion (чемпион):**

  * текущая «боевая» версия системы (модель, алгоритм, UI);
  * уже проверена и считается достаточно хорошей.

* **Challenger (претендент):**

  * новый кандидат, который пытается «сместить» чемпиона;
  * проверяется в эксперименте (A/B или A/B/n), где A — champion, B — challenger.

Это естественная схема эволюции систем:

1. Есть champion (baseline).
2. Появляется challenger.
3. Запускаем эксперимент champion vs challenger.
4. Если challenger статистически и практически лучше — он становится новым champion.

### 5.2. Трафиковые стратегии для champion/challenger

Частые схемы:

* **Малый трафик на challenger (10–20%)**:

  * снижает риск;
  * эксперимент идёт дольше, но безопаснее.

* **50/50 split champion vs challenger**:

  * быстрее собираем статистику;
  * выше риск потерь, если challenger плох.

* **Постепенное увеличение доли challenger**:

  * сначала 5–10%, затем 20–30%, позже 50% и т.д.,
  * при ухудшении метрик возвращаемся к champion.

---

## 6. Shadow deploy (теневой запуск)

### 6.1. Идея

**Shadow deploy** (shadow mode, dark launch):

* новый вариант (модель, сервис) **получает тот же трафик, что и прод**,
* но его ответы **НЕ влияют** на пользователя/бизнес,
* результаты записываются в логи для анализа.

То есть у нас две системы:

* «боевая» (основная) — её ответы идут в продукт,
* «теневая» — получает те же запросы, но живёт «в тени».

### 6.2. Зачем нужен shadow deploy

* Проверить **производительность** (latency, нагрузка, ресурсы) на реальном трафике;
* Оценить **распределение выходов** новой модели без риска (например, кредитные скоринги, антифрод);
* Найти **краевые кейсы и сбои**, которые не видны на оффлайн‑данных;
* Сравнить метрики «как если бы» мы использовали новую модель.

Часто pipeline выглядит так:

1. Оффлайн‑оценка модели.
2. Shadow deploy на небольшой части/всём трафике.
3. Если поведение адекватное — A/B‑тест champion vs новый вариант.

---

## 7. Трафиковые стратегии и сценарии использования

### 7.1. Когда что использовать

1. **Простая фича/изменение UI, низкий риск:**

   * A/B‑тест 50/50 или 70/30.

2. **Несколько вариантов дизайна, трафика достаточно:**

   * A/B/n‑тест (2–3 варианта),
   * контроль множественных сравнений.

3. **Сильно рискованная фича/модель (цены, кредитные лимиты):**

   * сначала shadow deploy,
   * затем A/B champion vs challenger с малой долей трафика (например, 90/10),
   * возможно, долгий горизонт измерения.

4. **Долгосрочный эффект множества релизов:**

   * постоянный holdout (1–10% пользователей),
   * регулярный отчёт: как пром изменился относительно holdout.

### 7.2. Комбинация подходов

Жизненный сценарий:

1. Есть champion‑модель / алгоритм.
2. Разрабатываем несколько challengers.
3. Оффлайн‑фильтрация → 1–2 лучших кандидата.
4. Shadow deploy для проверки стабильности.
5. A/B/n‑тест champion + challengers, разумный split трафика.
6. Победитель становится новым champion.
7. Holdout‑группа позволяет увидеть накопленный эффект выката нескольких поколений моделей.

---

## 8. Что фиксировать в отчёте

При описании A/B/n‑тестов и трафиковых стратегий полезно явно указывать:

* **Тип эксперимента:** A/B или A/B/n, какие варианты участвуют.
* **Контроль:** кто является champion / baseline.
* **Тип метрики:** бинарная (CR/CTR), непрерывная (LTV, чек), ratio‑метрики.
* **Схема разбиения трафика:**

  * доли по вариантам (например, A: 50%, B: 25%, C: 25%),
  * по пользователям/сессиям,
  * наличие и размер holdout.
* **Горизонт эксперимента:**

  * минимальное время (дни/недели),
  * минимальный размер выборки.
* **Статистические детали:**

  * какой тест (z, t, непараметрический, бутстрап, перестановочный),
  * как учитывались множественные сравнения.
* **Стратегию выката:**

  * есть ли shadow deploy,
  * как менялась доля трафика у challenger,
  * критерии промоутирования challenger → champion.

Это помогает не превращать A/B/n‑тесты в хаотичный набор запусков, а держать в голове целостную картину: кто с кем сравнивается, на каком трафике и как это влияет на риск и скорость принятия решений.
