# A/B‑тестирование в ML‑контексте

> **Идея:** для ML‑систем (рекомендатели, скоринговые модели, ранжирование, таргетинг) A/B‑тест — это не только «сравнить среднее», а часть цикла: оффлайн‑оценка → безопасный rollout → онлайн‑эксперимент → интерпретация эффекта.
>
> Ключевые элементы:
> - **Feature flags** и управление включением моделей/фич.
> - **Стратегии выката (rollout)** моделей: canary, progressive, shadow.
> - **Оффлайн vs онлайн метрики** и их связка.
> - **A/B‑тесты + uplift‑модели**: как совместить предсказания uplift с экспериментами.

---

## 1. A/B‑тест как часть ML‑пайплайна

### 1.1. Где A/B‑тесты встраиваются в ML‑жизненный цикл

Типичный цикл разработки ML‑модели для продукта:

1. **Формулировка задачи** (клик, конверсия, выручка, churn, вероятность отклика).
2. **Сбор и подготовка данных**, выбор признаков.
3. **Оффлайн‑обучение и валидация**:
   - метрики качества (AUC, logloss, NDCG, RMSE и т.п.),
   - сравнение с baseline‑моделью.
4. **Продакшен‑внедрение**:
   - деплой модели,
   - интеграция с сервисами.
5. **Онлайн‑оценка через A/B‑тест**:
   - связь «улучшения оффлайн‑метрик» и **онлайн‑бизнес‑метрик**.

A/B‑тест отвечает на вопрос:

> «Оправдывается ли сложность новой модели реальным улучшением продукта/бизнеса на живых пользователях?»

### 1.2. A/B‑тест vs просто смена модели

Смена модели без эксперимента опасна:

- оффлайн‑метрики могут не коррелировать с бизнес‑метриками;
- меняется дистрибуция данных (feedback loop, сдвиги в поведении);
- модель может вести себя нестабильно в углах распределения.

A/B‑тест позволяет:

- сравнить **старую (champion)** и **новую (challenger)** модели на одном и том же потоке запросов/пользователей;
- измерить **каузальный эффект** от новой модели.

---

## 2. Feature flags и управление моделями

### 2.1. Что такое feature flag в ML‑контексте

**Feature flag (фича‑флаг)** — технический переключатель, позволяющий:

- включать/выключать модель или фичу **без релиза кода**;
- управлять долями трафика, попадающими на разные варианты;
- быстро откатывать изменения.

На уровне логики:

- запрос попадает в систему;
- по user_id / experiment_id определяется группа (A/B или более сложная схема);
- флаг решает, какую модель вызывать: baseline или новую.

### 2.2. Типы feature flags

- **Binary flag**: включено/выключено (например, «использовать новую модель рекомендаций»).
- **Percentage flag / traffic allocation**:
  - X% трафика получают новый вариант,
  - (100 − X)% — старый.
- **Targeted flags**:
  - включение модели только для определённых сегментов (страна, устройство, тип клиента).

### 2.3. Требования к флагам

- **Детерминированность**: один и тот же пользователь попадает в одну и ту же группу.
- **Стабильность**: при изменении трафиковых долей старые пользователи не «прыгают» между моделями (если не задумано иначе).
- **Логирование**: флаг + версия модели должны попадать в логи для последующего анализа.

---

## 3. Rollout моделей: стратегии

### 3.1. Полная замена vs постепенный rollout

В ML‑системах полная замена модели «одним щелчком» редко безопасна.

Чаще используются стратегии:

1. **Canary release**:
   - сначала маленький сегмент трафика (например, 1–5%),
   - проверка стабильности (латентность, ошибки, грубые метрики),
   - затем постепенное увеличение доли.

2. **Progressive rollout**:
   - последовательное расширение доли трафика:
     5% → 10% → 25% → 50% → 100%,
   - на каждом шаге — мини‑анализ метрик и ошибок.

3. **Shadow deploy** (теневой запуск):
   - новая модель получает копию запросов,
   - её ответы записываются в логи, но **не влияют** на пользователя;
   - используется до запуска A/B‑теста для проверки «адекватности» поведения.

### 3.2. Champion / challenger в rollout

- **Champion** — текущая production‑модель.
- **Challenger** — новый кандидат.

Схема:

1. Оффлайн‑сравнение моделей.
2. Shadow‑режим на части трафика.
3. A/B‑тест: champion vs challenger.
4. При успехе challenger становится новым champion.

### 3.3. Rollback и аварийные сценарии

У любой стратегии rollout должны быть:

- **условия остановки/отката**:
  - рост ошибок (5xx, timeouts),
  - деградация ключевых бизнес‑метрик,
  - всплеск жалоб/поддержки;
- **механизм быстрого отката** (выключить флаг, вернуть champion‑модель);
- **але́рты** (SLA/SLO) на уровне логов/метрик.

---

## 4. Оффлайн vs онлайн метрики

### 4.1. Оффлайн‑метрики качества модели

Примеры:

- **Классификация**:
  - AUC‑ROC, AUC‑PR,
  - logloss, Brier score,
  - accuracy, F1 (с оговорками по дисбалансу).
- **Ранжирование / рекомендации**:
  - NDCG@k, MAP@k,
  - Recall@k, HitRate,
  - MRR.
- **Регрессия**:
  - RMSE, MAE,
  - R² (осторожно интерпретировать).

Оффлайн‑метрика оценивает:

> Насколько хорошо модель воспроизводит исторические данные / таргет на тестовых выборках.

### 4.2. Онлайн‑метрики (бизнес + поведение)

Онлайн‑метрики измеряются **на реальных пользователях** в A/B‑тесте:

- **Бизнес‑метрики**:
  - выручка, средний чек,
  - CR, ARPU, LTV,
  - маржа, прибыль.
- **Продуктовые/поведенческие**:
  - CTR, глубина сессии, время в продукте,
  - ретеншн, churn,
  - share of search / watch time / доля рекомендованных просмотров.
- **Технические**:
  - latency (p95, p99),
  - error rate,
  - нагрузка на инфраструктуру.

### 4.3. Разрыв между оффлайн и онлайн метриками

Проблемы:

- **Неполное соответствие цели обучения и цели бизнеса**:
  - модель оптимизирует surrogate‑метрику (CTR), но бизнесу важен LTV;
- **Изменение поведения пользователей** под действием модели:
  - feedback loops, пользователи адаптируются к новому ранжированию;
- **Distribution shift**:
  - новые пользователи, новые паттерны.

Вывод:

- Оффлайн‑метрика — фильтр кандидатов.
- Онлайн‑A/B — окончательное решение о полезности.

### 4.4. Связка оффлайн ↔ онлайн

Желательно:

- явно фиксировать **primary online‑метрики**, которые модель должна улучшать;
- подбирать оффлайн‑метрики, которые **максимально коррелируют** с ними;
- анализировать:
  - случаи, где оффлайн‑улучшение не дало онлайн‑эффекта,
  - и наоборот (поиск багов, недоучтённых факторов).

---

## 5. A/B‑тесты и uplift‑модели

### 5.1. Что такое uplift‑модель

**Uplift‑модель** предсказывает **разницу отклика при наличии/отсутствии воздействия** (treatment):

- классическая ML‑модель предсказывает $P(Y=1 \mid X)$,
- uplift‑модель пытается оценить $\tau(X) = E[Y \mid T=1, X] - E[Y \mid T=0, X]$,

где:

- $Y$ — исход (конверсия, покупка, отклик),
- $T$ — treatment (показ оффера, таргетинг и т.п.),
- $X$ — признаки пользователя/сессии.

Задача: **найти тех пользователей, для которых воздействие даёт максимальный прирост**, а не просто высокую вероятность $Y=1$.

### 5.2. Почему uplift‑модели не отменяют A/B‑тесты

Даже если есть uplift‑модель, всё равно нужен A/B‑тест:

- обучение модели опирается на исторические экспериментальные данные,
- применение модели **меняет политику воздействия** (кто получает оффер),
- это создаёт новый data‑generating process.

Нужен A/B‑тест, чтобы оценить:

> Насколько стратегия таргетинга на основе uplift‑модели лучше простых бейзлайнов (random, rule‑based, propensity‑model) по онлайн‑метрикам.

### 5.3. Дизайн A/B для uplift‑моделей

Варианты:

1. **Policy‑A/B**:
   - Группа A: простая стратегия (например, всем или rule‑based).
   - Группа B: стратегия на основе uplift‑модели (только top‑q процентов по $\tau(X)$).
   - Сравниваются бизнес‑метрики (выручка, CR, ROI кампании).

2. **A/B внутри таргетируемой популяции**:
   - Сначала отбираем пользователей, которых **вообще имеет смысл таргетировать** (по uplift/score).
   - Далее среди них запускаем A/B:
     - T: получившие воздействие,
     - C: не получившие,
   - сравниваем uplift в целевой группе.

### 5.4. Метрики для uplift‑экспериментов

Помимо стандартных CR/выручки, специфичны:

- **CATE‑оценки** по сегментам (Conditional Average Treatment Effect);
- uplift‑метрики по ранжированию:
  - Qini‑коэффициент,
  - uplift‑кривые,
  - AUUC (Area Under Uplift Curve).

Но решающее значение всё равно имеет **онлайн‑эффект на бизнес‑метриках**.

---

## 6. Практический шаблон A/B‑теста для модели

### 6.1. Планирование

1. **Формулируем цель**:
   - какую онлайн‑метрику хотим улучшить (primary),
   - какие метрики считаем вторичными.
2. **Определяем baseline**:
   - текущая модель / простое правило (champion).
3. **Выбираем дизайн эксперимента**:
   - A/B или A/B/n;
   - по пользователям или запросам;
   - доли трафика (например, 90/10 → 50/50).
4. **Планируем горизонты**:
   - длительность теста,
   - минимальный размер выборки,
   - MDE, α, мощность.

### 6.2. Техническая реализация

- Feature flags для включения/выключения новой модели.
- Логирование:
  - id пользователя/запроса,
  - версия модели,
  - флаг варианта (A/B),
  - входные признаки (по возможности в урезанном виде),
  - предсказания и фактический исход.
- Мониторинг технических метрик (latency, ошибки).

### 6.3. Анализ результатов

1. **Проверка качества эксперимента**:
   - балансировка групп по ключевым признакам (если нужно);
   - отсутствие багов в логике флагов/раутинга.
2. **Оценка эффекта**:
   - разница по primary‑метрике (absolute/relative uplift);
   - доверительные интервалы;
   - p‑value / байесовская оценка.
3. **Побочные эффекты**:
   - влияние на вторичные метрики (retention, churn, нагрузка и т.п.).
4. **Интерпретация**:
   - business case: перевод uplift в деньги и риски;
   - решение: промоутировать модель / дообучить / откатить.

---

## 7. Что фиксировать в отчёте по ML‑A/B‑тесту

Рекомендуемый минимум:

1. **Описание модели**:
   - чем challenger отличается от champion (фичи, архитектура, данные).
2. **Дизайн эксперимента**:
   - схема разбиения трафика;
   - по пользователям или запросам;
   - длительность, размер выборки, MDE.
3. **Метрики**:
   - primary и secondary;
   - оффлайн‑метрики (до эксперимента) и их связь с онлайн‑результатом.
4. **Результаты**:
   - absolute/relative uplift по ключевым онлайн‑метрикам;
   - доверительные интервалы, p‑values;
   - анализ по сегментам (если релевантно).
5. **Риски и выводы**:
   - есть ли деградации на отдельных сегментах;
   - оценка стабильности эффекта;
   - итоговое решение по rollout и план дальнейших шагов.

Такой формат позволяет рассматривать A/B‑тест не как разовый статистический ритуал, а как нормальный элемент ML‑инженерии: обе ноги — и оффлайн‑качество, и онлайн‑эффект на продукт и бизнес.

