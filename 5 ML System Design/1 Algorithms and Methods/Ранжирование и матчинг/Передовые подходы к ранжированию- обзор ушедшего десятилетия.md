---
Owner: IIvan Vashchenko
---
> [!important]
 > - Смещение (bias) в данных существенно влияет на качество моделей ранжирования. Для учёта смещения используются подходы, такие как **Unbiased LambdaMART**.
> - Списочные модели, такие как **DLCM** и **SetRank**, учитывают весь список документов и взаимодействие между ними, что улучшает качество ранжирования.
> - Использование эмбеддингов и комбинированных моделей, таких как **DUET-NDRM**, позволяет учитывать как точные совпадения, так и семантическое сходство между текстами.
> - **K-NRM** использует ядра для оценки релевантности и позволяет дообучать эмбеддинги под конкретную задачу, что делает модель более адаптивной и эффективной.

# Смещение в задачах ранжирования

Смещение (bias) в задачах ранжирования может быть нескольких типов:

1. **Позиционное смещение** — зависимость от позиции документа в выдаче. Например, документы, расположенные выше, получают больше кликов, даже если они менее релевантны.
2. **Социальное смещение** — вызвано различиями в восприятии людей из разных социальных групп. Например, пользователи из разных стран могут по-разному оценивать релевантность одних и тех же документов.
3. **Смещение из-за взаимодействия с системой** — обусловлено особенностями интерфейса или пользовательского поведения. Например, пользователь может не заметить документ, если для его нахождения требуется выполнить неочевидное действие.

Для учёта смещения в задачах ранжирования применяются различные модели, такие как **Generalized Bias Model**, использующая логистические регрессии для учёта позиции документов в выдаче.

  

# Unbiased LambdaMART

При обучении моделей ранжирования важно учитывать смещение в данных для повышения качества ранжирования. **Unbiased LambdaMART** — это усовершенствованная версия LambdaMART, которая корректирует функцию потерь с учётом позиционного смещения.

Функция потерь в Unbiased LambdaMART рассчитывается следующим образом:

$L_{unbiased} = \frac{L(f, x, c)}{t^+ t^-}$

Где:

- $L(f, x, c)$ — исходная функция потерь.
- $t^+$, $t^-$ — позиционные коэффициенты для положительных и отрицательных примеров соответственно.

Эти коэффициенты корректируют значение функции потерь в зависимости от позиции документа в выдаче, тем самым уменьшая влияние позиционного смещения.

  

# Listwise Context Model (DLCM)

**Deep Listwise Context Model (DLCM)** — это модель, учитывающая весь список документов при ранжировании, что делает её по-настоящему списочной (listwise). Основная идея заключается в том, чтобы каждый документ учитывал информацию о других документах в списке.

Архитектура **DLCM** включает рекуррентную нейронную сеть (например, GRU), которая обрабатывает документы снизу вверх, передавая контекстное состояние (hidden state) от одного документа к другому. Это позволяет учитывать взаимосвязь между документами и корректировать их релевантность в зависимости от контекста.

Функция потерь для **DLCM** основана на механизме внимания (attention), который предсказывает, сколько внимания пользователь уделит каждому документу. Формула конвертации предсказаний во внимание:

$\Huge a_i = \frac{e^{w_g x_i}}{\sum_{j=1}^{M} e^{w_g x_j}}$

Где:

- $w_g$ — обучаемый вектор весов.
- $x_i$ — вектор признаков для документа i.

  

# SetRank

**SetRank** — это подход, вдохновлённый архитектурой трансформеров. Основная идея заключается в учёте взаимодействия каждого документа с каждым другим документом в списке (концепция "каждый с каждым").

Архитектура SetRank включает несколько слоёв трансформеров, которые обрабатывают документы, учитывая их позиционное кодирование и информацию о запросе. На выходе модель предсказывает значение релевантности для каждого документа, после чего выполняется финальная сортировка.

Важной особенностью SetRank является его **инвариантность к порядку входных документов**. Это означает, что модель может адаптироваться к различным перестановкам документов и сохранять качество ранжирования.

  

# Embeddings и DUET-NDRM

Для ранжирования часто используются векторные представления слов и текстов, называемые **эмбеддингами**. Существует два основных типа моделей для работы с текстами:

1. **Локальные модели** — учитывают точное совпадение слов между запросом и документом, что полезно для задач, требующих учёта уникальных идентификаторов (например, артикулов товаров).
2. **Распределённые модели** — представляют слова и текст в виде векторов в латентном пространстве, что позволяет учитывать семантическую схожесть.

**DUET-NDRM** (Neural Document Ranking Model) — это ансамбль локальной и распределённой моделей. Оценки релевантности от обеих моделей объединяются для получения финального предсказания.

  

# CNNs для сопоставления предложений на естественном языке

Для задачи сопоставления текстов могут использоваться **сверточные нейронные сети (CNNs)**. На вход подаётся текстовая последовательность, которая представляется в виде матрицы эмбеддингов. Затем над этой матрицей выполняются операции свёртки и пулинга для извлечения признаков, характеризующих текст.

Пример операции свёртки по тексту:

$F(x) = \sum_{i=1}^{k} w_i x_i$

Где:

- $w_i$ — веса свёртки.
- $x_i$ — элементы входной последовательности.

Операция пулинга (например, MaxPooling) позволяет выделить наиболее значимые признаки из каждого фрагмента текста.

  

# End-to-End Neural Ad-hoc Ranking with Kernel Pooling (K-NRM)

**K-NRM** — это архитектура, использующая **ядерную свёртку (kernel pooling)** для оценки релевантности между словами запроса и документа. В качестве меры схожести используется **косинусное сходство**, а полученная матрица сходства сворачивается с помощью нескольких ядер (kernels).

Формула для расчёта значения ядра:

$\large K_k(M) = \sum_{i} \exp\left(-\frac{(M_{ij} - \mu_k)^2}{2\sigma_k^2}\right)$

Где:

- $M_{ij}$ — косинусное сходство между словом из запроса и словом из документа.
- $\mu_k,$ $\sigma_k$ — параметры ядра, задающие его центр и ширину.

Ядра позволяют оценить плотность распределения метрики косинусного сходства в различных частях интервала, что даёт возможность учитывать различные уровни схожести между словами.