---
Owner: IIvan Vashchenko
---
> [!important] 
> Трансформеры и **BERT** изменили подход к задачам NLP, обеспечив высокую точность и эффективность. Использование двунаправленного внимания и механизмов self-attention позволяет моделям понимать тексты глубже и учитывать как левый, так и правый контексты для каждого слова.
> 
> **BERT** и его производные (такие как **RoBERTa**, **DistilBERT**) применяются в самых различных областях — от анализа тональности до сложных систем поиска. Эти модели стали фундаментом для современных NLP-систем, и их использование в комбинации с другими архитектурами продолжает улучшать качество и точность работы с текстами.
> 
> Модели, такие как **CEDR**, **YATI**, **ColBERT** и многоступенчатое ранжирование, демонстрируют, как **BERT** и его модификации могут быть адаптированы для специфических задач, таких как ранжирование и поиск. Эти подходы показывают, что даже предобученные модели могут быть успешно адаптированы для решения узкоспециализированных задач, обеспечивая высокую точность и масштабируемость.

# Введение в трансформеры и BERT

**Трансформеры** — это архитектура, предложенная в работе "Attention is All You Need" (Vaswani et al., 2017), которая произвела революцию в обработке естественного языка (NLP). Основное преимущество трансформеров — использование механизма внимания (attention), позволяющего модели эффективно учитывать всю входную последовательность при генерации выходного представления.

**BERT (Bidirectional Encoder Representations from Transformers)** — это архитектура, основанная на трансформерах, предложенная Google, которая использует двунаправленное внимание для обучения контекстных представлений слов. Основное отличие BERT от предыдущих моделей, таких как GPT, заключается в использовании двунаправленного внимания, которое позволяет учитывать как левый, так и правый контексты для каждого слова.

  

# Представление текста (Tokenization)

Перед тем как подать текст в модель, необходимо выполнить его разбиение на токены. В BERT используется **WordPiece Tokenization**, который разбивает слова на субслова или морфемы. Это позволяет лучше справляться с редкими словами и неизвестными терминами.

Например, слово "unhappiness" может быть разбито на токены "un", "#\#happiness", что позволяет модели работать с его составляющими частями и улучшить понимание контекста. Такой подход помогает избежать проблемы Out-Of-Vocabulary (OOV), так как модель учится представлять составные части слова.

  

# Архитектура BERT

**BERT** использует только часть энкодера трансформера для обучения эмбеддингов слов. Основная идея BERT заключается в двунаправленном обучении, что означает использование контекста как слева, так и справа от каждого слова в предложении.

Архитектура BERT состоит из нескольких слоёв энкодера трансформера, каждый из которых включает в себя:

- **Многоголовое внимание (Multi-Head Attention)**.
- **Feed-forward нейронную сеть**.
- **Нормализацию и добавление остаточного соединения (Residual Connection & Layer Normalization)**.

BERT обучается на двух основных задачах: **Masked Language Modeling (MLM)** и **Next Sentence Prediction (NSP)**.

  

# Self-Attention

**Self-Attention** — это ключевой компонент трансформеров, который позволяет каждому слову во входной последовательности взаимодействовать со всеми остальными словами.

**Как работает Self-Attention:**

1. Для каждого слова в последовательности создаются три вектора: **Query (Q)**, **Key (K)** и **Value (V)**. Они вычисляются с использованием обучаемых матриц весов.
2. Для каждого слова вычисляется вес, показывающий, насколько оно важно для каждого другого слова. Это делается путём вычисления скалярного произведения Query и Key:  
    $Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) V$  
    Здесь softmax используется для нормализации весов, а \( d_k \) — размерность ключей, используемая для масштабирования.  
    
3. Полученные веса используются для взвешивания значений (Value), что позволяет модели уделять внимание наиболее важным словам в каждом контексте.

Многоголовое внимание (**Multi-Head Attention**) позволяет трансформеру вычислять несколько разных представлений внимания одновременно, что улучшает его способность захватывать различные аспекты значений и связей между словами.

## Обучение BERT: Masked Language Modeling (MLM)

В **Masked Language Modeling (MLM)** BERT обучается на предсказании случайно замаскированных слов в предложении. Во время обучения модель случайным образом выбирает 15% слов и заменяет их на специальный токен [MASK]. Задача модели — предсказать эти замаскированные слова, используя контекст с обеих сторон.

**Формула вероятности предсказания замаскированного слова**:  
$P(w_i | w_{\setminus i}) = softmax(W h_i)$  
где $h_i$ — выход энкодера для позиции $i$, а $W$ — обучаемая матрица весов. Этот подход позволяет модели лучше понимать семантические связи между словами.

Пример: в предложении "Кошка [MASK] на коврике" модель должна предсказать слово "сидит".

---

## Обучение BERT: Next Sentence Prediction (NSP)

**Next Sentence Prediction (NSP)** — это задача, при которой BERT обучается предсказывать, является ли второе предложение логическим продолжением первого. Модель получает пару предложений и должна определить, связаны ли они. Это помогает обучить модель лучше понимать отношения между предложениями и улучшает её эффективность в задачах, таких как вопросно-ответные системы и диалоговые модели.

Пример: для пары предложений "Кошка сидит на коврике. Она выглядит довольной." модель должна предсказать, что второе предложение логически продолжает первое.

---

  

# CEDR: Contextualized Embeddings for Document Ranking

**CEDR** — это модель, которая улучшает результаты ранжирования документов с помощью использования контекстуализированных эмбеддингов, полученных из BERT. Основная идея CEDR заключается в интеграции предобученных эмбеддингов в архитектуру, которая затем используется для точного ранжирования документов.

CEDR использует архитектуру трансформера для кодирования запроса и документа, а затем применяет различные методы взаимодействия (такие как Attention и Max-Pooling) для оценки их релевантности. Это помогает лучше учитывать сложные зависимости между запросами и документами.

  

# YATI: Yet Another Transformer (with Improvements)

**YATI** — это улучшенная версия трансформеров, разработанная для повышения эффективности и точности обработки текста. Основные улучшения YATI включают:

- **Оптимизацию слоёв внимания** для уменьшения вычислительной сложности.
- **Лучшие механизмы регуляризации**, что помогает избежать переобучения и повысить обобщающую способность модели.
- **Улучшение параметров предобучения**, что позволяет добиться лучшей сходимости при обучении на больших наборах данных.

YATI сохраняет двунаправленный характер обучения, что позволяет модели эффективно использовать контексты с обеих сторон каждого слова.

  

# ColBERT

**ColBERT (Contextualized Late Interaction over BERT)** — это модель, разработанная для улучшения производительности в задачах поиска и ранжирования. Основное отличие ColBERT заключается в использовании механизма **late interaction**, что позволяет модели эффективно комбинировать эмбеддинги запроса и документа на поздних стадиях обработки.

ColBERT сначала кодирует запрос и документ с помощью BERT, а затем применяет взаимодействие на уровне отдельных токенов, что делает процесс ранжирования более эффективным и масштабируемым для больших коллекций документов. Это достигается путём вычисления косинусного сходства между токенами запроса и документа и последующим агрегированием результатов.

  

# Multi-stage Document Ranking with BERT

Для эффективного ранжирования документов используются многоступенчатые подходы, в которых BERT играет ключевую роль. **Multi-stage Document Ranking** включает в себя следующие этапы:

1. **Первичный отбор кандидатов**: Быстрый и грубый отбор большого количества кандидатов с использованием более простых методов, таких как BM25.
2. **Ранжирование кандидатов**: Кандидаты ранжируются с помощью BERT или его производных. На этом этапе используется модель, которая может более точно оценить релевантность документов, учитывая контекст.
3. **Финальное уточнение**: В некоторых системах добавляется ещё один этап, на котором BERT дообучается на более узких задачах, что позволяет ещё больше улучшить результаты.

Использование BERT на втором и третьем этапах позволяет учитывать глубокие семантические зависимости и улучшить качество ранжирования по сравнению с традиционными методами.