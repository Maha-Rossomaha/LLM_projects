---
Owner: IIvan Vashchenko
---
> [!important] 
> Эмбеддинги слов стали ключевым компонентом современных NLP-систем. Они позволяют моделям глубже понимать тексты, учитывать семантические связи и взаимодействия между словами, что критически важно для задач поиска, ранжирования, классификации и генерации текста.
> 
> Рассмотренные подходы — от **Word2Vec** и **FastText** до **DSSM** и **C-DSSM** — демонстрируют эволюцию методов представления текста в виде векторов. Каждый метод имеет свои преимущества и недостатки. Например, **Word2Vec** и **FastText** отлично подходят для генерации эмбеддингов слов, тогда как **DSSM** больше ориентирована на задачи информационного поиска.
> 
> Перспективным направлением дальнейших исследований является комбинирование различных методов для достижения лучших результатов в конкретных задачах. К примеру, использование свёрток (**C-DSSM**) и графовых моделей взаимодействия (**MatchPyramid**) может дать преимущество в задачах ранжирования и рекомендаций. Современные методы, такие как **BERT** и его производные, также развивают идеи эмбеддингов, позволяя учитывать контекст на уровне предложений и целых текстов, что открывает новые горизонты для NLP-систем.

# Введение в текстовые эмбеддинги

Эмбеддинги слов — это векторные представления, описывающие слова в многомерном пространстве. Ключевая идея состоит в том, что слова со схожими значениями или контекстами имеют близкие векторные представления. Это позволяет моделям машинного обучения лучше понимать тексты.

Текстовые эмбеддинги применяются в широком спектре задач: от классификации текстов до рекомендательных систем. Они помогают моделям улавливать семантические связи между словами, что открывает возможности для решения более сложных задач обработки естественного языка.

Давайте рассмотрим несколько методов создания эмбеддингов, включая **Word2Vec**, **FastText** и **Deep Structured Semantic Models (DSSM)**.

  

# Word2Vec

**Word2Vec** — это алгоритм языкового моделирования, обучающий векторные представления слов. Обучение происходит на текстах с использованием скользящего окна для формирования контекста слов.

## Метод CBoW (Continuous Bag of Words)

**CBoW** предсказывает текущее слово на основе окружающих слов (контекста). Например, в предложении "Какое-нибудь достаточно длинное предложение" модель попытается предсказать слово "достаточно" на основе слов "Какое-нибудь", "длинное" и "предложение".

**Формула для CBoW**:  
$P(w_t | w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2})$

где $w_t$ — текущее слово, а $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$ — слова контекста.

---

## Метод Skip-gram

**Skip-gram** решает обратную задачу: предсказывает контекст на основе текущего слова. Например, для слова "предложение" Skip-gram пытается определить окружающие его слова.

**Skip-gram** формирует пары для предсказания соседних слов по одному, что позволяет обучить модель прогнозировать не только слова, но и вероятности их появления в контексте.

**Пример распределения для Skip-gram**:  
$P(w_{context} | w_{central})$

где $w_{context}$ — слово из контекста, а $w_{central}$ — центральное слово.

---

## Негативное семплирование

Для повышения эффективности обучения применяется метод **негативного семплирования**. Суть метода в добавлении в выборку негативных примеров (слов, не встречающихся в контексте текущего слова). Это учит модель различать реальные контексты от случайных, делая её более устойчивой и точной.

**Формула для негативного семплирования**:

$L = \sum_{w_{context} \in C} \log(\sigma(v_{w_{central}} \cdot v_{w_{context}})) + \sum_{w_{neg} \in N} \log(1 - \sigma(v_{w_{central}} \cdot v_{w_{neg}}))$

где $v_{w_{central}}$ и $v_{w_{context}}$ — эмбеддинги центрального и контекстного слова соответственно, а $w_{neg}$ — случайно выбранное негативное слово.

---

  

# FastText

**FastText** — усовершенствованная версия Word2Vec, разработанная Facebook, учитывающая морфологию слов. Вместо представления слова целиком, FastText разбивает его на **N-граммы** (последовательности символов фиксированной длины). Это позволяет учитывать внутрисловные связи и улучшать качество эмбеддингов, особенно для редких и составных слов.

## Разбиение на N-граммы

Пример разбиения слова "астрофизика" на триграммы:

- Добавляем символы начала и конца слова: `#астрофизика#`.
- Разбиваем на триграммы: `#ас`, `аст`, `стр`, `тро`, `роф` и т.д.

Эмбеддинг слова в FastText — это сумма эмбеддинга самого слова (если оно есть в словаре) и среднего значения всех эмбеддингов N-грамм. Это позволяет модели лучше работать со словами, отсутствующими в словаре (решая проблему **Out Of Vocabulary**).

**Формула эмбеддинга для слова в FastText**:

$v_{word} = v_{self} + \frac{1}{N} \sum_{n=1}^{N} v_{n-gram}$

где $v_{self}$ — эмбеддинг самого слова, а $v_{n-gram}$ — эмбеддинги N-грамм.

---

# DSSM и C-DSSM

**Deep Structured Semantic Models (DSSM)** — это модель, разработанная в Microsoft, которая использует эмбеддинги для задачи информационного поиска. Она позволяет создать семантическое представление текстов (например, запросов и документов) и вычислять их сходство.

## DSSM

**DSSM** обучается на кликовых данных — кликали ли пользователи на документ, предложенный в ответ на запрос. Основная задача **DSSM** — представить запросы и документы в виде эмбеддингов так, чтобы семантически схожие тексты имели близкие представления. Таким образом, можно сравнивать тексты по их векторным представлениям, что позволяет определить, насколько релевантен документ для конкретного запроса.

**Как работает DSSM:**

1. **Преобразование текста в символные N-граммы**: Сначала текст запроса и документа разбивается на символные N-граммы (обычно N = 3), что позволяет захватывать различные комбинации букв и учитывать даже малые различия.
2. **Кодирование N-грамм**: Каждой N-грамме сопоставляется одно горячее кодирование (one-hot encoding), а затем эти представления преобразуются в более компактные эмбеддинги с помощью полносвязного слоя.
3. **Полносвязные слои**: После кодирования N-граммы пропускаются через несколько полносвязных слоёв с нелинейностями (например, ReLU), что позволяет создать более глубокое и значимое представление текста.
4. **Выходной эмбеддинг**: Итоговые эмбеддинги запроса и документа представляют собой векторы, которые используются для оценки их схожести.

Для измерения схожести между запросом и документом используется **косинусное сходство**:

$S(query, document) = \cos(v_{query}, v_{document}) = \frac{v_{query} \cdot v_{document}}{||v_{query}|| \, ||v_{document}||}$

Затем на основе косинусного сходства предсказывается вероятность того, что пользователь кликнет на данный документ в ответ на запрос:

$P(click | query, document) = \sigma(\cos(v_{query}, v_{document}))$

где $\sigma$ — сигмоидальная функция, используемая для нормализации значений.

---

## C-DSSM (Convolutional DSSM)

Для решения проблемы утраты информации о порядке слов в DSSM была предложена свёрточная модификация — **C-DSSM** (Convolutional DSSM). В этой модели текст сначала разбивается на N-граммы, а затем применяется свёртка по скользящему окну. Это позволяет учитывать более сложные паттерны взаимодействия слов и сохранять информацию о локальных связях в тексте.

**Как работает C-DSSM:**

1. **Преобразование текста**: Как и в DSSM, текст запроса и документа разбивается на N-граммы и кодируется.
2. **Свёрточные слои**: Применяются свёрточные фильтры, которые проходят по последовательности N-грамм и извлекают локальные паттерны. Это помогает выявлять важные комбинации символов и слов, влияющие на семантику текста.
3. **Max-Pooling**: После применения свёрток используется операция **max-pooling**, выбирающая наиболее значимые признаки из каждого окна. Это позволяет сократить размерность представления и сохранить только самые информативные признаки.
4. **Полносвязные слои**: После pooling-слоя результат подаётся на полносвязный слой для получения финального эмбеддинга текста. Эти эмбеддинги затем используются для вычисления косинусного сходства между запросом и документом.

**Преимущества C-DSSM:**

- **Учёт порядка слов**: В отличие от обычного DSSM, свёрточные слои позволяют учитывать порядок слов и их взаимодействие, что улучшает качество представлений для задач, где порядок слов имеет значение.
- **Локальные паттерны**: Свёртки помогают выделять локальные паттерны, что особенно полезно для анализа сложных текстов, в которых важно учитывать не только наличие слов, но и их контекстуальные комбинации.

---

  

# Другие подходы: MatchPyramid и Conv-KNRM

**MatchPyramid** — архитектура, использующая свёртки и пулинг для анализа матрицы взаимодействий слов из двух текстов (например, запроса и документа). Эта модель создаёт матрицу взаимодействий, где каждый элемент соответствует косинусному сходству между эмбеддингами слов из двух текстов.

Свёрточные слои выделяют сложные паттерны взаимодействия между словами, а пулинг помогает уменьшить размерность матрицы, сохраняя только наиболее важные признаки.

**Conv-KNRM** — модификация известного метода KNRM, где вместо отдельных слов используется скользящее окно, а взаимодействие между ними осуществляется с помощью свёрток. Это позволяет учитывать не только парные взаимодействия, но и более сложные структуры текста.