---
Owner: IIvan Vashchenko
---
> [!important] 
> Приближённый поиск ближайших соседей — важная часть систем ранжирования и матчинга, выполняющая роль кандидатной модели (выделение подмножества объектов для дальнейшей обработки).
> 
> Основные идеи связаны с упрощением задачи методом разделения пространства на изолированные кластера. Для повышения точности можно обходить несколько кластеров либо брать несколько начальных инициализаций.
> 
> Если данных много, то нужно применять квантизацию векторов — это слегка снизит качество, но улучшит время работы и вес индекса.
> 
> _$\text{HNSW}+\text{PQ}+\text{IVF}+\text{метрика}=\text{результат}$_
> 
> Библиотеки, с которых стоит начать: [NMSLIB](https://github.com/nmslib/nmslib), [FAISS](https://github.com/facebookresearch/faiss), [ANNOY](https://github.com/spotify/annoy) [(см. похожий датасет в бенчмарках, учитывая свой сценарий использования)](https://github.com/erikbern/ann-benchmarks).
> 
>   
> 
> - **K-D Tree** эффективен для задач с низкой размерностью, но его производительность снижается в пространствах высокой размерности.
> - **ANNOY** и **LSH** идеально подходят для задач, где критично время ответа и допустима незначительная потеря точности.
> - **HNSW** обеспечивает одну из лучших производительностей для поиска в больших объёмах данных, сочетая графовые методы и иерархические уровни.
> 
> Эти методы можно комбинировать для достижения оптимальных результатов. Например, **LSH** можно использовать для предварительного хеширования данных, а затем применить более точный метод, такой как **HNSW**, для поиска ближайших соседей среди отобранных кандидатов. Такой гибридный подход позволяет сбалансировать **скорость поиска** и **точность** — что крайне важно для масштабируемых систем, работающих с огромными объёмами данных, таких как рекомендательные системы крупных интернет-магазинов или поисковые системы.

# Примечание к лекции

> “В лекции мы обещали приложить к конспекту урока памятку, но за прошедшие два года индустрия изменилась, а памятка утратила актуальность. FAISS стал стандартом, в него добавили много нового функционала. Единственное, что предлагаем сделать — почитайте про старые и новые опции в документации, чтобы лучше понимать, какие из них применимы для ваших задач и размеров датасета.

> А еще за это время появился [https://github.com/qdrant/qdrant](https://github.com/qdrant/qdrant), который мы рекомендуем изучить самостоятельно. Он весьма удобен — например, поддерживает префильтрацию документов во время поиска по некоторому набору правил. Эта функция эффективна для работы с бизнес-логикой, поскольку не придется перебирать все объекты при поиске. Рекомендуем самостоятельно изучить.”

---

# Введение в задачу поиска ближайших соседей

Задача **поиска ближайших соседей** состоит в нахождении наиболее схожих объектов для заданного запроса в векторном пространстве. Каждый объект представлен вектором признаков, и цель — найти k ближайших объектов относительно вектора запроса. Этот подход широко применяется в рекомендательных системах, обработке изображений, задачах кластеризации и многих других областях.

Поиск ближайших соседей в крупной базе данных с высокой размерностью может быть чрезвычайно ресурсоёмким. Методы полного перебора имеют сложность $O(N)$, где $N$ — число объектов. С ростом размерности пространства эффективность прямого поиска резко снижается из-за феномена, известного как **проклятие размерности**. Для снижения вычислительных затрат применяются различные методы **приближённого поиска ближайших соседей (ANN)**, позволяющие уменьшить время выполнения за счёт незначительной потери точности.

---

## Применение в системах ранжирования и рекомендации

**Поиск ближайших соседей** является основой многих рекомендательных систем и систем ранжирования. Например, в рекомендательных системах пользователи и товары представляются в виде векторов (эмбеддингов), и для каждого пользователя рекомендуется список товаров на основе поиска ближайших соседей в этом многомерном пространстве.

В системах ранжирования можно использовать подходы ANN для предварительного отбора кандидатов, которые затем более тщательно ранжируются с помощью точных методов.

**Рассмотрим пример использования в рекомендательной системе**:

1. **Формирование векторов предпочтений**: каждый пользователь и каждый товар представляются вектором признаков.
2. **Поиск похожих пользователей**: для пользователя-запроса выполняется поиск ближайших соседей в пространстве пользователей.
3. **Рекомендации**: товары, которые интересны ближайшим соседям, предлагаются в качестве рекомендаций пользователю-запросу.

Этот подход позволяет учитывать интересы пользователей и находить рекомендации, которые максимально соответствуют их предпочтениям.

---

# Методы приближённого поиска ближайших соседей

## **Сравнение и оценка методов ANN**

Бенчмарк большинства методов ANN: [github](https://github.com/erikbern/ann-benchmarks).

---

## Product Quantization (PQ)

**Product Quantization (PQ)** — метод, который разделяет векторы на подвекторы и использует кластеризацию для уменьшения их размерности. Основная идея заключается в разбиении каждого вектора на несколько подвекторов и их независимой квантизации, что позволяет уменьшить количество данных, необходимых для хранения и поиска.

В **Product Quantization** мы действительно проходимся по всей базе данных, но используем **квантованные представления** векторов, что делает процесс гораздо быстрее и менее затратным по памяти, чем работа с исходными векторами.

### **Разбиение на подвекторы**:

- Каждый вектор делится на несколько подвекторов одинаковой длины.
- Например, если исходный вектор имеет размерность 128, его можно разделить на 8 подвекторов размером по 16.
- Разделённые подвектора по всем объектам можно кластеризовать с помощью `k-means`, но использовать не произвольное количество кластеров, а константное — `256`.

### **Кластеризация методом k-means**:

- Для каждого набора подвекторов проводится кластеризация с использованием метода k-means. $k=256$ - связано это с тем, что в один байт можно записать целочисленное значение от 0 до 255, то есть 256 уникальных значений. И тогда такой подвектор превращается просто в id соответствующего кластера. Так как групп векторов имеется несколько, то схожая операция проделывается и для них.
- Каждый подвектор заменяется на индекс ближайшего кластера, что позволяет значительно уменьшить объём данных.

### **Что в итоге хранится?**

- **Кодбуки**: Все центроиды $C^1, C^2, \dots, C^M$
- **Квантованные данные**: Для каждого вектора x хранятся только индексы ближайших центроидов  
    $[q(x^1), q(x^2), \dots, q(x^M)]$

### **Поиск (Online)**

1. **Квантование запроса**
    
    Запросный вектор $q$ также разбивается на подвекторы:  
    $q = [q^1, q^2, \dots, q^M]$
    
2. **Вычисление приближённого расстояния**
    
    Для каждого подвектора $q^i$ и индекса $q(x^i$) хранятся предварительно рассчитанные расстояния между подвектором запроса и всеми центроидами кодбука $C^i$. Расстояние между полными векторами вычисляется как сумма расстояний:  
    $\large\|x - q\|^2 \approx \sum_{i=1}^M \|c^i_{q(x^i)} - q^i\|^2$
    
3. **Сортировка и выдача ближайших соседей**
    
    Выбираются k-ближайших векторов на основе приближённых расстояний.
    

### **Преимущества**:

- При использовании методов типа HNSW или Annoy: PQ помогает ускорить вычисления расстояний.
- Существенная экономия памяти.
- Быстрая операция поиска за счёт использования предвычисленных кластерных центров.

### **Недостатки**:

- Потеря точности из-за квантования.
- Подходит не для всех типов данных.
- Неэффективно при очень большом количестве наблюдений

---

## **K-D Tree**

_**На практике при наличии более продвинутых подходов полный вариант KD-Tree используется редко.**_

**K-D Tree** (k-dimensional tree) — структура данных для хранения точек в k-мерном пространстве, обеспечивающая эффективный поиск ближайших соседей. Принцип работы основан на рекурсивном разбиении пространства гиперплоскостями, что позволяет быстро отсекать обширные области и сокращать количество проверок.

### **Построение дерева**:

- На каждой итерации выбирается координата для разбиения, обычно с наибольшей дисперсией.
- Точки сортируются по выбранной координате, и медиана используется для разделения множества на две части.
- Левое поддерево включает точки со значениями меньше или равными медиане, правое — с большими значениями.
- Процесс рекурсивно повторяется для каждой части, пока в узле не останется одна точка или их небольшое количество.

### **Поиск ближайших соседей**:

- Поиск начинается с корневого узла. На каждом шаге алгоритм определяет, в какую сторону от гиперплоскости узла лежит точка запроса — в левую или правую ветвь.
- Достигнув листового узла, алгоритм начинает обратный подъём по дереву, проверяя другие ветви на наличие потенциально более близких соседей.
- Для оптимизации поиска применяется метод отсечения, позволяющий исключить из рассмотрения области, которые заведомо не содержат точек ближе текущего кандидата.

### **Преимущества**:

- Эффективен для низких размерностей (до 20–30).

### **Недостатки**:

- При увеличении размерности эффективность резко падает.
- В худшем случае обход может занять время, близкое к линейному.

---

## ANNOY (Approximate Nearest Neighbors Oh Yeah)

**ANNOY**, разработанный в Spotify, — это метод приближённого поиска ближайших соседей. Основная идея заключается в использовании множества случайно построенных деревьев, что позволяет создать множество разбиений пространства и таким образом увеличить вероятность нахождения точного соседа.

В целом метод похож на **KD-Tree**, кроме выбора разбивающих плоскостей. Для каждого разбиения случайным образом выбираются две точки, они соединяются отрезком, а затем через его середину строится перпендикулярная гиперплоскость.

### **Построение деревьев**:

- Создаётся несколько деревьев (например, 10 или 20), каждое из которых строится на основе случайного выбора осей и разбиений.
- На каждом шаге выбираются две случайные точки, через которые проводится случайная гиперплоскость, разделяющая множество точек на две части.
- Дерево строится до тех пор, пока в каждом листе не останется фиксированное количество точек.

### **Поиск ближайших соседей**:

- Для каждого запроса выполняется поиск в каждом из построенных деревьев.
- Все найденные кандидаты из разных деревьев собираются вместе и ранжируются в зависимости от расстояния до запроса.
- Возвращаются k ближайших соседей.

### **Преимущества**:

Кажется, что проблемы, озвученные для **KD-Tree** на этом этапе решены хотя бы частично. Появилась возможность контролировать количество финальных объектов в листах, однако не всегда такое нововведение способно улучшить алгоритм. И всё же мы построили дерево поиска, которое позволяет не перебирать все объекты для расчета дистанций.

- Хорошая балансировка между скоростью и точностью.
- Множественное построение деревьев повышает устойчивость к ошибкам.
- Очередь с приоритетом (priority queue);
- Возможность при близости к критерию разбиения заходить в оба поддерева;
- Построение леса деревьев (с разными разбиениями);
- Очередь с приоритетом доступна всем деревьям (не работаем с «плохими» деревьями, в которых неудачное разбиение на подпространства).

### **Недостатки**:

- Требует большого объёма памяти для хранения деревьев.
- Сложно контролировать точность в сравнении с другими методами.
- Статическая база данных: эффективен для поиска, но не поддерживает добавление данных после построения деревьев.

---

## **IVF (Inverted File Index)**

**IVF (Inverted File Index)** — это популярный метод приближённого поиска ближайших соседей (**ANN**, Approximate Nearest Neighbors), который основывается на идее кластеризации данных для предварительной фильтрации кандидатов. IVF особенно полезен для больших баз данных, так как помогает уменьшить количество вычислений за счёт разделения пространства на области.

IVF организует векторы в кластеры и хранит информацию о принадлежности каждого вектора в виде инвертированных списков (**inverted lists**). На этапе поиска рассматриваются только несколько ближайших кластеров, что значительно уменьшает количество вычислений.

### **Построение индекса (Offline)**

1. **Кластеризация базы данных**:
    - Используется метод кластеризации (обычно $k$-means), чтобы разбить все векторы на $k$ кластеров.
    - Центроиды этих кластеров будут использоваться как "репрезентативные точки" для выбора областей.
2. **Создание инвертированных списков**:
    - Для каждого кластера хранится список векторов, которые ему принадлежат.
    - Вектор относится к кластеру, центр которого находится ближе всего:  
        $\text{assign}(x) = \text{argmin}_{i} \|x - c_i\|^2$  
        Где $c_i$ — центроид $i$-го кластера.
3. **Хранение индекса**:
    - Сохраняются:
        - Координаты центроидов.
        - Инвертированные списки (список идентификаторов векторов для каждого кластера).

### Поиск (Online)

1. **Определение ближайших кластеров для запроса**:
    - Запросный вектор $q$ сравнивается с центроидами кластеров.
    - Выбираются $n$-ближайших кластеров к запросу $q$ (обычно $n≪k$).
2. **Фильтрация кандидатов**:
    - Рассматриваются только векторы, принадлежащие выбранным $n$-кластеров.
3. **Точный или приближённый расчёт расстояний**:
    - Для кандидатов из выбранных кластеров вычисляются расстояния до $q$.
    - Сортировка кандидатов по расстоянию, чтобы выбрать $k$-лучших.

### **Интеграция IVF с Product Quantization (IVF-PQ)**

Для ещё большей оптимизации IVF часто комбинируется с **Product Quantization (PQ)**. Это называется **IVF-PQ**.

1. **Квантование векторов внутри кластеров**:
    - После того как векторы распределены по кластерам, их подвекторы кодируются с помощью PQ, заменяя точные координаты на индексы центроидов.
2. **Поиск с приближением**:
    - Для кандидатов из выбранных кластеров расстояния рассчитываются с использованием квантованных представлений (lookup tables).
    - Это снижает объём памяти и ускоряет вычисления.

### Преимущества:

- **Масштабируемость**: Подходит для больших баз данных
- **Гибкость**: Работает с высокой размерностью ($D>100$).
- **Скорость**: Отсеивает большинство векторов на этапе кластеризации.

### Недостатки:

- **Падение качества при малом числе кластеров**: Если кластеров мало ($k$), в одном кластере может оказаться слишком много векторов.
- **Необходимость оптимальной настройки** $k$ **и** $n$: Нужно подбирать оптимальные параметры для баланса между скоростью и точностью.

---

## LSH (Locality-Sensitive Hashing)

**Locality-Sensitive Hashing (LSH)** — это метод, основанный на хешировании, который группирует похожие объекты в один и тот же "бин" (bucket). Цель LSH — с высокой вероятностью поместить в один хеш-бин объекты, находящиеся в близком соседстве в пространстве векторов.

### **Хеширование с гиперплоскостями**:

- Используются случайные гиперплоскости, чтобы определить, с какой стороны лежит точка. Если точка находится с одной стороны гиперплоскости, она попадает в один хеш-бин, если с другой — в другой.
- Таким образом, близкие друг к другу объекты с высокой вероятностью попадают в один и тот же хеш-бин.

### **Поиск по хеш-таблице**:

- Для точки запроса находятся все точки, попавшие в тот же хеш-бин.
- Эти кандидаты затем проверяются на предмет расстояния, и выбираются ближайшие соседи.
    
    **Формула хеш-функции**:  
    $h(x) = \begin{cases} 0, & \text{если } w^T x + b < 0 \\ 1, & \text{если } w^T x + b \geq 0 \end{cases}$  
    Где:  
    
    $w$ — случайный вектор, определяющий гиперплоскость.
    
    $b$ — случайное смещение.


### **Преимущества**:

- Простота реализации и высокая скорость поиска.
- Хорошая производительность при использовании нескольких хеш-функций.

### **Недостатки**:

- Результаты могут быть неточными при высоких размерностях.
- Требуется большой объём памяти при увеличении количества хеш-функций.

---

## FastScan

FastScan — это метод «быстрого линейного перебора» (brute force), в котором для вычисления расстояний или сходства между векторами используются аппаратные оптимизации (SIMD, распараллеливание, кэш-оптимизация), а также сжатые форматы данных (например, int8 или float16). Это даёт значительный выигрыш в производительности по сравнению с наивным сканированием, хотя по времени всё ещё остаётся $O(n)$ операция при большом количестве векторов.

1. **Линейный перебор**
    
    FastScan — это метод **точного** или псевдо-точного (если применяется квантизация) поиска, который, по сути, перебирает все векторы в базе, вычисляя расстояние (L2, косинусное сходство, скалярное произведение) между ними и запросным вектором. В классическом brute force перебор может быть крайне медленным при больших объёмах данных.
    
2. **Ключевая оптимизация**
    
    FastScan «разгоняет» этот перебор при помощи:
    
    - **SIMD** (Single Instruction Multiple Data) — AVX2/AVX-512/SSE-инструкции, позволяющие проводить несколько операций умножения/сложения за один такт.
    - **Кэш- и память-оптимизации** — упорядочивание данных так, чтобы отрабатывались большие блоки последовательно (cache-friendly layout), выравнивание по границе 16 или 32 байт, минимизация пропускной способности памяти.
    - **Cжатых форматов** — использование int8, float16 или других форматов. Чем меньше размер на элемент, тем больше векторов помещается в кэш.
3. **Алгоритмическая база**
    
    Приёмлемая асимптотика у FastScan не меняется: это всё равно $O(n)$ по числу векторов. Но за счёт «железных» оптимизаций постоянные множители в этом $O(n)$ уменьшаются, и линейный перебор оказывается быстрее, чем кажется на первый взгляд.
    

### Как работает FastScan «под капотом»

1. **Загрузка запроса в SIMD-регистры**
    - Запросный вектор (например, размерности $d=128$) раскладывается по SIMD-регистрам.
    - Пример: если у вас AVX2 (256-битные регистры), туда помещаются 8 элементов float32.
2. **«Пакетная» обработка базы**
    - База векторов делится на блоки, размер которых соответствует размеру SIMD-регистров.
    - Для каждой порции координат выполняется умножение на соответствующие координаты запроса и аккумуляция сумм (для скалярного произведения) либо вычисляются квадраты разностей (для L2).
3. **Кэш-предзагрузка (prefetching)**
    - Современные процессоры и алгоритмы FastScan часто используют предзагрузку данных в кэш (prefetch instructions), чтобы подготавливать следующие блоки векторов ещё до того, как будут нужны результаты предыдущего блока.
4. **Распараллеливание**
    - Часто алгоритм дополнительно параллелят на уровне потоков (thread-level parallelism), деля базу на разные куски, обрабатываемые параллельно на разных ядрах CPU.
5. **Поиск топ-K**
    - Во время сканирования можно поддерживать структуру данных (например, мин-кучу) для хранения топ-K ближайших результатов, динамически обновляя её при каждой итерации.

### Преимущества FastScan

1. **Простота реализации**
    - Нет необходимости строить сложные индексы (HNSW, графы, деревья и т.д.).
    - Подходит для случаев, когда данные не слишком велики или когда необходимо работать с «сырой» базой.
2. **Высокая скорость на «умеренных» наборах**
    - Для баз в несколько сотен тысяч или даже миллионов векторов, если объём памяти позволяет, FastScan может быть почти таким же быстрым, как простые приближённые структуры, но без потери точности.
3. **Гибкость**
    - Можно легко подменить метрику (L2, косинус, dot product), не требуется перестраивать сложные структуры.
    - Можно скомбинировать со способом фильтрации: если заранее отфильтрованное подмножество относительно невелико, то линейный перебор с FastScan эффективен.
4. **Нет «ложных» результатов**
    - FastScan даёт точный результат (если не применять квантизацию). При использовании квантизации результат может быть близок к точному (в зависимости от уровня сжатия).

### Недостатки FastScan

1. **Линейная сложность**
    - При очень больших коллекциях (сотни миллионов и более) даже самый быстрый линейный перебор может оказаться непрактичным, если нужно много запросов в секунду.
2. **Требования к памяти**
    - Всё равно приходится хранить полный набор векторов, причём если они в float32, это довольно объёмно (128-мерный вектор в float32 — это 512 байт на один вектор).
    - Квантизация помогает (int8: 128 байт на вектор), но при этом могут возникать ошибки округления.
3. **Ограничения железа**
    - Для максимальной выгоды от SIMD вам нужны современные процессоры с расширенными инструкциями (AVX-512, AVX2). На старых CPU, ARM (за исключением новых, поддерживающих NEON), или в средах без оптимизированных инструкций прироста будет меньше.

### Когда стоит использовать FastScan

1. **Небольшие или средние объёмы данных**
    - Если векторов не слишком много (до нескольких миллионов), FastScan даёт быстроту разработки и высокую точность.
2. **Гибридные решения**
    - Можно сначала отбирать кандидатов приблизительным индексом (например, HNSW или IVF), а затем на небольшом подмножестве (тысяча-десятки тысяч) применять FastScan для «доточенного» ранжирования.
3. **Сценарии с регулярными обновлениями**
    - Если данные часто обновляются, перестраивать сложные структуры накладно. FastScan избавляет от такой проблемы, ведь ничего перестраивать не нужно: вектор просто добавляется в общий массив.
4. **Точность критична**
    - Когда невозможно допустить ошибки, а Approximate Nearest Neighbor (ANN) может упустить часть релевантных результатов, FastScan позволяет убедиться в 100% точности (если не используется квантизация).

- Здесь мы показываем идею пакетной обработки (разделение массива на части) и параллелизации.
- Реальная «fastscan»-оптимизация в C++ может задействовать AVX2/AVX-512 для умножений внутри каждого потока, предзагрузку данных в кэш и т.д.

### Итоги

- **FastScan** — это не формальный «уникальный алгоритм», а название подхода или «метки» для высокопроизводительного линейного перебора.
- Использует аппаратные инструкции, сжатие и умные оптимизации памяти, позволяющие существенно ускорять перебор, оставаясь при этом $O(n)$.
- Удобен при средних объёмах данных или в случаях, когда мы хотим 100% точность без сложных индексов.
- Для очень больших коллекций и высоких нагрузок чаще применяют приближённые структуры (IVF, HNSW, PQ), но даже там FastScan может использоваться на заключительных этапах ранжирования.

---

  

# Графовые методы поиска ближайших соседей

## NSW (Navigable Small World)

**Navigable Small World (NSW)** — это графовая структура, в которой каждый объект соединён с рядом ближайших соседей. NSW представляет собой навигационный граф, в котором короткие и длинные связи позволяют эффективно перемещаться по пространству.

В теории графов "Small World" определяется как сеть, в которой типичное расстояние _`L`_ между двумя произвольно выбранными вершинами растёт пропорционально логарифму от числа вершин _`N`_ в сети: $L \sim \log(N)$.

### **Построение графа**:

- Для каждой точки создаются рёбра с её ближайшими соседями, что формирует высокосвязную структуру.
- Рёбра представляют как "короткие", так и "длинные" связи, что обеспечивает лёгкое перемещение по графу.
- Граф ненаправленный, т.е. переходы взаимно обратны.

На этапе построения графа (так как это предварительная работа, которая выполняется разово, а не во время каждого запроса) можно высчитать много расстояний между точками. Скажем, для каждой точки случайно выбираются 5% точек из всей выборки, считается расстояние до них, и из 10 ближайших точек выбираются 5 точек, до которых прокладываются рёбра. Здесь 5 и 10 — это гиперпараметры алгоритма. Таким образом формируются короткие связи. Из этих же 5% точек можно выбрать аналогичным образом ещё 5 точек, но уже максимально удалённых от исходной, и записать связи между ними в виде рёбер.

### **Поиск в графе**:

![[image 19.png|image 19.png]]

- Начальная точка выбирается случайно.
- На каждой итерации рассчитываются расстояния до соседей, и выбирается точка с минимальным значением расстояния.
- Процесс продолжается до тех пор, пока не найдена точка, которая минимизирует расстояние до запроса.

### **Преимущества**:

- Высокая эффективность поиска в графе.
- Простота добавления новых точек.

### **Недостатки**:

- Построение графа может быть медленным для больших объёмов данных.

---

## HNSW (Hierarchical NSW)
[Описание на medium](https://phann123.medium.com/what-is-hnsw-hierarchical-navigable-small-world-49935be7fc05) 

![[image 1 9.png|image 1 9.png]]

**Hierarchical NSW (HNSW)** — это усовершенствование NSW, в котором граф организован иерархически, с уровнями, представляющими различные масштабы "приближения" к целевой точке. В названии метода указано _hierarchical_, или иерархический. В этом и заключается вся суть. Авторы подхода заметили, что выгоднее сначала перебирать только длинные связи для уточнения места в пространстве, где лежит искомая точка, а затем углубляться и перебирать всё более и более локальные связи.

### **Иерархическая структура**:

- На верхнем уровне граф состоит из "длинных" связей, что позволяет быстро сузить область поиска.
- На нижних уровнях добавляются "короткие" связи, что помогает найти ближайших соседей с большей точностью.

### **Итеративный поиск**:

- Поиск начинается на верхнем уровне, и по мере приближения к цели происходит спуск на уровень ниже, где поиск уточняется с использованием дополнительных связей.

### **Преимущества**:

- Высокая точность и эффективность поиска.
- Поддержка динамического добавления новых объектов.
- Большие базы данных: Отличается от NSW более высокой скоростью поиска при обработке миллиардов объектов.

### **Недостатки**:

- Высокая сложность реализации.
- Необходимость тщательной калибровки количества уровней и числа соседей.

---

  

# Комбинация методов FAISS

**FAISS** — библиотека от Facebook. Поддерживает работу с GPU (для тех алгоритмов, где она возможна) и множество оптимизаций (даже для простого полного поиска на CPU). Индекс частично может быть сохранён на диске. Есть возможность быстро создавать разные индексы для поиска, комбинируя механики.

В одну строку можно создать индекс для работы с большим количеством данных:

[![](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image.png)](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image.png)

В этом примере подразумевается:

- Работа с векторами размерности 128.
- Эта величина снижается до 64 методом главных компонент — на это указывает PCA64 в начале строки, задающей индекс.
- Затем происходит кластеризация на чуть более чем 16 тыс. кластеров методом K-Means, и объекты, попавшие в каждый кластер, записываются в IVF индекс, простой файл, как мы обсуждали ранее.
- Поиск ближайших кластеров для рассмотрения будет осуществляться с помощью иерархической навигации по графу, где точки представляют собой центроиды кластеров. То есть происходит поиск не ближайших точек, а максимально релевантных кластеров для поиска кандидатов — всего будет выбрано 32 центроида, задающих 32 кластера из 16384. При этом поиск в рамках этих кластеров будет максимально честным, с полным переборов всех кандидатов из файлов обратного индекса, на что указывает алиас Flat.
    
    иерархической навигации по графу
    

[![](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image_m4FlAbI.png)](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image_m4FlAbI.png)

Во втором примере более простой подход:

- Сначала создаётся целых 260 тыс. кластеров.
- Затем исходные вектора квантизуются в 64 бита. С одной стороны, это облегчает вычисления, ускоряет их, а также уменьшает расход памяти — как оперативной, так и на диске. С другой стороны, получается некоторое падение точности из-за сжатия векторов, однако по количеству кластеров (260 тыс.) можно предположить, что стартовое количество документов огромно, и по нему просто невозможно максимально точно за разумное время производить поиск. Поэтому приходится идти на уступки с Product Quantization.

Гайдлайн FAISS по подбору параметров для задачи поиска предлагает использовать степени двойки в качестве количества кластеров.

[![](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image_3LKOyUh.png)](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/163/543/3663/image_3LKOyUh.png)

Примеры рекомендуемых комбинаций поиска для датасетов разного размера.

Приступая к решению прикладной задачи, полезно сначала оценить размеры базы документов, её особенности и пройтись по списку для определения того, какими методами стоит воспользоваться. Также следует попробовать несколько разных дистанций для расчёта близости векторов. Это может быть косинусное расстояние, L1, L2 или что-то более сложное.

---