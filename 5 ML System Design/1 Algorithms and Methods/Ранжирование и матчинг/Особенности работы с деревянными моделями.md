---
Owner: IIvan Vashchenko
---
> [!important] 
> - **RankNet** минимизирует количество попарных ошибок, но не всегда оптимизирует итоговую метрику ранжирования.
> - **LambdaRank** решает эту проблему, используя градиенты, зависящие от влияния на метрику, что улучшает качество ранжирования.
> - **MART** — это метод бустинга деревьев, корректирующий ошибки предыдущих моделей с помощью градиентов.
> - **LambdaMART** объединяет LambdaRank и MART, используя бустинг деревьев для оптимизации метрик ранжирования.
> - **YetiRank** идёт ещё дальше, добавляя веса и шум для улучшения обучения и достижения наилучших результатов в задачах ранжирования.
> 
> Эти методы демонстрируют, как можно улучшить качество ранжирования, учитывая важность каждого документа и оптимизируя итоговые метрики, а не просто минимизируя ошибки.

# RankNet и его проблемы

**RankNet** — это попарный (pairwise) подход к ранжированию, в котором система сравнивает документы попарно и минимизирует количество неправильно упорядоченных пар. Однако у этого метода есть ограничения: минимизация попарных ошибок не всегда улучшает метрики ранжирования, такие как Average Precision (AP) или Discounted Cumulative Gain (DCG). В результате некоторые метрики, например, **Winner Takes All (WTA)** (бинарная метрика, оценивающая релевантность самого первого документа), могут даже ухудшаться.

![[image 20.png|image 20.png]]

Основная проблема RankNet в том, что он не учитывает разную важность документов. Некоторые документы более значимы и должны ранжироваться в первую очередь. Для решения этой проблемы предлагаются следующие подходы:

1. Перевзвешивание пар документов при расчёте функции потерь.
2. Изменение функции потерь с учётом целевой метрики.
3. Модификация градиентов пропорционально их влиянию на целевую метрику.

Последний подход был реализован в методе **LambdaRank**.

  

# LambdaRank

**LambdaRank** — это усовершенствование RankNet, где функция потерь основывается на **градиенте**, пропорциональном влиянию перестановки на целевую метрику, например, nDCG. Ключевая идея заключается в том, что вместо минимизации кросс-энтропии для каждой пары документов система оптимизирует метрику, напрямую отражающую качество ранжирования.

Градиенты рассчитываются так, чтобы важные изменения в позициях документов (сильно влияющие на метрику) имели больший вес, чем менее значимые. Это позволяет системе учитывать приоритетность документов, которые сильнее влияют на итоговую метрику, а не просто минимизировать число инверсий в ранжировании.

**Формула для расчёта градиента функции потерь** $\lambda_{ij}$ в LambdaRank:

$\Large \lambda_{ij} = -\frac{\partial C}{\partial s_i} = \frac{1}{1 + e^{-(s_i - s_j)}} \cdot \Delta \text{nDCG}_{ij}$

Где:

- $\large s_i$, $\large s_j$ — предсказанные значения релевантности для документов $i$ и $j$.
- $\Delta \text{nDCG}_{ij}$ — изменение метрики $nDCG$ при перестановке документов $i$ и $j$.

Грубо говоря, чем больше прирост метрики от повышения или понижения позиции в ранжировании, тем больше это влияет на изменение предсказаний модели. В итоге более релевантные документы давят сверху, заставляя нерелевантные опускаться вниз, а менее релевантные давят снизу, заставляя всплывать подходящие документы.

  

# MART (Multiple Additive Regression Trees)

**MART** — это метод бустинга регрессионных деревьев. Он основан на построении последовательности деревьев, каждое из которых корректирует ошибки предыдущих. Алгоритм работает следующим образом:

1. На каждом этапе строится новое дерево, цель которого — минимизировать ошибку на текущей итерации.
2. Градиент (или антиградиент) используется для корректировки предсказаний, показывая, насколько нужно изменить предсказание для улучшения модели.
3. В каждом конечном узле дерева значение предсказания обновляется для минимизации функции потерь.

**Формула обновления предсказания**:

$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$

Где:

- $F_m(x)$ — предсказание на текущей итерации.
- $F_{m-1}(x)$ — предсказание на предыдущей итерации.
- $\nu$ — шаг обучения (learning rate).
- $h_m(x)$ — новое дерево, обученное на остатках предыдущего этапа.

  

# LambdaMART

**LambdaMART** — это комбинация **LambdaRank** и **MART**, использующая идеи бустинга деревьев для улучшения качества ранжирования. LambdaMART объединяет преимущества LambdaRank (оптимизация целевых метрик) и деревьев решений (возможность построения сложных зависимостей). Алгоритм учитывает лямбда-градиенты, показывающие, как перестановка двух документов влияет на метрику, и использует их для построения новых деревьев.

На каждом этапе обучения нового дерева алгоритм оценивает изменение целевой метрики (например, $nDCG$) при перестановке двух документов и использует эту информацию для вычисления целевых значений. Таким образом, каждое новое дерево оптимизирует метрику, а не просто минимизирует ошибку предсказания.

**Формула обновления весов в LambdaMART**:

$\Delta w_k = -\eta \cdot \sum_{i=1}^N \lambda_i \cdot \frac{\partial F(x_i)}{\partial w_k}$

Где:

- $\eta$ — шаг обучения.
- $\lambda_i$ — градиент, учитывающий влияние перестановки на метрику.
- $F(x_i)$ — предсказание модели для документа i.
- $w_k$ — веса модели.

[[Data Science/Tables/Статьи/Статьи/Untitled]]

# YetiRank

**[YetiRank](https://proceedings.mlr.press/v14/gulin11a/gulin11a.pdf)** — это метод, используемый в CatBoost, разработанный для оптимизации задач ранжирования. Он представляет собой усовершенствованную версию попарного подхода, как в RankNet, но с дополнительными весами для каждой пары документов. Эти веса учитывают важность конкретной пары и сложность правильного ранжирования.

Основные особенности YetiRank:

- **Перевзвешивание пар**: Каждой паре документов присваивается вес, показывающий её важность для обучения. Это помогает модели уделять больше внимания важным парам и игнорировать менее значимые.
- **Добавление шума**: На каждой итерации в предсказания добавляется случайный шум, что позволяет лучше оценить важность каждой пары и избежать переобучения. Затем документы переранжируются, и метрика MRR (Mean Reciprocal Rank) используется для увеличения весов более важных пар.
- **Факторизация градиентов**: YetiRank использует метод факторизации градиентов, что существенно ускоряет процесс обучения. Вместо квадратичной сложности по числу документов метод достигает почти линейной зависимости, делая обучение более эффективным.

**Формула для вычисления весов пар документов**:

В качестве основной функции потерь используется та же самая формула, что была и в RankNet при pairwise-подходе, однако каждая пара дополнительно взвешивается множителем $w_{ij}$ . Авторы также упоминают возможность добавления уже изученной нами $\lambda_i$ при расчёте градиентов.

$\Large w_{ij} = N \cdot c(l_i, l_j)$

Где:

- $N$ — нормировочный коэффициент.
- $c(l_i, l_j)$— функция, учитывающая важность и сложность ранжирования пары документов с метками релевантности $l_i$ и $l_j.$

YetiRank позволяет оптимизировать сложные метрики ранжирования напрямую, что делает его особенно эффективным для задач, требующих высокого качества ранжирования, таких как информационный поиск и рекомендательные системы.