---
Owner: IIvan Vashchenko
---
> [!important]
> 
> - **Pointwise** подходы подходят для простых задач и базируются на классических методах регрессии и классификации, но не учитывают взаимодействие между документами.
> - **Pairwise** подходы, такие как RankNet, позволяют моделировать относительный порядок между документами, что лучше отражает задачу ранжирования.
> - **Listwise** подходы, такие как ListNet, нацелены на оптимизацию порядка всего списка, что делает их наиболее эффективными для задач, где важна точная последовательность документов.

# Три подхода к обучению моделей ранжирования

Существует три основных подхода к обучению моделей ранжирования:

1. **Pointwise (поточечный)**
    - Функция ошибки рассчитывается по каждому объекту отдельно (в пару к запросу).
    - Зависимость от конкретного документа и запроса $x_{j}^{q}$.
    - Модель является функцией $f(x_{j}^{q})$, и функция потерь $l(f(x_{j}^{q}), r_{j}^{q})$ измеряет отклонение от меры релевантности $r_{j}^{q}$.
    - Примеры: косинусное расстояние между эмбеддингами, логистическая регрессия, многослойный перцептрон (MLP), индекс BM25.
2. **Pairwise (попарный)**
    - Функция ошибки рассчитывается по паре объектов (в пару к запросу).
    - Примером является **RankNet** — один из первых подходов к ранжированию, в котором используется сиамская нейронная сеть для обучения относительного порядка двух документов.
    - При использовании пары документов модель обучается минимизировать количество неправильных ранжирований (инверсий).
3. **Listwise (списочный)**
    - Функция ошибки рассчитывается на всём списке документов для конкретного запроса.
    - Пример — **ListNet**, где задача заключается в упорядочивании всех документов в соответствии с их релевантностью запросу.
    - В отличие от попарного подхода, здесь рассматривается вероятность всей перестановки документов, и в качестве функции потерь может использоваться кросс-энтропия или дивергенция Кульбака-Лейблера.

## Pointwise-подход и BM25 (Best Matching)

**BM25** — это функция ранжирования, используемая в поисковых системах для оценки релевантности документов относительно некоторого запроса. BM25 является усовершенствованным вариантом TF-IDF и работает по принципу "мешка слов" (bag of words).

**Формула оценки релевантности BM25**:

$\Large \text{score}(D, Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$

Где:

- $f(q_i, D)$— частота слова $q_i$ в документе $D$.
- $D$ — длина документа, $avgdl$ — средняя длина документов в коллекции.
- $k_1$ и $b$ — параметры, регулирующие важность частоты и нормализацию по длине документа.

**IDF** (обратная частота документа) учитывает, насколько часто слово встречается в корпусе:

$\large IDF(q_i) = \ln\left(\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1\right)$- сглаженный вариант $IDF$

Где $N$ — общее количество документов, $n(q_i)$ — количество документов, содержащих слово $q_i.$

### **Недостатки BM25**

- Значение отрицательно, если производится расчёт для слова, входящего более чем в половину документов (частотные слова, stop-слова);
- Функция сконструирована вручную и ничего не обучается (два параметра _$k_1$_ и _$b$_ можно перебрать).

### **Достоинства BM25**

- Всё еще очень хороший и невероятно быстрый способ фильтрации кандидатов из огромного корпуса документов;
- Формула максимально проста.

---

## Pairwise-подход и RankNet

**RankNet** — это попарный подход к обучению ранжирования, разработанный для минимизации количества неверных ранжирований. В RankNet используется нейросеть, обучаемая предсказывать, какой из двух документов должен быть выше в списке.

Функция потерь в RankNet основана на **кросс-энтропии** между предсказанной вероятностью и истинной меткой:

$\Large C_{ij} = -Y_{ij} \cdot \log(P_{ij}) - (1 - Y_{ij}) \cdot \log(1 - P_{ij}) = -Y_{ij}​ o_{ij}​ + \log(1+e^{o_{ij}}​)$

Где $P_{ij}$ — вероятность того, что документ $i$ должен быть отранжирован выше документа $j$.

Для вычисления вероятности используется логистическая функция:

$\Large P_{ij} = \frac{1}{1 + e^{-o_{ij}}}$

Где $o_{ij} = f(x_i) - f(x_j)$ — разность логитов для двух документов.

Таким образом, целью обучения является минимизация функции потерь $C_{ij}$, которая стремится к правильному ранжированию пар документов.

---

## Listwise-подход и ListNet

**ListNet** — это списочный подход, в котором функция потерь рассчитывается на всём списке документов для конкретного запроса. Задача состоит в том, чтобы максимизировать вероятность того, что документы будут отранжированы в правильном порядке.

В ListNet используется функция **SoftMax** для преобразования скоров документов в вероятности:

$\Large P(j) = \frac{e^{s_j}}{\sum_{k=1}^{n} e^{s_k}}$

Где $s_j$ — скор документа $j$.

Для обучения модели применяется кросс-энтропия между предсказанным распределением вероятностей и истинным распределением релевантности:

$\Large L(y, z) = - \sum_{j=1}^{n} y_j \log(z_j)$

Где $y_j$ — истинная вероятность для документа $j$, а $z_j$ — предсказанная вероятность.