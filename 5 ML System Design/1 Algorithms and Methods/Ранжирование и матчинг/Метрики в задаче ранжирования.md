---
Owner: IIvan Vashchenko
---
> [!important]
> 
> - **Ранжирование** — это не только классификация документов на релевантные и нерелевантные, но и определение оптимального порядка их отображения.
> - **Метрики качества** включают **precision**, **recall**, **F-меру**, **AP**, **MAP**, **DCG** и другие, каждая из которых измеряет различные аспекты качества ранжирования.
> - **Оценка эффективности** учитывает не только качество результата, но и время выполнения, ресурсозатратность и удобство для пользователя.
> - **Использование сложных метрик**, таких как **PFound** и **MRR**, позволяет точнее учитывать поведение пользователя и давать оценку, приближенную к реальным сценариям использования системы.
> 
> Эти метрики и методы помогают создавать эффективные системы ранжирования, способные предоставлять пользователям релевантную информацию быстро и точно, улучшая общий пользовательский опыт.

# Что измерять в ранжировании?

При оценке качества ранжирования учитываются три ключевых аспекта:

- **Качество/точность** — оценивает, насколько точно система ранжирует релевантные документы выше нерелевантных. Это критически важно, так как пользователь ожидает видеть наиболее релевантные результаты в начале списка. Чем выше релевантность первых документов, тем эффективнее система.
- **Эффективность** — определяется скоростью выдачи ответа системой и ресурсами, необходимыми для выполнения задачи. Важно, чтобы система была не только точной, но и работала быстро, особенно при обработке большого объёма документов.
- **Удобство использования** — отражает, насколько полезна система для решения задач пользователя. Включает в себя удобство интерфейса, понятность представленных результатов и общее впечатление от использования (UX).

  

# Оценка качества ранжирования

Для оценки качества ранжирования применяется **Методология Кранфилда (Cranfield Evaluation Methodology)**. Она включает:

- **Фиксированный набор документов** и **набор запросов**. Это создаёт единую базу для тестирования различных систем.
- **Оценки релевантности** пар запрос-документ, часто устанавливаемые пользователями системы. Это необходимо для адекватной оценки результатов.
- **Репрезентативность наборов** — запросы и документы должны охватывать широкий спектр тем и типов информации, чтобы результаты оценки были применимы в реальных условиях.

**Методология Кранфилда** позволяет объективно сравнивать различные системы ранжирования на одной и той же базе данных и делать выводы об их качестве.

### Precision, Recall, F-мера и PR-кривая

- **Precision** (точность) — доля документов, классифицированных системой как релевантные и действительно оказавшихся таковыми. Precision показывает, насколько выдача системы «очищена» от нерелевантных документов. Формула:
    - $\text{Precision} = \frac{ | \{ \text{relevant documents} \} \cap \{ \text{retrieved documents} \} | }{ | \{ \text{retrieved documents} \} | }$
- **Recall** (полнота) — доля всех релевантных документов, найденных системой. Recall показывает, насколько полно система охватывает все релевантные объекты. Формула:
    - $\text{Recall} = \frac{ | \{ \text{relevant documents} \} \cap \{ \text{retrieved documents} \} | }{ | \{ \text{relevant documents} \} | }$
- **F-мера** — гармоническое среднее между precision и recall, объединяющее обе метрики в одну с возможностью регулирования их относительной важности через параметр \( \beta \).Формула:
    
    - $F_\beta = (1 + \beta^2) \cdot \frac{\text{precision} \cdot \text{recall}}{\beta^2 \cdot \text{precision} + \text{recall}}$
    
    Обычно используется **F1-мера** (при $\beta = 1$), придающая равное значение precision и recall.
    
- **PR-кривая** — отображает зависимость между precision и recall при разных порогах релевантности. Итоговая метрика **PR-AUC** (площадь под PR-кривой) используется для оценки качества модели в целом. Чем больше площадь под кривой, тем лучше модель.

> **Accuracy** (доля правильных ответов) часто оказывается неподходящей метрикой для задач ранжирования, особенно при значительном дисбалансе классов, когда большинство документов нерелевантны. Высокое значение accuracy может не отражать реальное качество системы, поскольку даже модель, не выдающая релевантные документы, может показывать высокую точность за счёт преобладания нерелевантных классов. В таких случаях accuracy достигает максимума, не решая фактическую задачу поиска.

---

### Average Precision и Mean Average Precision

- **Average Precision (AP)** — показывает, насколько много релевантных документов содержится в выдаче, особенно среди топовых результатов. Метрика чувствительна к позиции релевантных документов в списке. Формула:
    
    $AP = \sum_{k=1}^{n} (\text{Recall@k} - \text{Recall@(k-1)}) \cdot \text{Precision@k}$
    
    ![[image 17.png|image 17.png]]
    
- **Mean Average Precision (MAP)** — среднее значение AP по всем запросам, позволяющее оценить общую эффективность системы для множества различных запросов. Формула:
    
    $\large MAP = \frac{1}{Q} \sum_{q=1}^{Q} AP(q)$
    
    Здесь $Q$ — общее число запросов.
    

---

### Переход к многоуровневой задаче и Gain

Для улучшения оценки качества ранжирования вводится понятие **многоуровневой релевантности**. Например, выделяют три уровня:

1. Нерелевантно
2. В целом релевантно
3. Очень релевантно

На основе этой шкалы рассчитывается **Cumulative Gain (CG)**, суммирующий уровни релевантности всех документов. Однако CG не учитывает позицию документов в выдаче, поэтому вводится **Discounted Cumulative Gain (DCG)**, уменьшающий вклад документов, находящихся ниже в списке, с учётом их позиции.

Формула для DCG:

$$DCG@k = \sum_{i=1}^{k} \frac{gain_i}{\log_2(i + 1)}$$

---

### PFound (Yandex)

Метрика **PFound**, используемая Яндексом, оценивает вероятность того, что пользователь найдёт релевантный результат. Она учитывает два аспекта:

- **Вероятность просмотра** следующего документа — пользователи просматривают документы сверху вниз, и вероятность просмотра следующего документа снижается, если предыдущий оказался нерелевантным.
- **Вероятность остановки** — пользователь может прекратить просмотр, найдя релевантный результат или по другим причинам (например, потеря интереса).

Формула расчёта PFound учитывает эти вероятности, что позволяет точнее оценивать качество выдачи.

---

### Исторические метрики

- **MRR (Mean Reciprocal Rank)** — среднее значение обратного ранга первого релевантного документа. Эта метрика используется, когда в ответ на запрос ожидается один релевантный документ. MRR показывает, насколько близко к началу списка выдачи находится релевантный документ. Формула:
    
    $MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}$
    
- **Коэффициент корреляции Кендалла (Kendall's τ)** — оценивает согласованность порядка между двумя ранжированиями. Он измеряет количество согласованных и несогласованных пар, что позволяет оценить качество ранжирования с точки зрения согласованности. Формула:
    
    $\tau = \frac{(\text{число согласованных пар}) - (\text{число несогласованных пар})}{\binom{n}{2}}$