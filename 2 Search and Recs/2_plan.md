# –ü–ª–∞–Ω –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π: –°–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

> **–§–æ–∫—É—Å:** –Ω–∞–≤—ã–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤–æ‚Äë—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –±–∞–∑–µ LLM‚Äëretriever‚Äë—Å—Ç–µ–∫–∞. –¢–µ–º—ã –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã, CI/CD –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ.

---

## I. Retrieval‚Äë—Å—Ç–µ–∫
- **Dense search:**   
  üîó [Dimension, Pooling and Normalization]()  
  üîó [Cross and Bi Encoders and Twin Towers]()  
  üîó [Late Interaction]()  
  üîó [Vector Imbalance]()  
  üîó [Asymmetric Search]()  
  üîó [Embedding Drift]()  

- **Sparse signatures:**   
  üîó [Inverted Index]()  
  üîó [BM25](https://habr.com/ru/articles/545634/)  
  üîó [SPLADE](https://arxiv.org/abs/2107.05720)  
  üîó [DocT5Query –∏ Query Expansion]()  
  üîó [Neural Sparse Retrieval](https://qdrant.tech/articles/modern-sparse-neural-retrieval/)  
  üîó [Data Drift]()  

- **Hybrid fusion:**:
  üîó [Score fusion]()  
  üîó [Reciprocal Rank Fusion (RRF)]()  
  üîó [w_{lex}¬∑BM25 + w_{dense}¬∑\cos]()  
  üîó [lexical fallback]()  
  üîó [–ø—Ä–æ–±–ª–µ–º—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏]()  

- **ANN indexing:**   
  üîó [Multi-shard]()  
  üîó [HNSW](https://habr.com/ru/companies/vk/articles/338360/)  
  üîó [IVF](https://medium.com/@Jawabreh0/inverted-file-indexing-ivf-in-faiss-a-comprehensive-guide-c183fe979d20)  
  üîó [PQ](https://www.pinecone.io/learn/series/faiss/product-quantization/)  
  üîó [IVF-PQ](https://lancedb.github.io/lancedb/concepts/index_ivfpq/)  
  üîó [OPQ](https://ieeexplore.ieee.org/abstract/document/6678503/)  
  üîó [ScaNN](https://habr.com/ru/articles/591241/)  
  üîó [Tail Latency](https://zilliz.com/ai-faq/why-is-tail-latency-p95p99-often-more-important-than-average-latency-for-evaluating-the-performance-of-a-vector-search-in-userfacing-applications)  
  üîó [Metadata Filtering]()  

- **Corpora Quality:**   
  üîó [Deduplication Cleaning (MinHash/SimHash)]()  
  üîó [Segmentation Long Docs]()  
  üîó [Small Corpora Retrieval]()  


## II. –ö–∞—Å–∫–∞–¥¬†reranking
- **Late‚Äëinteraction** (ColBERT)¬†vs **Cross‚Äëencoder** (bge‚Äëreranker)¬†‚Äî trade‚Äëoff latency¬†‚Üî quality.  
- –ú—É–ª—å—Ç–∏‚Äë—ç—Ç–∞–ø: bi‚Äëencoder ‚Üí ColBERT¬†‚Üí cross; –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π¬†$K/M$.  
- **–§–∏—á–∏ –¥–ª—è rerank:** dense sim, BM25, –ø–æ–∑–∏—Ü–∏—è, —Å–≤–µ–∂–µ—Å—Ç—å, user signals.

## III. Learning‚Äëto‚Äërank
- **–ú–æ–¥–µ–ª–∏:** LambdaMART, RankNet, ListNet, TF‚ÄëRanking, LightGBM‚Äëranker.  
- **Pairwise vs listwise:** –∫–æ–≥–¥–∞ —á—Ç–æ.  
- **On‚Äëline fine‚Äëtune:** soft‚Äëlabels –∏–∑ –∫–ª–∏–∫–æ–≤, knowledge¬†distillation –∏–∑ cross‚Äëencoder.  
- **Distillation:**  
  ‚Äì *Hard-label distillation:* —É—á–µ–Ω–∏–∫ —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä —É—á–∏—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, top‚Äë1).  
  ‚Äì *Soft-label distillation:* —É—á–µ–Ω–∏–∫ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ (softmax) —É—á–∏—Ç–µ–ª—è.  
  ‚Üí Soft‚Äëlabel distillation —á–∞—â–µ –¥–∞—ë—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã—Ö –∏ —Ä–∞–Ω–∂–∏—Ä—É—é—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö.

## IV. RAG¬†–∏ generative¬†search
- Pipeline: retriever¬†‚Üí reranker¬†‚Üí LLM‚Äë–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä.  
- **Dynamic context selection:** –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è top‚ÄëK, –º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å.  
- Multi‚Äëhop / memory‚Äëaugmented RAG; answer re‚Äëranking.
- Conversational retrieval:
  - Multi-turn query refinement (–ø–æ—à–∞–≥–æ–≤–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞).
  - Context carry-over ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ retriever.
  - Query planning: —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã –∏ –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.
  - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å reranker‚Äô–∞–º–∏ –∏ multi-hop RAG.
  - –ú–µ—Ç—Ä–∏–∫–∏: groundedness –Ω–∞ —Å–µ—Å—Å–∏–∏, cumulative recall@K, –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —É—Ç–æ—á–Ω–µ–Ω–∏–π.



## V. Online‚Äëfeedback¬†–∏ bandits
- **Explore‚ÄëExploit:** Thompson Sampling, Œµ‚Äëgreedy, UCB¬†‚Äî —Ä–æ—Ç–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.  
- **Counterfactual LTR:** IPS, DLA, SNIPS, off‚Äëpolicy evaluation.  
- **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:** Vowpal¬†Wabbit `--cb`, Meta BanditPAM.

## VI. Tail‚Äëlatency
- Hedged queries, replication, timeout‚Äëbased early abort.  
- **Adaptive¬†nprobe / efSearch** (IVF/HNSW) –ø–æ–¥ SLA.  
- –°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (draft + verifier), streaming¬†LLM.  
- –ö—ç—à‚Äë—Å–ª–æ–∏: embedding, ANN‚ÄëtopK, rerank scores.

## VII. Embedding¬†lifecycle
- Drift‚Äëmonitor: PSI, KL‚Äëdiv, embedding norm shift.  
- Shadow‚Äëindex –∏ alias‚Äëswitch –¥–ª—è zero‚Äëdowntime re‚Äëindex.  
- –ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å re‚Äëtrain¬†/ re‚Äëembed; —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π.

## VIII. –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è¬†–∏¬†cold‚Äëstart
- **Cold‚Äëstart items:** meta‚Äëfeatures, zero‚Äëshot E5/BGE‚ÄëM3, graph‚Äëpropagation.  
- **Cold‚Äëstart users:** popular‚Äëfallback, persona embeddings, federated warm‚Äëup.  
- Two‚Äëtower + LightGCN –¥–ª—è user√óitem¬†—Å–∏–º–º–µ—Ç—Ä–∏–∏.

## IX. Bias¬†–∏ fairness
- –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å‚Äëbias, exposure‚Äëparity @K, calibration.  
- –ú–∏—Ç–∏–≥–∞—Ç–æ—Ä—ã: re‚Äërank‚Äëconstraints, FairMatch, Œîpop penalty.

## X. –ú–µ—Ç—Ä–∏–∫–∏¬†–∫–∞—á–µ—Å—Ç–≤–∞
- **Offline:** MRR, nDCG@K, Recall@K, MAP; bootstrap¬†CI.  
- **Online:** CTR, dwell‚Äëtime, p50/p95 latency, Œîbusiness¬†metric; Sequential¬†/¬†CUPED¬†/ Bayesian¬†A/B.  
- **Fairness¬†metrics:** disparity¬†ratio, fairness@K.

## XI. –î–æ—Ä–æ–∂–Ω–∞—è¬†–∫–∞—Ä—Ç–∞¬†–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π

| –≠—Ç–∞–ø | –¢–µ–º–∞                            | –ü—Ä–∞–∫—Ç–∏–∫–∞ / –ù–∞–≤—ã–∫–∏                                             | –†–µ–∑—É–ª—å—Ç–∞—Ç                                 |
| ---- | ------------------------------- | ------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| 1    | **–ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫**               | BM25 vs Dense (Sentence-BERT, E5, BGE), Recall/nDCG —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ | –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–∑–Ω–∏—Ü—ã sparse vs dense         |
| 2    | **Hybrid retrieval**            | Weighted Sum, Reciprocal Rank Fusion, –ø–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤            | –†–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞ —Å—á—ë—Ç –≥–∏–±—Ä–∏–¥–∞             |
| 3    | **ANN –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è**              | FAISS IVF-PQ, HNSW, —Ç—é–Ω–∏–Ω–≥ `nlist`, `nprobe`, `M`, `ef`       |  Trade-off recall/latency, –≥—Ä–∞—Ñ–∏–∫–∏         |
| 4    | **Reranking**                   | Bi-encoder ‚Üí ColBERT ‚Üí Cross-encoder                          | –ö–∞—Å–∫–∞–¥ rerank —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π              |
| 5    | **Learning-to-rank**            | LightGBM Ranker, LambdaMART, pairwise/listwise, distillation  | –û—Å–≤–æ–µ–Ω–∏–µ LTR, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –Ω–µ–π—Ä–æ–Ω–∫–∞–º–∏      |
| 6    | **RAG**                         | Retriever + reranker + LLM                                    | End-to-end RAG pipeline                   |
| 7    | **Online feedback / Bandits**   | CTR simulation, Œµ-greedy, Thompson Sampling, VW `--cb`        | –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ explore‚Äìexploit                |
| 8    | **Embedding lifecycle & Drift** | Shadow index, PSI/KL-div, alias switch                        | Zero-downtime reindex, drift monitor      |
| 9    | **–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è / Cold-start** | MovieLens: Two-Tower, MF (ALS/BPR), LightGCN                  | –ë–∞–∑–æ–≤—ã–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å, cold-start —Ä–µ—à–µ–Ω–∏—è |
| 10   | **Bias & Fairness**             | Popularity bias, FairMatch, disparity\@K, calibration         | –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏          |
| 11   | **Tail latency & –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**  | Hedged queries, adaptive nprobe, p50/p95 latency              | SLA-aware –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è                     |
| 12   | **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ñ–ª–∞–π–Ω/–æ–Ω–ª–∞–π–Ω**       | MRR, nDCG, Recall\@K, bootstrap CI, CTR, A/B —Å–∏–º—É–ª—è—Ü–∏–∏        | –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –±–∏–∑–Ω–µ—Å-—ç—Ñ—Ñ–µ–∫—Ç          |

### 1. –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫

**–¶–µ–ª—å**: –ø–æ–Ω—è—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É sparse –∏ dense –ø–æ–∏—Å–∫–æ–º.
- –í–∑—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç (MS MARCO, NQ, HotpotQA).  
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å BM25 —á–µ—Ä–µ–∑ rank_bm25 –∏–ª–∏ elasticsearch.  
- –ü–æ–¥–∫–ª—é—á–∏—Ç—å –≥–æ—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–µ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, sentence-transformers/all-MiniLM).  
- –°–¥–µ–ª–∞—Ç—å dense vs BM25 —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (Recall@10, nDCG@10).

### 2. Hybrid retrieval

**–¶–µ–ª—å**: –Ω–∞—É—á–∏—Ç—å—Å—è –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Å–∏–≥–Ω–∞–ª—ã.
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Weighted Sum –∏ Reciprocal Rank Fusion –¥–ª—è BM25 + dense.  
- –ü–æ–¥–æ–±—Ä–∞—Ç—å –≤–µ—Å–∞ (grid search).  
- –°—Ä–∞–≤–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏: hybrid vs pure dense vs BM25.

### 3. ANN –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è

**–¶–µ–ª—å**: —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ FAISS / HNSW.

- –ü–æ—Å—Ç—Ä–æ–∏—Ç—å FAISS IVFPQ (–∏–≥—Ä–∞—Ç—å—Å—è —Å nlist, nprobe).  
- –ü–æ—Å—Ç—Ä–æ–∏—Ç—å HNSW (–∏–≥—Ä–∞—Ç—å—Å—è —Å M, ef).  
- –°—Ä–∞–≤–Ω–∏—Ç—å recall vs latency (–ø—Ä–∏ 100k –∏ 1M –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤).  
- –°–¥–µ–ª–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ ¬´recall@10 vs –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞¬ª.

### 4. Reranking

**–¶–µ–ª—å**: —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç reranker.

- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bi-encoder –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.  
- –í–∑—è—Ç—å ColBERT (late interaction).  
- –í–∑—è—Ç—å cross-encoder (–Ω–∞–ø—Ä–∏–º–µ—Ä, cross-encoder/ms-marco-MiniLM-L-6-v2).  
- –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–∞—Å–∫–∞–¥: bi-encoder ‚Üí ColBERT ‚Üí cross.  
- –ò–∑–º–µ—Ä–∏—Ç—å trade-off latency/–∫–∞—á–µ—Å—Ç–≤–æ.

### 5. Learning-to-rank

**–¶–µ–ª—å**: –æ—Å–≤–æ–∏—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏.

- –í–∑—è—Ç—å —Ñ–∏—á–∏: BM25, dense sim, —Å–≤–µ–∂–µ—Å—Ç—å, –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.  
- –û–±—É—á–∏—Ç—å LightGBM Ranker (pairwise –∏ listwise).  
- –°—Ä–∞–≤–Ω–∏—Ç—å —Å –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ reranker-–∞–º–∏.  
- –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å online fine-tune: —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å soft-labels –∏–∑ cross-encoder.

### 6. RAG (Retrieval-Augmented Generation)

**–¶–µ–ª—å**: —Å–æ–±—Ä–∞—Ç—å end-to-end RAG pipeline.

- –í–∑—è—Ç—å retriever (bi-encoder).  
- –î–æ–±–∞–≤–∏—Ç—å reranker.  
- –ü–µ—Ä–µ–¥–∞–≤–∞—Ç—å top-K –≤ LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, Llama-3-Instruct).  
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å dynamic context selection: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è top-K –∏ –≤—ã–±–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö.  
- –ó–∞–º–µ—Ä–∏—Ç—å faithfulness (–Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç—ã grounded).

### 7. Online feedback & Bandits

**–¶–µ–ª—å**: –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º —É–ª—É—á—à–µ–Ω–∏–µ–º.

- –°–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–∫–∏ (CTR) –Ω–∞ —Å–≤–æ–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö.  
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Œµ-greedy –∏ Thompson Sampling –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è.  
- –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–∏–Ω–∞–º–∏–∫—É CTR.  
- –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å Vowpal Wabbit --cb (contextual bandits).

### 8. Embedding lifecycle & Drift

**–¶–µ–ª—å**: –Ω–∞—É—á–∏—Ç—å—Å—è –∫–∞—Ç–∏—Ç—å –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–µ–∑ –±–æ–ª–∏.

- –ü–æ—Å—Ç—Ä–æ–∏—Ç—å shadow-–∏–Ω–¥–µ–∫—Å –Ω–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö.  
- –ü—É—Å—Ç–∏—Ç—å —á–∞—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç—É–¥–∞.  
- –°—Ä–∞–≤–Ω–∏—Ç—å nDCG/latency.  
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PSI/KL-div –¥–ª—è –æ—Ü–µ–Ω–∫–∏ drift –º–µ–∂–¥—É —Å—Ç–∞—Ä—ã–º–∏ –∏ –Ω–æ–≤—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏.

### 9. –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ cold-start

**–¶–µ–ª—å**: –ø–æ—Ç—Ä–æ–≥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ä—É–∫–∞–º–∏.

- –í–∑—è—Ç—å MovieLens 1M.  
- –û–±—É—á–∏—Ç—å Two-Tower –º–æ–¥–µ–ª—å (user tower + item tower).  
- –°—Ä–∞–≤–Ω–∏—Ç—å —Å Matrix Factorization (ALS/BPR).  
- –†–µ—à–∏—Ç—å cold-start item —á–µ—Ä–µ–∑ meta-features (–∂–∞–Ω—Ä—ã).  
- –†–µ—à–∏—Ç—å cold-start user —á–µ—Ä–µ–∑ popular-fallback.

### 10. Bias –∏ fairness

**–¶–µ–ª—å**: –ø–æ–Ω—è—Ç—å —ç—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã.

- –ò–∑–º–µ—Ä–∏—Ç—å popularity bias: –∫–∞–∫ —á–∞—Å—Ç–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ–∏–ª—å–º—ã –ø–æ–ø–∞–¥–∞—é—Ç –≤ top-K.  
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å re-rank —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –Ω–∞ diversity (FairMatch).  
- –°—Ä–∞–≤–Ω–∏—Ç—å nDCG vs fairness@K.

### 11. Tail latency & –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–¶–µ–ª—å**: –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏.

- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å hedged queries (–¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ –¥–≤–∞ –∏–Ω–¥–µ–∫—Å–∞, –±—Ä–∞—Ç—å –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç).  
- –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å adaptive nprobe –≤ FAISS (–º–µ–Ω—å—à–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ).  
- –û—Ü–µ–Ω–∏—Ç—å p50/p95 latency –¥–æ –∏ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.

### 12. –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ñ–ª–∞–π–Ω/–æ–Ω–ª–∞–π–Ω

**–¶–µ–ª—å**: –æ—Å–≤–æ–∏—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏—è.

- –°—á–∏—Ç–∞—Ç—å MRR, nDCG, Recall@K.  
- –î–µ–ª–∞—Ç—å bootstrap –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã.  
- –°–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å A/B-—Ç–µ—Å—Ç (–æ–Ω–ª–∞–π–Ω –º–µ—Ç—Ä–∏–∫–∏ CTR, dwell).