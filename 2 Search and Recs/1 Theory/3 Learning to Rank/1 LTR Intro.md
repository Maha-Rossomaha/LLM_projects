# Learning-to-Rank (LTR) Intro

**Learning-to-Rank (LTR)** — это класс алгоритмов машинного обучения, задача которых — **упорядочить** множество кандидатов по степени их релевантности к запросу.

В отличие от классификации или регрессии, LTR работает **на уровне списков** (рангов), а не индивидуальных примеров.

---

## 1. Формализация задачи

**Вход:**

* Запрос $q$
* Множество кандидатов $\mathcal{D} = {d_1, \dots, d_n}$
* Для каждого $d_i$ известен вектор признаков $x_i = \phi(q, d_i)$ и метка релевантности $y_i$

**Задача:**
Найти функцию $f(q, d_i)$, которая упорядочит $d_i$ по убыванию релевантности к запросу $q$:

$$
\text{rank}(d_1, \dots, d_n) = \text{argsort}_{i}(f(q, d_i))
$$

---

## 2. Отличие от классификации и регрессии

|               | Классификация       | Регрессия            | Learning-to-Rank                     |
| ------------- | ------------------- | -------------------- | ------------------------------------ |
| Цель          | Предсказать класс   | Предсказать число    | Отсортировать по релевантности       |
| Метка         | $y \in {0,1,...}$ | $y \in \mathbb{R}$ | $y \in {0,1,2,3}$ или перестановка |
| Потеря        | Cross-entropy       | MSE, MAE             | Pairwise / Listwise loss             |
| Применяется к | Индивидам           | Индивидам            | Спискам (кандидатам для запроса)     |

---

## 3. Типы релевантности: Binary vs Graded

* **Binary relevance**: $y_i \in {0, 1}$

  * 1 = релевантен, 0 = нерелевантен
  * Пример: клик → 1, нет → 0

* **Graded relevance**: $y_i \in {0, 1, 2, 3}$

  * 0 = не релевантен, 3 = очень релевантен
  * Пример: ручная разметка, время на документе, позиция

**Graded relevance** позволяет использовать метрики вроде nDCG и строить listwise-loss.

---

## 4. Структура данных для LTR

LTR обучается на наборах query → кандидаты → метки. Формат данных обычно следующий:

| query_id | document_id | features (x)     | label (y) |
| --------- | ------------ | ---------------- | --------- |
| 101       | D1           | \[0.8, 0.2, 0.9] | 3         |
| 101       | D2           | \[0.5, 0.4, 0.7] | 2         |
| 101       | D3           | \[0.3, 0.1, 0.2] | 0         |

LTR-модель обучается на **группах** по query_id, пытаясь выстроить внутри каждой правильный порядок.

---

## 5. Где применяется

### Поисковые системы

* Ранжирование документов по запросу
* Примеры: Google, Bing, Elastic, OpenSearch

### Рекомендательные системы

* Сортировка товаров, фильмов, видео и др.
* Пример: "Лучшие предложения для тебя"

### Диалоговые агенты

* Отбор и сортировка ответов (retrieval-based dialog)
* RAG: выбор фрагментов для LLM

---

## 6. Пример: базовый LTR с LightGBM

```python
import lightgbm as lgb
import numpy as np
import pandas as pd

df = pd.DataFrame({
    'query_id': [1, 1, 1, 2, 2, 2],
    'label':     [3, 2, 0, 0, 1, 2],
    'f1':        [0.9, 0.6, 0.1, 0.2, 0.5, 0.7],
    'f2':        [0.8, 0.5, 0.2, 0.1, 0.4, 0.6],
})

X = df[['f1', 'f2']]
y = df['label']
group = df.groupby('query_id').size().tolist()  # [3, 3]

model = lgb.LGBMRanker(objective='lambdarank', metric='ndcg')
model.fit(X, y, group=group)

preds = model.predict(X)
df['pred'] = preds
print(
    df.sort_values(
        ['query_id', 'pred'], 
        ascending=[True, False]
    )
)
```
