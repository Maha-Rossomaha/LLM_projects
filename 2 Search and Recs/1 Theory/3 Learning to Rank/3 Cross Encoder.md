# Cross-Encoder

## 1. Что такое Cross-Encoder

Cross-encoder — это архитектура на базе трансформера, которая принимает на вход пару *(query, документ)*, объединяет их в одну последовательность и прогоняет через модель. Благодаря механизму **self-attention** токены запроса и документа могут напрямую взаимодействовать друг с другом.

- **Вход**: `[CLS] query tokens [SEP] document tokens [SEP]`.
- **Модель**: трансформер (BERT, RoBERTa, DeBERTa и др.).
- **Выход**: скалярный скор релевантности (например, через линейный слой поверх `[CLS]`).

### Отличие от Bi-Encoder

- **Bi-encoder**: кодирует query и документ независимо, сравнивает эмбеддинги (косинус, dot-product). Быстро, подходит для массового retrieval.
- **Cross-encoder**: кодирует их совместно, учитывая перекрёстные зависимости между токенами. Дорого, но точнее.

### Роль в пайплайне поиска

- Используется как **reranker**: переупорядочивает небольшой top-K кандидатов, которые пришли из быстрого retriever.
- Даёт прирост по метрикам качества (MRR, nDCG), но ограничен по latency.

### Плюсы и минусы

- **Плюсы**: максимальная точность, богатое моделирование взаимодействий.
- **Минусы**: высокая вычислительная стоимость, нельзя прогонять через все документы коллекции.

### Современные практики

- Distillation: cross-encoder обучает более дешёвую модель (bi-encoder или LambdaMART), передавая soft-labels.
- Dynamic reranking: система может включать/выключать cross-encoder в зависимости от SLA (например, отключать при перегрузке).

## 2. Связь с классическими LTR-моделями

### RankNet

- Pairwise-модель: учится предсказывать, какой из двух документов более релевантен.
- Использовала простую нейросеть (MLP).

### LambdaRank / LambdaMART

- LambdaRank: улучшение RankNet, использует λ-градиенты для оптимизации nDCG напрямую.
- LambdaMART: комбинация LambdaRank и градиентного бустинга по деревьям (MART).
- Массово применяется в поисковых системах (Bing, Яндекс).

### ListNet / ListMLE

- Listwise-подход: оптимизируют распределение по всему списку документов.

### Cross-Encoder vs классические LTR

- Cross-encoder можно рассматривать как современный «RankNet на стероидах»: архитектура нейросети усложнилась (трансформеры вместо MLP), но принцип pairwise/listwise тот же — модель учится давать более высокие скоры релевантным документам.
- В отличие от LambdaMART, cross-encoder напрямую работает с текстами, а не с hand-crafted фичами.

## 3. Связь с бустингами

- LightGBM-ranker (или XGBoost-ranker) часто используется как быстрый ранжировщик.
- Их можно обучать на **feature-based input** (BM25, длина, свежесть, пользовательские сигналы).
- Также — на **distilled soft-labels** от cross-encoder: бустинг учится воспроизводить распределение скоров, которое генерирует более точный, но медленный teacher.

Таким образом, cross-encoder и бустинг не конкурируют, а дополняют друг друга.

## 4. Эволюция ранжировщиков

1. **RankNet** — первые нейросети для pairwise LTR.
2. **LambdaMART** — бустинг с $\lambda$-градиентами, стандарт индустрии.
3. **Cross-encoder** — трансформеры для reranking.
4. **Гибриды**: distillation (cross → bi-encoder или LambdaMART), late interaction (ColBERT), комбинированные каскады.

---

## 5. Ключевые выводы

- Cross-encoder = **нейросетевой reranker** с максимальным качеством.
- RankNet, LambdaRank, LambdaMART, ListNet, ListMLE = **семейство алгоритмов LTR**.
- Cross-encoder может использоваться как teacher, а быстрые модели (bi-encoder, LambdaMART) — как student.
- В продакшн-системах часто строят **каскад retrieval → reranking**, где cross-encoder — финальный «тяжёлый» слой, обеспечивающий качество.
