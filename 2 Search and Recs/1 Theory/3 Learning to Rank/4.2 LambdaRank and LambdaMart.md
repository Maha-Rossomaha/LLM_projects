# LambdaRank / LambdaMART — градиентная оптимизация метрик ранжирования

**LambdaRank** и **LambdaMART** — это модели класса pairwise Learning-to-Rank, в которых градиенты loss-функции напрямую зависят от ранговых метрик (например, nDCG). В отличие от RankNet, здесь ключевая идея — использовать не фиксированные метки, а **градиенты, пропорциональные улучшению метрики при перестановке пары**.

LambdaMART — это практическая реализация LambdaRank на основе **градиентного бустинга над деревьями** (MART = Multiple Additive Regression Trees).

---

## 1. Основная идея LambdaRank

### В RankNet

* Рассматриваются все пары документов $(i, j)$ в рамках одного запроса.
* Для каждой пары строится вероятность правильного порядка через функцию:
  $P_{ij} = \sigma(s_i - s_j),$
  где $s_i$ — скор документа $d_i$.
* Лосс считается как бинарная кросс-энтропия относительно $P_{ij}$ и метки $y_{ij}$.
* Важно: **все пары влияют одинаково**, независимо от того, стоят ли документы в топ-3 или внизу списка.

### В LambdaRank

LambdaRank изменяет только **градиенты**, а не саму функцию потерь. Ключевая идея:

1. **Псевдоградиенты $\lambda_{ij}$**

   * Мы не выписываем явно loss.
   * Вместо этого определяем, какой градиент должен пойти на $s_i$ и $s_j$.
   * Это позволяет встраивать знание о метрике качества напрямую в процесс обучения.

2. **Масштабирование на $|\Delta nDCG_{ij}|$**

   * Считаем, насколько изменится итоговый nDCG, если документы $d_i$ и $d_j$ поменять местами.
   * Если перестановка почти не влияет на метрику (например, документы далеко внизу), вес маленький.
   * Если перестановка сильно портит nDCG (например, перепутали топовые результаты), вес большой.

   Формула:
   $\lambda_{ij} \propto |\Delta nDCG_{ij}| \cdot \sigma(-(s_i - s_j))$

3. **Интерпретация**

   * Градиенты теперь не равны для всех пар.
   * Чем выше документ в списке и чем выше разница в релевантности, тем сильнее его вклад в обучение.
   * Модель учится уделять внимание тем ошибкам, которые критичнее для пользователя.

### Интуитивный смысл

* RankNet: "любая ошибка одинаково важна".
* LambdaRank: "важнее исправить ошибки наверху списка, так как они сильнее влияют на пользовательский опыт".

### Пример

* Представим, что перепутаны документы на позициях 2 и 3.
* Перестановка повлияет на nDCG меньше, чем если перепутаны документы на позициях 1 и 2.
* Значит, $|\Delta nDCG_{ij}|$ для первой пары будет больше → больший градиент.

---

## 2. Градиенты LambdaRank

Пусть $s_i = f(q, d_i)$ — score документа, а $y_i$ — его релевантность.

Пусть $(i, j)$ — пара с $y_i > y_j$. Тогда изменение nDCG при перестановке:

$$
\Delta \text{nDCG}_{ij} = | \text{nDCG}_{\text{before}} - \text{nDCG}_{\text{after}} |
$$

Затем определяем $\lambda_{ij}$ как:

$$
\lambda_{ij} = \Delta \text{nDCG}_{ij} \cdot \sigma(-s_i + s_j)
$$

Где $\sigma$ — сигмоида (или её производная, в зависимости от варианта реализации).

Общий градиент по $s_i$:

$$
\lambda_i = \sum_{j: y_i > y_j} \lambda_{ij} - \sum_{j: y_j > y_i} \lambda_{ji}
$$

---

## 3. LambdaMART = LambdaRank + GBDT

### Основная идея

* LambdaMART объединяет механизм псевдоградиентов из **LambdaRank** и алгоритм **градиентного бустинга над решающими деревьями (GBDT)**.
* Вместо нейросети (как в RankNet) здесь используется ансамбль деревьев.

### Процесс обучения

1. **Формирование пар**
   Для каждого запроса строятся все пары документов $(i, j)$, где $y_i > y_j$.

2. **Вычисление $\lambda_i$**

   * Сначала считаются $\lambda_{ij}$ для каждой пары, как в LambdaRank.
   * Затем для каждого документа $i$ агрегируется $\lambda_i$:

     $$
     \lambda_i = \sum_{j: y_i > y_j} \lambda_{ij} - \sum_{j: y_j > y_i} \lambda_{ji}
     $$

3. **Использование $\lambda_i$ в boosting**

   * В алгоритме градиентного бустинга $\lambda_i$ интерпретируются как псевдоградиенты loss-функции.
   * На их основе обучается новое дерево, аппроксимирующее направление улучшения метрики.

4. **Обновление модели**

   * Каждое дерево добавляется в ансамбль с определённым весом.
   * Итоговый скор документа $s_i$ постепенно уточняется от итерации к итерации.

### Практическое значение

* LambdaMART оптимизирует метрики ранжирования (например, nDCG\@k) напрямую.
* Является стандартом де-факто в поисковых системах, рекомендательных сервисах и рекламе.
* Поддерживается в библиотеках **LightGBM**, **XGBoost (ранжирование)**, **CatBoost**.


---

## 4. Примеры

### 4.1 LambdaRank

```python
# Пример toy-реализации LambdaRank в терминах RankNet с учетом градиентов по nDCG

import torch
import torch.nn as nn
import torch.nn.functional as F
from itertools import combinations

# Считаем DCG для списка релевантностей

def dcg(relevances):
    rel = torch.tensor(relevances, dtype=torch.float32)
    return torch.sum((2 ** rel - 1) / torch.log2(torch.arange(2, len(rel) + 2).float()))

# Простая нейросеть для score-документов
class RankNet(nn.Module):
    def __init__(self, n_features):
        super().__init__()
        self.ff = nn.Sequential(
            nn.Linear(n_features, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.ff(x).squeeze(-1)

# Один батч с 1 запросом, 4 документов
features = torch.tensor([
    [0.1, 0.2],
    [0.4, 0.3],
    [0.7, 0.8],
    [0.5, 0.6],
])
labels = torch.tensor([0, 1, 3, 2])

model = RankNet(n_features=2)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)

for epoch in range(50):
    scores = model(features)
    loss = 0.0
    
    # все пары (i, j), где label[i] > label[j]
    for i, j in combinations(range(len(labels)), 2):
        if labels[i] == labels[j]:
            continue
        if labels[i] > labels[j]:
            i_high, j_low = i, j
        else:
            i_high, j_low = j, i

        # считаем позиционную дельту DCG
        sorted_ids = scores.argsort(descending=True)
        rels_before = labels[sorted_ids]
        dcg_before = dcg(rels_before)

        swapped = sorted_ids.clone()
        pos_i = (sorted_ids == i_high).nonzero().item()
        pos_j = (sorted_ids == j_low).nonzero().item()
        swapped[pos_i], swapped[pos_j] = swapped[pos_j], swapped[pos_i]

        rels_after = labels[swapped]
        dcg_after = dcg(rels_after)

        delta_ndcg = abs(dcg_before - dcg_after)
        pred_diff = scores[j_low] - scores[i_high]
        lambda_ij = delta_ndcg * torch.sigmoid(pred_diff)

        loss += lambda_ij

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}: loss = {loss.item():.4f}")
```

### 4.2 LambdaMART

```python
# Псевдореализация LambdaMART (градиентный бустинг деревьев с lambda-градиентами)
# Для упрощения: логика 1 дерева, без полноценного бустинга

import numpy as np
from sklearn.tree import DecisionTreeRegressor
from itertools import combinations

# Вспомогательная функция: DCG

def dcg(relevances):
    relevances = np.asarray(relevances)
    return np.sum((2**relevances - 1) / np.log2(np.arange(2, len(relevances)+2)))

# Пример выборки (один запрос, 5 документов)
X = np.array([
    [0.1, 0.2],
    [0.3, 0.1],
    [0.7, 0.8],
    [0.5, 0.4],
    [0.4, 0.9],
])
y = np.array([0, 1, 3, 2, 1])

# Инициализируем предсказания (скоры)
scores = np.zeros(len(y))

# Вычисляем lambda-градиенты для одной итерации бустинга
lambdas = np.zeros_like(y, dtype=np.float32)

for i, j in combinations(range(len(y)), 2):
    if y[i] == y[j]:
        continue
    if y[i] > y[j]:
        i_high, j_low = i, j
    else:
        i_high, j_low = j, i

    sorted_ids = np.argsort(-scores)
    dcg_before = dcg(y[sorted_ids])

    swapped = sorted_ids.copy()
    pi, pj = np.where(sorted_ids == i_high)[0][0], np.where(sorted_ids == j_low)[0][0]
    swapped[pi], swapped[pj] = swapped[pj], swapped[pi]
    dcg_after = dcg(y[swapped])

    delta_ndcg = abs(dcg_before - dcg_after)
    score_diff = scores[i_high] - scores[j_low]
    rho = 1 / (1 + np.exp(score_diff))
    lambda_ij = delta_ndcg * rho

    lambdas[i_high] -= lambda_ij
    lambdas[j_low] += lambda_ij

# Обучаем одно дерево на этих градиентах
reg = DecisionTreeRegressor(max_depth=3)
reg.fit(X, lambdas)

# Обновляем скоры (одна итерация boosting)
scores += reg.predict(X)

print("Updated scores:", scores.round(4))
```

---

## 5. Преимущества и особенности обучения

### Преимущества

* Оптимизирует именно **ранговые метрики** (nDCG, MAP)
* Учитывает позицию документа в списке
* Поддерживается во многих библиотеках (LightGBM, XGBoost, CatBoost)
* Хорошо работает с табличными признаками (features)

### Особенности

* Требует группировки данных по запросам (query groups)
* Чувствителен к шумным меткам
* Относительно затратен по времени обучения на больших группах

---

## 6. Вывод

LambdaMART — одна из самых мощных и широко применяемых моделей ранжирования. В отличие от RankNet, она позволяет напрямую оптимизировать метрику качества, такую как nDCG\@k, через механизм псевдо-градиентов ("лямбд"). Она особенно эффективна в задачах с табличными признаками и фиксированным числом кандидатов на запрос.
