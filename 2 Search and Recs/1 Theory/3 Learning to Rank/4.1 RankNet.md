# RankNet

**RankNet** — классическая модель для задач Learning-to-Rank. Она формулирует задачу ранжирования как задачу бинарной классификации пар документов: какой из двух более релевантен запросу.

RankNet использует feed-forward нейросеть для оценки score $s_i = f(q, d_i)$, а затем обучается с помощью **pairwise cross-entropy loss**.

---

## 1. Формулировка задачи

Имеется:

* Запрос $q$
* Два документа $d_i$ и $d_j$, такие что $y_i > y_j$ (то есть $d_i$ более релевантен)
* Модель выдаёт скоры $s_i = f(q, d_i)$ и $s_j = f(q, d_j)$

Задача: максимизировать вероятность, что $d_i$ действительно выше в ранге, чем $d_j$

Для этого вводится вероятность через сигмоиду:

$$
P(d_i \succ d_j) = \sigma(s_i - s_j) = \frac{1}{1 + \exp(-(s_i - s_j))}
$$

Где:

* $s_i, s_j$ — скалярные оценки релевантности
* $\sigma$ — сигмоида

---

## 2. Функция потерь (pairwise cross-entropy)

Вводим бинарную метку:

$$
y_{ij} = \begin{cases}
1, & \text{если } d_i \succ d_j \\
0, & \text{иначе}
\end{cases}
$$

Тогда loss на паре:

$$
\mathcal{L}_{ij} = - y_{ij} \cdot \log P_{ij} - (1 - y_{ij}) \cdot \log (1 - P_{ij})
$$

Где $P_{ij} = \sigma(s_i - s_j)$ — предсказанная вероятность

Общий loss:

$$
\mathcal{L}_{\text{RankNet}} = \sum_{(i,j) \in \text{Pairs}} \mathcal{L}_{ij}
$$

---

## 3. Обучение и реализация

Для обучения необходимо сформировать все (или подмножество) пар $d_i, d_j$ в пределах одного запроса, где $y_i \ne y_j$.

Варианты моделей:

* Нейросеть $f(q, d)$ (BERT, MLP, CNN, etc.)
* LightGBM / CatBoost — можно использовать вручную сконструированные пары

Пример реализации на PyTorch:

```python
import torch
import torch.nn as nn

class RankNet(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.scorer = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.scorer(x).squeeze(-1)

# Пример batch из пар (d_i, d_j)
x_i, x_j = torch.randn(32, 20), torch.randn(32, 20)
y_ij = torch.ones(32)  # предполагаем, что d_i > d_j

model = RankNet(input_dim=20)
s_i = model(x_i)
s_j = model(x_j)

loss_fn = nn.BCEWithLogitsLoss()
loss = loss_fn(s_i - s_j, y_ij)
loss.backward()
```

---

## 4. Плюсы и минусы RankNet

**Плюсы:**

* Простая и эффективная формулировка задачи
* Не требует полной перестановки кандидатов
* Основан на вероятностной интерпретации
* Может обучаться на имплицитных метках (клики, предпочтения)

**Минусы:**

* Обучение на $O(n^2)$ парах на каждый запрос — неэффективно при большом K
* Не напрямую оптимизирует nDCG или MAP
* Зависит от качества парной генерации (шумные пары ухудшают обучение)

---

## 5. Связь с LambdaRank и LambdaMART

RankNet — основа для **LambdaRank**, который вводит идею масштабирования градиентов по важности влияния на метрику (например, nDCG). Это позволяет совместить pairwise loss с оптимизацией конкретной метрики.

LambdaMART = LambdaRank + GBDT (градиентный бустинг на деревьях)
