# Distillation и Online Fine-Tuning

## 1. Введение

Distillation и online fine-tuning являются ключевыми методами улучшения моделей ранжирования (LTR) в поисковых и рекомендательных системах. Distillation позволяет переносить знания от более сложной и точной модели (например, cross-encoder) к более быстрой модели (bi-encoder, LambdaMART, LightGBM), а online fine-tuning корректирует модель на основе реальных пользовательских сигналов (кликов, dwell time), компенсируя смещения (biases).

---

## 2. Distillation

### 2.1. Общая идея

Distillation — это метод обучения модели-«ученика» (student) на основе сигналов от модели-«учителя» (teacher). Учитель обычно более тяжёлый и точный (cross-encoder), ученик — лёгкий и быстрый (bi-encoder, GBDT, LambdaMART). Цель — сохранить качество учителя при меньшей вычислительной стоимости.

### 2.2. Hard-label distillation

- Учитель (cross-encoder) предсказывает наиболее релевантный документ $d^\*$ для запроса $q$.
- Для обучающего датасета формируются бинарные метки:

$$
y_{q,d} = \begin{cases}
1, & d = \arg\max_{d'} f_{teacher}(q, d') \\
0, & \text{иначе}
\end{cases}
$$

- Ученик обучается в **pairwise** или **pointwise** режиме, имитируя выбор учителя.

**Применение:**

- LightGBM (pointwise/binary classification).
- LambdaMART (pairwise ranking).

**Плюсы:** простота, совместимость с классическими LTR.
**Минусы:** теряется информация о распределении релевантности.

### 2.3. Soft-label distillation

- Вместо выбора одного документа берётся распределение релевантности от учителя:

$$
p_{q,d} = \frac{\exp(f_{teacher}(q, d)/T)}{\sum_{d'} \exp(f_{teacher}(q, d')/T)}
$$

где $T$ — температура сглаживания.

- Функция потерь ученика:

$$
\mathcal{L} = - \sum_{d \in D_q} p_{q,d} \cdot \log p_{student}(q,d)
$$

- Таким образом, ученик учится воспроизводить распределение «relevance distribution».

**Применение:**

- Listwise модели (TF-Ranking, neural LTR).
- LightGBM и LambdaMART через listwise-адаптации.

**Плюсы:**

- Сохраняется относительный порядок документов.
- Лучше подходит для listwise-задач.

**Минусы:**

- Более сложная реализация, нужно хранить распределения.

### 2.4. Практические аспекты

- Используют **distillation cross → bi**: cross-encoder генерирует распределения, bi-encoder или GBDT обучается на них.
- Возможна комбинация hard + soft меток (joint loss).
- Эмпирически soft-distillation даёт прирост в nDCG и MRR, особенно для длинных хвостов.

---

## 3. Online Fine-Tuning

### 3.1. Мотивация

Реальные данные содержат implicit feedback (клики, время просмотра, скроллы). Они:

- масштабируемы (миллионы событий);
- отражают реальные предпочтения;
- но искажены biases (position bias, selection bias).

### 3.2. Использование кликов

- Для пары $(q,d)$ можно ввести вероятность клика:

$$
y_{q,d} = \begin{cases} 1, & \text{если клик был} \\ 0, & \text{иначе} \end{cases}
$$

- Однако простая интерпретация ведёт к смещениям. Нужна коррекция.

### 3.3. Коррекция смещений

#### Position bias

Вероятность клика зависит от позиции документа в списке. Чтобы корректировать:

- **Inverse Propensity Weighting (IPW):**

$$
\hat{L} = \sum_{q,d} \frac{y_{q,d}}{\pi_{pos(d)}} \cdot \ell(f(q,d))
$$

где $\pi_{pos(d)}$ — вероятность наблюдения позиции.

- **Doubly Robust (DLA):** сочетает IPW и модельную оценку пропенсити.

- **SNIPS:** стабилизированная версия IPS с нормализацией:

$$
\hat{L} = \frac{\sum_{q,d} \frac{y_{q,d}}{\pi_{pos(d)}} \cdot \ell(f(q,d))}{\sum_{q,d} \frac{1}{\pi_{pos(d)}}}
$$

#### Selection bias

Некоторые документы никогда не показываются, поэтому они не могут быть выбраны. Для борьбы используют **exploration** (рандомизация выдачи).

### 3.4. Bandit setting

Задача ранжирования может быть рассмотрена как **contextual bandit**:

- Контекст = запрос $q$.
- Действие = выбор документа $d$ для показа.
- Награда = клик/время просмотра.

Модель обучается минимизировать regret:

$$
R_T = \sum_{t=1}^T \Big( r(q_t, d_t^*) - r(q_t, d_t) \Big)
$$

где $d_t^*$ — оптимальный документ.

Используются алгоритмы:

- $\varepsilon$-greedy.
- Thompson Sampling.
- UCB (Upper Confidence Bound).

### 3.5. Пример применения

- Локальная модель (LightGBM/LambdaMART) обучается оффлайн на relevance-джаджментах.
- Онлайн собираются клики.
- Через IPW+SNIPS корректируются метки.
- Модель периодически дообучается (online fine-tuning), снижая gap между оффлайн и онлайн средой.

---

## 4. Выводы

- Distillation позволяет использовать мощные модели как «учителей» для более лёгких ранжировщиков.
- Soft-label distillation чаще превосходит hard-label, особенно для listwise моделей.
- Online fine-tuning позволяет адаптировать модель под реальные данные, но требует коррекции biases.
- В связке distillation + online fine-tuning достигается баланс: высокое качество от учителя и адаптивность от онлайн-данных.

