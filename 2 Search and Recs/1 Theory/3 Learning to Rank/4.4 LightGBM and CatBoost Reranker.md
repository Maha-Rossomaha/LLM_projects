# LightGBM Reranker и CatBoost Reranker

## 1. Теоретическая основа

Обе библиотеки (LightGBM и CatBoost) поддерживают **learning-to-rank (LTR)** как отдельный режим обучения. Их алгоритмы основаны на градиентном бустинге над деревьями решений (GBDT).

- **LightGBM** реализует задачи `lambdarank` и `rank_xendcg`, оптимизируя приближения к метрикам ранжирования (nDCG, MAP).
- **CatBoost** поддерживает режим `YetiRank` и его модификации, также напрямую ориентированные на nDCG.

Основная идея: бустинг по деревьям учится не просто предсказывать скоры, а минимизировать loss, связанный с качеством итогового порядка.

## 2. Математика

1. **LightGBM LambdaRank**

   - Вводятся так называемые *λ-градиенты*, зависящие от пар документов с разными релевантностями.
   - Для пары документов $i, j$ с метками $y_i > y_j$:
     $$
     \lambda_{ij} = \frac{-\sigma}{1 + e^{\sigma (s_i - s_j)}} \cdot |\Delta \text{nDCG}_{ij}|,
     $$
     где $s_i, s_j$ — предсказанные скоры, $\Delta \text{nDCG}_{ij}$ — изменение метрики, если поменять документы местами.
   - Таким образом, LightGBM напрямую оптимизирует приближение nDCG.

2. **CatBoost YetiRank**

   - Строится стохастический loss, который моделирует вероятность правильного порядка на основе softmax:
     $$
     L = - \sum_{q \in Q} \sum_{i \in q} \log \frac{\exp(s_i)}{\sum_{j \in q} \exp(s_j)}.
     $$
   - При этом используются случайные подвыборки пар и списков, чтобы стабилизировать обучение.

## 3. Особенности обучения

- **Фичи:** деревья могут работать как с числовыми, так и с категориальными признаками (CatBoost лучше обрабатывает категории).
- **Группировка:** обучение всегда проводится на уровне запросов (query group). Внутри группы документы сравниваются друг с другом.
- **Метрики:** поддерживаются nDCG\@k, MAP, Precision\@k; оптимизируются их приближения.

### Плюсы и минусы

**LightGBM Reranker:**

- Очень высокая скорость обучения и инференса.
- Эффективен при больших наборах признаков.
- Хорошо оптимизирует nDCG через LambdaRank.

* Хуже работает с категориальными признаками (нужен one-hot/target encoding).

**CatBoost Reranker:**

- Родная поддержка категориальных фичей (упрощает пайплайн).
- Более устойчивая оптимизация на малых данных за счёт стохастичности YetiRank.

* Медленнее LightGBM.
* Требует тонкой настройки параметров при больших датасетах.

## 4. Примеры 

### 4.1. LightGBM Reranker

```python
import lightgbm as lgb

# Признаки документов
X_train = ...   # shape (n_samples, n_features)
y_train = ...   # релевантности
query_train = ...  # размеры групп (число документов на запрос)

params = {
    'objective': 'lambdarank',
    'metric': 'ndcg',
    'ndcg_eval_at': [10],
    'learning_rate': 0.05,
    'num_leaves': 63,
    'min_data_in_leaf': 20
}

dtrain = lgb.Dataset(X_train, y_train, group=query_train)
model = lgb.train(params, dtrain, num_boost_round=500)
```

### 4.2. CatBoost Reranker

```python
from catboost import CatBoostRanker, Pool

X_train = ...
y_train = ...
query_train = ...  # query_id для каждого объекта

train_pool = Pool(X_train, y_train, group_id=query_train)

model = CatBoostRanker(
    iterations=500,
    learning_rate=0.05,
    depth=8,
    loss_function='YetiRank',
    eval_metric='NDCG:top=10',
    random_seed=42,
    verbose=100
)

model.fit(train_pool)
```

## 5. Когда использовать

- LightGBM Reranker — при больших датасетах, числовых фичах и жёстком latency budget.
- CatBoost Reranker — когда важны категориальные фичи и устойчивость на небольших выборках.

Оба метода часто применяются как **baseline rerankers** в индустриальных поисковых и рекомендательных системах.
