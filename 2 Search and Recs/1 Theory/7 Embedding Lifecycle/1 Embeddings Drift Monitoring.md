# Embedding Drift Monitoring

## 1. Зачем мониторить embedding drift

Векторное представление — это **функция**:
$$
f_{\theta}: \text{text} \rightarrow \mathbb R^d,
$$
где $d$ — размерность эмбеддинга, $\theta$ — параметры модели.

Качество поиска / RAG / рекомендаций зависит от **геометрии** пространства:

* какие расстояния между документами;
* какие расстояния между запросами и документами;
* как расположены кластеры тематик.

Дрейф эмбеддингов означает, что:

* либо поменялись **данные** (документы, язык, стиль запросов),
* либо поменялась **модель** ($\theta$),
* либо изменилась **предобработка** (tokenization, truncation, normalization).

Это может:

* испортить Recall@K / nDCG,
* привести к “поиску не по теме”,
* сделать бесполезным исторический опыт модели (обучение, тюнинг).

Поэтому нужен **мониторинг**:

* заметить, что пространство “поехало” (drift),
* понять, где именно (queries vs docs, head vs tail),
* принять решение: переобучать, переиндексировать, подкрутить pipeline.

---

## 2. Общая схема мониторинга

Базовая идея:

1. Выбираем **baseline окно** (например, первый месяц после выката модели).
2. Регулярно (например, раз в день/неделю) берём **current окно** (последние N часов/дней).
3. Для каждого объекта (документ/запрос) считаем несколько **скалярных признаков** из эмбеддинга:
   * норму $|x|_2$,
   * расстояние до центра кластера,
   * косинус до базового центра,
   * проекции на главные компоненты и т.п.
4. Для каждого такого скалярного признака строим **распределения**: baseline vs current.
5. Считаем **PSI** и/или **KL** между этими распределениями.
6. Интерпретируем: “нет дрейфа / умеренный / сильный”, плюс алёрты.

Ключевой момент: **высокая размерность** $\mathbb R^d$ → нельзя просто взять multidimensional KL; мы проецируем пространство на несколько информативных скаляров и мониторим их.

---

## 3. PSI и KL-divergence по embedding distribution

### 3.1. KL-divergence

Для двух распределений $P$ и $Q$ (baseline и current) по некоторому скалярному признаку $z$ (например, норма):

**Дискретный случай (биннинг):**
$$
D_{KL}(P | Q) = \sum_{i=1}^B p_i \log \frac{p_i}{q_i},
$$

где:

* $B$ — число бинов,
* $p_i$ — доля наблюдений в $i$-м бине в baseline,
* $q_i$ — доля наблюдений в $i$-м бине в current.

**Интерпретация:**

* $D_{KL} = 0$ — распределения идентичны.
* Чем больше $D_{KL}$, тем сильнее отличия.
* Несимметричная величина: $D_{KL}(P|Q) \neq D_{KL}(Q|P)$.

**Практические моменты:**

* Нельзя иметь $q_i=0$ и $p_i>0$ (лог бесконечности) → нужен smoothing (добавить маленький $\epsilon$ к каждому $q_i$).
* Для чувствительных систем часто мониторят **обе** величины $D_{KL}(P|Q)$ и $D_{KL}(Q|P)$, либо берут симметризованную версию (например, $0.5(D_{KL}(P|Q)+D_{KL}(Q|P))$).

---

### 3.2. PSI (Population Stability Index)

PSI — эмпирическая метрика из скоринг-систем, заточенная именно под **drift в распределениях признаков**.

Определение (через те же бины):

$$
\text{PSI} = \sum_{i=1}^B (p_i - q_i) \cdot \ln \frac{p_i}{q_i}
$$

Разница с KL:

* PSI больше “наказвает” изменение долей (есть множитель $(p_i-q_i)$).
* Но по сути это очень близкий родственник KL, “заточенный” под мониторинг скор-карт.

**Эмпирические пороги (очень популярны):**

* PSI < 0.1 — изменений почти нет.
* 0.1 ≤ PSI < 0.25 — умеренный дрейф, стоит присмотреться.
* PSI ≥ 0.25 — сильный дрейф, нужно расследование / переобучение.

Эти пороги не математически строгие, но хорошо прижились в индустрии.

---

### 3.3. Как применить PSI/KL к эмбеддингам

Буквально по координатам вектора делать нельзя (слишком шумно и высокоразмерно). Нормальный подход:

1. Выбираем **скалярный признак** $z(x)$ от вектора $x$:

   * $|x|_2$ — норма,
   * $\cos(x, c_0)$ — косинус до baseline-центроида,
   * расстояние $d(x, c_0)$ и т.п.
2. По $z$ строим гистограмму для baseline и current:

   * один и тот же набор бинов (например, по квантилям baseline).
3. Считаем PSI и/или KL между этими гистограммами.

Можно мониторить несколько $z$:

* $z_1(x) = |x|_2$
* $z_2(x) = d(x, c_0)$
* $z_3(x) = \cos(x, c_0)$
* $z_4(x) = \text{proj}_1(x)$ — первая главная компонента.

И уже по ним строить агрегированный отчёт.

---

### 3.4. Мини-пример PSI на Python

```python
import numpy as np

def psi(baseline, current, n_bins=10, eps=1e-6):
    """
    baseline, current: 1D numpy arrays (например, нормы эмбеддингов)
    """
    # общие границы бинов по baseline
    bin_edges = np.quantile(baseline, np.linspace(0, 1, n_bins + 1))
    # чтобы избежать совпадения границ из-за повторов
    bin_edges[0] -= 1e-9
    bin_edges[-1] += 1e-9

    p_counts, _ = np.histogram(baseline, bins=bin_edges)
    q_counts, _ = np.histogram(current, bins=bin_edges)

    p = p_counts / p_counts.sum()
    q = q_counts / q_counts.sum()

    # защита от деления на ноль
    p = np.clip(p, eps, 1)
    q = np.clip(q, eps, 1)

    return np.sum((p - q) * np.log(p / q))
```

---

## 4. Norm shift, cosine-shift, centroid-shift

Это более “геометричные” показатели дрейфа.

Пусть у нас есть эмбеддинги документов или запросов:

* baseline: ${x_i^{(0)}}_{i=1}^{N_0}$
* current: ${x_j^{(1)}}_{j=1}^{N_1}$

### 4.1. Norm shift

Норма:
$$
r = |x|_2.
$$

Идея: следим за распределением $r$.

* $\mu_0 = \mathbb E[r \mid \text{baseline}]$
* $\mu_1 = \mathbb E[r \mid \text{current}]$

Смотрим:

* относительное изменение нормы:
  $$
  \Delta_{\text{norm}} = \frac{\mu_1 - \mu_0}{\mu_0}
  $$
* PSI / KL по распределению норм (как выше).

Интерпретация:

* если модель / preprocessing не менялись, а нормы резко выросли или упали → что-то пошло не так в данных (например, массово появились очень длинные документы, другая языковая статистика).
* при смене модели контролируем, чтобы нормы не “улетели” слишком сильно (иначе косинусные/inner-product расстояния перестали быть сопоставимы).

---

### 4.2. Centroid shift

Центроид baseline:
$$
c_0 = \frac{1}{N_0} \sum_{i=1}^{N_0} x_i^{(0)}.
$$

Центроид current:
$$
c_1 = \frac{1}{N_1} \sum_{j=1}^{N_1} x_j^{(1)}.
$$

Мера дрейфа центра:

* евклидово расстояние:
  $$
  d_c = |c_1 - c_0|_2
  $$
* можно нормализовать на “типичный масштаб”:
  $$
  d_c^{\text{rel}} = \frac{|c_1 - c_0|_2}{\mathbb E[|x - c_0|_2]}
  $$
  — насколько центр сдвинулся относительно типичного разброса точек.

Интерпретация:

* $d_c^{\text{rel}} \ll 1$ — распределение примерно вокруг того же центра;
* $d_c^{\text{rel}} \approx 1$ — центр уехал на величину порядка стандартного отклонения;
* $d_c^{\text{rel}} \gg 1$ — сильный дрейф.

---

### 4.3. Cosine-shift

Косинус до baseline-центроида:

$$
\cos(x, c_0) = \frac{x \cdot c_0}{|x|_2 , |c_0|_2}.
$$

Смотрим распределение этого косинуса:

* baseline: $u^{(0)}_i = \cos(x_i^{(0)}, c_0)$
* current: $u^{(1)}_j = \cos(x_j^{(1)}, c_0)$

Дальше — классика:

* сравниваем $\mathbb E[u^{(0)}]$ и $\mathbb E[u^{(1)}]$,
* считаем PSI / KL по распределениям $u$.

Интерпретация:

* если большинство векторов “смотрит” в другую сторону (средний косинус падает), это явный признак смены домена, языка, или модели.

---

### 4.4. Мини-пример centroid & cosine shift на Python

```python
import numpy as np

def centroid(embs):
    return embs.mean(axis=0)

def centroid_shift(baseline_embs, current_embs):
    c0 = centroid(baseline_embs)
    c1 = centroid(current_embs)
    num = np.linalg.norm(c1 - c0)
    denom = np.mean(np.linalg.norm(baseline_embs - c0, axis=1))
    return num, num / (denom + 1e-9)

def cosine_to_centroid(embs, c0):
    num = embs @ c0
    denom = np.linalg.norm(embs, axis=1) * (np.linalg.norm(c0) + 1e-9)
    return num / (denom + 1e-9)

# пример:
# c_dist, c_rel = centroid_shift(baseline_docs, current_docs)
# cos_base = cosine_to_centroid(baseline_docs, centroid(baseline_docs))
# cos_curr = cosine_to_centroid(current_docs, centroid(baseline_docs))
# psi_cos = psi(cos_base, cos_curr)
```

---

## 5. Drift по запросам и по документам (отдельные пайплайны)

Это критически важно: **распределения эмбеддингов запросов и документов принципиально разные**.

### 5.1. Документы

Особенности:

* как правило, стабильнее по распределению;
* изменения связаны с:
  * входящим контентом (новые источники, новые языки, новые типы документов),
  * правилами предобработки (chunking, очистка, фильтрация).

Мониторим:

* norm shift, centroid shift по эмбеддингам документов;
* PSI/KL по:

  * нормам,
  * расстоянию до baseline-центроида,
  * косинусу до baseline-центроида,
  * возможно, проекциям на PCA-компоненты.

Сегментация документов (опционально):

* по языку,
* по типу контента (FAQ, статьи, тех. документация),
* по времени (старые vs новые документы).

---

### 5.2. Запросы

Особенности:

* распределение запросов может сильно меняться: тренды, сезоны, новые продукты;
* запросы часто гораздо короче, чем документы;
* есть очень сильное различие между head / mid / tail.

Поэтому для **запросов нужно выделять отдельный drift-пайплайн**:

1. Собираем логи запросов за baseline окно.
2. Собираем логи за текущий период.
3. Строим эмбеддинги запросов.
4. Считаем те же метрики:

   * norm shift,
   * centroid shift,
   * cosine-shift до baseline-центроида запросов,
   * PSI/KL по нормам и косинусам.

Важно: если запросы сильно дрейфуют, но документы нет, то:

* ANN индекс документов вроде бы “здоров”,
* но retriever начинает получать **совсем другие запросы** → offline метрики (на старых query-judgements) перестают быть репрезентативны.

---

## 6. Sampling buckets: head / mid / tail

Почему нельзя смотреть на запросы “в целом”:

* head-запросы (часто повторяющиеся) имеют:

  * много статистики,
  * огромный вклад в онлайн метрики (CTR, конверсии).
* tail-запросы (редкие) шумные:

  * могут дрейфовать сильнее,
  * но каждый по отдельности почти ничего не значит,
  * вместе они определяют чувство “насколько поиск умный вообще”.

### 6.1. Как определять head / mid / tail

Один из простых вариантов:

1. Возьмём окно логов (например, месяц baseline).
2. Посчитаем частоту каждого уникального запроса.
3. Отсортируем запросы по убыванию частоты.
4. Определим:

   * head: первые X% запросов по частоте (например, 5–10%),
   * mid: следующие Y% (например, 20–30%),
   * tail: всё остальное.

Можно также использовать квантили по cumulative frequency:

* head — запросы, которые покрывают первые 50% всех обращений;
* mid — следующие 40%;
* tail — последние 10%.

### 6.2. Отдельный drift-мониторинг для каждого bucket’а

Для каждого bucket’а:

* строим эмбеддинги запросов;
* считаем norm shift, cosine-shift, centroid-shift;
* считаем PSI/KL по выбранным скалярам.

Интерпретация:

* **head-дрейф**: очень опасен, почти всегда влияет на core бизнес-метрики.
* **mid-дрейф**: заметен, но чуть менее критичен.
* **tail-дрейф**: нормальное явление (новые темы, модные мемы), важно следить, чтобы система не ломалась, но принимать решение нужно аккуратно.