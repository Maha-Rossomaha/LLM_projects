# Интеграция с LLM и генерация ответа

## 1. Prompt templates для RAG

**Цель:** встроить retrieved-контекст в запрос к LLM так, чтобы модель использовала только релевантные факты и минимизировала галлюцинации.

### Основные элементы шаблона

- **Инструкция модели:** задаёт стиль ответа, тон (например, академический или разговорный), язык, формат (список, абзац, таблица).
- **Контекст:** список top-K документов или их фрагментов. Может вставляться целиком, в сокращённой форме или с выделением ключевых частей.
- **Запрос пользователя:** исходный вопрос или уточнение.
- **Системные указания:** напоминания «используй только предоставленные документы», «давай ссылки на источники».

### Варианты построения шаблонов

- **Vanilla:** простое объединение инструкций, документов и вопроса.
- **Segmented:** документы группируются по темам, каждая группа вводится заголовком.
- **Condensed:** перед вставкой документы сокращаются через summarizer или extractive retriever.
- **Chain-of-thought prompting:** добавление скрытых шагов рассуждений, где модель сначала анализирует документы, а затем формирует итоговый ответ.
- **Citation-enforced:** в шаблон вшивается требование сопровождать каждый факт ссылкой на документ.

### Пример структуры промпта

Ниже показан один из базовых примеров шаблона, который можно использовать в RAG-системах.

**Пример. Чат-бот справочной службы**

```
Ты — виртуальный ассистент компании X. Отвечай строго на основе предоставленных документов, не добавляй собственных фактов. Если ответа нет — напиши, что информации нет.

Документы:
[1] <текст документа 1>
[2] <текст документа 2>
...

Вопрос клиента: <запрос пользователя>

Инструкция: Дай ясный и точный ответ на русском языке. Обязательно укажи номер документа в скобках, на который опираешься.
```

---

## 2. Extractive vs Abstractive synthesis

- **Extractive synthesis**: модель выбирает и пересобирает куски текста напрямую из документов.
  - **Как работает:** LLM получает retrieved-документы и извлекает из них наиболее релевантные предложения или абзацы. Затем эти фрагменты соединяются в ответ.
  - **Техники:**
    - Ranking предложений по релевантности и включение только top-N.
    - Highlighting ключевых фраз (например, через attention-weight).
    - Heuristic rules (поиск предложений с ключевыми словами).
  - **Плюсы:** высокая точность, минимум галлюцинаций, возможность прямого цитирования.
  - **Минусы:** ответы могут быть сухими, фрагментированными, с плохой связностью.
  - **Примеры применения:** FAQ-боты, юридические консультанты, медицинские ассистенты, где важна точность формулировок.
- **Abstractive synthesis**: модель переформулирует информацию своими словами.
  - **Как работает:** LLM читает retrieved-документы и генерирует новый текст, который передаёт ту же суть, но в более плавной и естественной форме.
  - **Техники:**
    - Seq2seq-подходы (encoder-decoder трансформеры) для перефразирования.
    - Summarization моделей (BART, Pegasus, LongT5) для сжатого пересказа.
    - Контролируемое абстрагирование: добавление инструкций «ответь кратко», «дай объяснение простыми словами».
  - **Плюсы:** ответы более естественные, читаемые, связные, лучше подходят для конечного пользователя.
  - **Минусы:** риск галлюцинаций и искажения фактов, возможное появление неточных формулировок.
  - **Примеры применения:** персональные ассистенты, образовательные сервисы, рекомендательные системы, где важен удобочитаемый текст.
- **Комбинированные подходы**:
  - Extract-then-abstract: сначала выбираем ключевые фрагменты, затем LLM делает связный пересказ.
  - Abstract-with-grounding: LLM генерирует текст, но обязана вставлять цитаты или ссылки.

---

## 3. Методы борьбы с галлюцинациями

- **Confidence scoring**:
  - **Суть метода:** измерение того, насколько уверена модель в сгенерированном ответе.
  - **Подходы:**
    - Анализ распределения вероятностей токенов: если вероятность следующего слова низкая, значит уверенность мала.
    - Агрегация по всему ответу: средняя энтропия, min/max confidence.
    - Self-consistency check: сравнение нескольких перегенераций и проверка согласованности.
    - Calibration models: обучение отдельной модели, которая предсказывает вероятность корректности ответа на основе фичей (логиты, длина, использование источников).
  - **Применение:**
    - Снижение доверия к ответам с низкой уверенностью.
    - Вынесение предупреждений пользователю («ответ может быть неточным»).
    - Автоматический триггер на повторную генерацию (reroll) или вызов fallback-стратегии (например, показать только цитаты).
  - **Ограничения:** модель может быть уверена в галлюцинациях, поэтому требуется дополнительная валидация через grounding.
- **Reroll**:
  - **Суть метода:** повторная генерация ответа, если есть подозрение на галлюцинации или низкую уверенность.
  - **Когда применяется:**
    - При низком confidence score.
    - Когда система grounding проверкой выявляет несоответствие между ответом и retrieved-документами.
    - При запросах, где важна высокая точность (например, медицина, юриспруденция).
  - **Стратегии reroll:**
    - *Простая перегенерация:* повторяем генерацию с тем же промптом.
    - *С варьированием температуры:* создаём несколько версий ответа с разной степенью креативности.
    - *Majority voting:* генерируем N вариантов и выбираем наиболее часто встречающиеся факты.
    - *Ensemble reroll:* разные модели (или разные чекпоинты одной модели) генерируют ответы, затем происходит агрегация.
  - **Применение:**
    - Система может автоматически запускать reroll и показывать пользователю только лучший вариант.
    - Можно отображать несколько вариантов для выбора, если важно сохранить прозрачность.
  - **Ограничения:** reroll увеличивает затраты по времени и вычислениям, поэтому важно задавать чёткие правила активации.
- **Self-consistency**:
  - **Суть метода:** генерация нескольких независимых ответов на один и тот же запрос и последующая проверка согласованности фактов.
  - **Как работает:**
    - LLM запускается несколько раз с одинаковым промптом (или с небольшими вариациями, например, разной температурой).
    - Полученные ответы анализируются: какие факты совпадают, а какие различаются.
    - Итоговый ответ формируется на основе большинства (majority voting) или слияния согласованных частей.
  - **Методы агрегации:**
    - *Majority voting* — выбираем факты, встречающиеся в большинстве ответов.
    - *Confidence-weighted voting* — учитываем не только частоту, но и уверенность модели в конкретных токенах.
    - *Graph-based consistency* — строим граф фактов и выделяем наиболее согласованный подграф.
  - **Применение:**
    - Уменьшение риска случайных галлюцинаций.
    - Повышение доверия к ответам при критически важных задачах (медицина, наука).
  - **Ограничения:** требует дополнительных запусков модели → увеличивает latency и стоимость, поэтому применяется в задачах, где критична точность.
- **Groundedness check**:
  - **Суть метода:** верификация того, что все утверждения в ответе LLM опираются на предоставленные документы, а не на «знания из головы» модели.
  - **Как работает:**
    - Для каждого предложения или факта в ответе проверяется, есть ли подтверждающий текст во вставленных retrieved-документах.
    - Можно использовать семантическое сравнение (cosine similarity между эмбеддингами факта и фрагментов документов).
    - Альтернативно — обучить классификатор, который определяет, grounded ли факт.
  - **Техники:**
    - *Sentence-level alignment*: поиск ближайшего по смыслу предложения в retrieved-корпусе.
    - *Token-level attribution* (например, метод RARR) — сопоставление токенов ответа с токенами из источников.
    - *Automatic fact-checking pipelines* — отдельные модели, проверяющие факт против базы знаний.
  - **Применение:**
    - Подсветка частей ответа, не имеющих подтверждения (highlight «галлюцинаций»).
    - Автоматический триггер для reroll или fallback-ответа.
    - Использование в системах, где обязательна проверка ссылками (юриспруденция, медицина, академическая среда).
  - **Ограничения:** требует дополнительных вычислений; семантические методы могут давать ложные срабатывания, поэтому иногда комбинируются несколько техник.

---

## 4. Provenance: ссылки на документы, цитирование, offsets

- **Ссылки на документы**:
  - Добавление номеров или гиперссылок к источникам в тексте ответа.
- **Цитирование**:
  - Прямое выделение фрагментов текста из оригинальных документов.
  - Использование кавычек или специальных блоков.
- **Offsets**:
  - Указание точных позиций (например, начало/конец вхождения в документе).
  - Полезно для воспроизводимости и верификации.
- **Traceability**:
  - Система должна хранить mapping от сгенерированного ответа к исходным данным.
  - Важно для доверия и аудита.
