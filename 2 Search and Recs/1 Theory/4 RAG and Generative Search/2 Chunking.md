# Chunking в RAG

## 1. Зачем нужен chunking

Chunking — это процесс разбиения длинных документов на более короткие отрезки (чанки), пригодные для обработки retriever'ом и/или генератором. Он критически важен в RAG:

* **LLM не может читать весь документ** (ограничено context window).
* **Retriever работает на уровне чанков**, а не целых документов.
* **Точность ответа и groundedness** напрямую зависят от качества чанков.

Цель: разбиение должно сохранять семантику, быть достаточно подробным и масштабируемым.

---

## 2. Базовые методы разбиения

### 2.1 Fixed-size chunking

* Каждый чанк содержит ровно $k$ токенов.
* Простая реализация, широко используется в продакшене (Chroma, Pinecone, Weaviate).
* Проблема: может резать смысловые блоки на части.

Формально, пусть документ представлен токенами $t_1, \dots, t_n$. Тогда:

$$
C_j = (t_{(j-1)k + 1}, \dots, t_{jk})
$$

### 2.2 Sliding window

* Перекрывающиеся чанки: каждое новое окно смещается на $s < k$ токенов.
* Сохраняет больше связности.

$$
C_j = (t_{(j-1)s + 1}, \dots, t_{(j-1)s + k})
$$

Параметры:

* $k$: размер окна
* $s$: шаг
* $o = k - s$: размер overlap

### 2.3 Sentence-based

* Чанк формируется по предложениям (через `nltk.sent_tokenize`).
* Можно комбинировать предложения до достижения порога по длине.

---

## 3. Семантическое разбиение (Semantic Chunking)

### Идея:

Вместо слепого счёта токенов — анализ смысловых границ:

* Абзацы, заголовки, подзаголовки.
* Дискурсивные маркеры (например, "Однако", "Таким образом").
* Падение семантического сходства между соседними фразами.

### Методы:

1. **Heuristic split**: разбиваем по `\n\n`, `#`, `---`, Markdown-разметке.
2. **TextTiling / TextSeg** — анализ семантической когезии.
3. **Embedding similarity**:

   * Каждое предложение кодируется в вектор.
   * Расстояние между соседними векторами.
   * Если семантический сдвиг превышает порог $\delta$, ставим границу.

Формально:

$$
\text{boundary at } i \text{ if } 1 - \cos(\theta) = 1 - \frac{\langle e_i, e_{i+1} \rangle}{\|e_i\|\cdot\|e_{i+1}\|} > \delta
$$

---

## 4. Chunk fusion и агрегация

Иногда приходится объединять маленькие чанки для уменьшения числа retrieval-запросов, повышения связности и экономии места в prompt:

* **Суммаризация соседних чанков**: применяется, когда несколько смежных чанков охватывают одну тему. Можно использовать:

  * **Extractive summarization** (например, TextRank) — выбор ключевых предложений.
  * **Abstractive summarization** (например, через T5, BART) — переформулировка в более краткую форму.
* **Concatenation**: если сумма токенов нескольких соседних чанков не превышает лимит контекста LLM (например, 4096 или 8192 токенов), их можно объединить и подать как один блок. Это уменьшает количество retrieval-запросов и сохраняет более широкий контекст. Особенно полезно при использовании sliding window с overlap, когда чанки частично перекрываются и логически связаны.
* **Hierarchical chunking**: используется для построения иерархии из чанков (чанки второго уровня). Например:

  * Чанки первого уровня — по предложениям или абзацам.
  * Чанки второго уровня — логически связанные группы чанков (например, вся секция документа).
* Также можно использовать стратегию кластеризации чанков на основе сходства их embedding-векторов. Например, если два последовательных чанка имеют высокую косинусную близость (выше порога $au$), они могут быть объединены в один блок. Это позволяет автоматически восстанавливать логические секции документа. Такая стратегия может применяться и на втором уровне иерархии (например, объединение чанков по секциям, главам или смысловым группам).

    * Пример:

    ```LaTeX
    Допустим, у нас есть три чанка:

    C1: "История компании началась в 1998 году, когда..."
    C2: "В 1999 году компания расширила свою деятельность в Европу."
    C3: "Формула энтропии Шеннона выглядит следующим образом: H = -\Sigma p(x)\log p(x)."

  Чанки C1 и C2 связаны по теме (история компании), а C3 — из другой области (информатика). Если по эмбеддингам косинусная близость между C1 и C2 = 0.92 (выше au), а между C2 и C3 = 0.35 (ниже au), то C1 и C2 объединяются в один блок, а C3 остаётся отдельно.  
  Это позволяет автоматически восстанавливать логические секции документа. Такая стратегия может применяться и на втором уровне иерархии (например, объединение чанков по секциям, главам или смысловым группам).
  ```
---

## 5. Анализ trade-off'ов

### Recall vs Precision

* **Recall увеличивается:** за счёт того, что маленькие чанки повышают разрешающую способность retriever'а. То есть даже если нужный факт занимает всего одно-два предложения, он попадает в свой собственный чанк и с большей вероятностью будет извлечён.
* **Precision падает:** текст становится шумнее. Каждая единица становится слишком локальной, чтобы эффективно представлять смысл. Это приводит к следующим эффектам:
  * **Семантическая фрагментация:** часть предложения может попасть в один чанк, а часть — в другой, и оба по отдельности теряют смысл.
  * **Рост числа кандидатов:** у вас резко увеличивается количество чанков $\rightarrow$ retriever вынужден выбирать из гораздо большего пула, в котором появляется много нерелевантных или слабоинформативных блоков.
  * **Фальшивые совпадения:** retriever может выбрать чанк, содержащий частичное совпадение по словам, но не несущий нужного смысла (например, заголовок без текста).
  * **Следствие:** мы чаще получаем похожий, но бесполезный контекст. 

### Recall vs Latency

* **Sliding window с overlap** $\rightarrow$ лучше **coverage**, но дороже по времени и памяти:
  * **Sliding window с overlap** (например, окно в 256 токенов и шаг 128) создаёт перекрывающиеся чанки. Это помогает не потерять смысловые связи между чанками и повышает шанс, что нужный факт попадёт в один из них — таким образом улучшается **coverage** (покрытие релевантных фрагментов текста).
  * Однако это также увеличивает количество чанков, которые нужно хранить, обрабатывать и индексировать, особенно при ANN-индексации или генерации. В результате:
    * Увеличивается время препроцессинга (больше чанков надо создать и вставить в индекс).
    * Увеличивается объём памяти и хранилища (векторов больше, размер индекса больше).
    * Увеличивается время поиска, если retriever не оптимизирован.
* **Семантический split** $\rightarrow$ дороже в препроцессинге, но может улучшить **grounding**:
  * Требует дополнительных вычислений:
    * Расчёт эмбеддингов для предложений или абзацев.
    * Вычисление семантической близости между соседними сегментами.
    * Запуск алгоритмов типа **TextTiling** или кластеризации.
    * Нельзя просто разбить текст по длине — нужно анализировать структуру, что медленнее и требует памяти.
  * **Семантический split** делает чанки смыслово законченными, поэтому retriever и генератор получают более информативные единицы:
    * **Grounding** — это способность LLM опираться на реальные, релевантные источники.
    * Это снижает шанс галлюцинаций и улучшает **faithfulness**.

---

## 6. Chunking-aware retrieval

* Для каждой задачи можно подбирать размер чанка и overlap.
* Для dense retrieval чаще используют 128–256 токенов.
* Для SPLADE или BM25 — 300–500 токенов.
* Некоторые retriever'ы (например, ColBERT) хранят токеновые эмбеддинги и лучше работают с короткими чанками.

---

## 7. Примеры 

### 7.1. Sliding window

```python
def sliding_window(text, tokenizer, window_size=128, step=64):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    for i in range(0, len(tokens) - window_size + 1, step):
        chunk = tokens[i:i+window_size]
        chunks.append(tokenizer.decode(chunk))
    return chunks
```

### 7.2. Semantic chunking через embedding similarity

```python
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer("all-MiniLM-L6-v2")

sentences = text.split(". ")
embeddings = model.encode(sentences)

threshold = 0.7
chunks, current_chunk = [], [sentences[0]]

for i in range(1, len(sentences)):
    sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]
    if sim < threshold:
        chunks.append(". ".join(current_chunk))
        current_chunk = []
    current_chunk.append(sentences[i])

if current_chunk:
    chunks.append(". ".join(current_chunk))
```

---

## 8. Практические рекомендации

* **Для QA по документам**: лучше короткие чанки (100–300 токенов), sliding window.
* **Для генерации длинных ответов**: полезна семантическая агрегация.
* **Для юридических и технических текстов**: sentence или paragraph-based split.
* **Для многодокументных задач**: лучше uniform chunking с post-filtering.