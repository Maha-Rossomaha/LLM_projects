# Инструменты и практики для RAG

## 1. Фреймворки для RAG

- **LangChain**

  - Универсальный фреймворк для построения LLM-приложений.
  - Поддержка цепочек (chains), memory, агентов.
  - Подключение к различным retriever’ам и векторным БД.
  - Богатая экосистема интеграций.

- **LlamaIndex (бывший GPT Index)** 

  - Специализируется на построении индексов и retrieval.
  - Удобен для работы с длинными документами, разбиение на чанки.
  - Поддерживает hybrid search (BM25 + dense).

- **Haystack (deepset)** 

  - Open-source фреймворк для RAG и поиска.
  - Поддерживает пайплайны (retriever → reranker → генератор).
  - Интеграции с Hugging Face, Elasticsearch, FAISS, Milvus.

- **RAGatouille** 

  - Лёгкая Python-библиотека для прототипирования RAG.
  - Упрощает эксперименты с эмбеддерами и dense retrieval.
  - Подходит для небольших проектов и обучения.

---

## 2. API и inference-сервера

- **OpenAI Assistants API** 

  - Высокоуровневый API для построения ассистентов с retrieval.
  - Поддержка документов, memory, функций.
  - Удобен для быстрого прототипирования.

- **vLLM + retriever плагины** 

  - vLLM — высокопроизводительный inference-движок для LLM.
  - Поддерживает плагинный подход для интеграции retrieval.
  - Можно строить собственные кастомные пайплайны (retriever + reranker + генерация).

---

## 3. Сравнительные таблицы

### LangChain vs LlamaIndex vs Haystack vs RAGatouille

| Фреймворк        | Прототипы | Продакшн | Кастомизация | Поддержка БД                            |
| ---------------- | --------- | -------- | ------------ | --------------------------------------- |
| **LangChain**    | ⭐⭐⭐⭐      | ⭐⭐⭐      | Высокая      | Широкая                                 |
| **LlamaIndex**   | ⭐⭐⭐       | ⭐⭐       | Средняя      | Средняя                                 |
| **Haystack**     | ⭐⭐        | ⭐⭐⭐⭐     | Средняя      | Отличная (Elasticsearch, FAISS, Milvus) |
| **RAGatouille**  | ⭐⭐⭐       | ⭐        | Низкая       | Базовая                                 |

### OpenAI Assistants API vs vLLM

| Инструмент                    | Скорость             | Гибкость                 | Стоимость      | Ограничения               |
| ----------------------------- | -------------------- | ------------------------ | -------------- | ------------------------- |
| **OpenAI Assistants API**     | Высокая (облачн.)    | Низкая (фикс. API)       | Подписка       | Нет локального контроля   |
| **vLLM + retriever плагины**  | Очень высокая (лок.) | Высокая (кастом плагины) | Зависит от GPU | Требует DevOps и ресурсов |

---

## 4. Best practices

### Как тестировать качество retriever

- Использовать метрики **MRR (Mean Reciprocal Rank)** и **nDCG (normalized Discounted Cumulative Gain)**.
- Проверять Recall\@K: процент релевантных документов, попавших в top-K.
- Собирать тестовые сеты запросов и ответов (gold standard).

### Когда использовать hybrid search vs dense-only

- **Hybrid search (BM25 + dense)** 
  - Хорош при шумных или коротких запросах.
  - Устойчив к опечаткам и редким словам.
  - Лучше подходит для мультиязычных коллекций.
- **Dense-only** 
  - Эффективен, когда есть качественные эмбеддинги и большой корпус.
  - Лучше работает на длинных, смысловых запросах.

### Советы по выбору векторных БД

- **FAISS**: локальные эксперименты, высокая скорость, но нет встроенной репликации.
- **Qdrant**: Rust-бэкенд, поддержка фильтрации, хорошая интеграция с Python.
- **Milvus**: зрелая экосистема, удобен для продакшна, поддержка шардирования и репликации.

---

## 5. Deployment аспекты

### Как деплоить LangChain/Haystack на Kubernetes

- Создать Docker-образы с зависимостями (Python, модели, retriever).
- Использовать Helm Charts или Kustomize для деплоя.
- Настроить autoscaling (HPA/KEDA) для сервисов retriever и LLM.
- Включить Prometheus/Grafana для мониторинга latency и Recall\@K.

### Интеграция с vLLM для ускорения инференса

- Разворачивать vLLM как inference-сервер (поддержка OpenAI API совместимости).
- Подключать retriever как внешний плагин.
- Настроить batching и KV-cache для увеличения пропускной способности.
- Использовать GPU autoscaling для оптимизации затрат.

