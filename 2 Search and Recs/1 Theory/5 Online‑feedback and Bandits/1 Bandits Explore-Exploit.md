# Explore–Exploit и bandit-алгоритмы

## 1. Введение

В онлайн‑системах поиска и рекомендаций стоит задача баланса **exploit** (показывать известные лучшие результаты) и **explore** (исследовать новые варианты, которые могут оказаться лучше). Это называют **explore–exploit dilemma**.

Пример: если всегда показывать только топ‑документы по текущей модели, система не узнает о новых кандидатах. Если слишком много экспериментировать — пользователи будут недовольны качеством.

Решение дают алгоритмы **multi‑armed bandits** (многорукие бандиты), которые управляют тем, какой вариант выбрать на каждом шаге.

---

## 2. Базовые алгоритмы

### 2.1. ε‑greedy

**Идея:** в большинстве случаев выбираем лучший вариант по текущим данным (exploit), но с небольшой вероятностью ε пробуем случайный вариант (explore).

$$
a_t = \begin{cases}
\arg\max_a \; Q(a), & \text{с вероятностью } 1-\varepsilon, \\
\text{случайный } a, & \text{с вероятностью } \varepsilon
\end{cases}
$$

**Параметр $\varepsilon$:**

* Фиксированный (например, 0.1 → 10% шагов exploration).
* Decaying $\varepsilon$: начинать с высокой вероятности exploration и постепенно уменьшать её (например, $\varepsilon_t = \varepsilon_0 / \sqrt{t}$).

**Плюсы:** простота, быстрая адаптация в начале.  
**Минусы:** при фиксированном $\varepsilon$ всегда тратим часть шагов на exploration; при слишком маленьком — риск застрять на субоптимуме.

**Практика:** ротация 1–2 документов в выдаче; отдельный слот «новые/непопулярные» в рекомендациях.

**Python (Bernoulli‑вознаграждения):**

```python
import random

class EpsilonGreedy:
    def __init__(self, n_arms: int, eps: float = 0.1):
        self.n = n_arms
        self.eps = eps
        self.counts = [0]*n_arms
        self.values = [0.0]*n_arms  # Q(a)

    def select(self) -> int:
        if random.random() < self.eps:
            return random.randrange(self.n)
        return max(range(self.n), key=lambda i: self.values[i])

    def update(self, arm: int, reward: float) -> None:
        self.counts[arm] += 1
        n = self.counts[arm]
        self.values[arm] += (reward - self.values[arm]) / n
```

---

### 2.2. Upper Confidence Bound (UCB1)

**Идея:** добавляем к текущей оценке качества бонус за неопределённость. То есть в алгоритме UCB (Upper Confidence Bound) мы берём не только текущую оценку средней награды для каждой «руки» (например, CTR варианта), но и добавляем к ней дополнительный *бонус*, который зависит от того, насколько мы в этой руке неуверенны.

* Если руку показывали мало раз ($n_i$ маленькое), бонус будет большим — алгоритм даст ей шанс «проявить себя».
* Если руку показывали много раз, бонус почти исчезает, и решение всё больше опирается на реальную среднюю награду.

То есть «бонус за неопределённость» — это способ формально учесть **exploration**: он гарантирует, что даже редко пробуемые варианты будут иногда выбраны, пока мы не накопим достаточно данных о них. В итоге алгоритм балансирует исследование и эксплуатацию автоматически, не задавая вручную вероятность случайного выбора, как в ε-greedy.

$$
\mathrm{UCB_i(t)} =  \hat{\mu}_i(t) + c\sqrt{\frac{\ln t}{n_i(t)}}, \quad \alpha_t=\arg\max_i\mathrm{UCB_i(t)}.
$$

Где $\hat\mu_i$ — средняя награда руки $i$; $n_i$ — сколько раз её показывали; $t$ — номер шага; $c>0$ — коэффициент «смелости».

#### Почему именно такой бонус
 - Член $\sqrt{\ln t / n_i}$ — это концентрационная поправка (по сути, граница уверенности Хёффдинга): чем меньше наблюдений у руки или чем больше пройдено шагов, тем вероятнее, что текущая средняя недооценивает истинную.
 - Логарифм $\ln t$ растёт очень медленно → бонусы постепенно стихают.

#### Инициализация и тай‑брейки
- Руки с $n_i=0$ надо принудительно показать хотя бы 1 раз (или возвращать их до первого показа).
- При равенстве UCB разумно случайно разрывать ничьи.

#### Числовой мини‑пример
Пусть $t=105$, $c=2$.
- Рука A: $\hat\mu_A=0.30$, $n_A=5$;
- Рука B: $\hat\mu_B=0.25$, $n_B=100$.  
  
Тогда 
$$
\mathrm{UCB_A} = 0.30 + 2\sqrt{\frac{\ln 105}{5}}\approx0.30 + 2 \cdot 0.67\approx1.64,
\\
\mathrm{UCB_B} = 0.25 + 2\sqrt{\frac{\ln 105}{10}}\approx0.25 + 2 \cdot 0.21\approx0.67,
$$
**A выигрывает** — у неё больше неопределённости, и алгоритм даёт ей шанс.

**Python:**

```python
import math

class UCB1:
    def __init__(self, n_arms: int, c: float = 2.0):
        self.n = n_arms
        self.c = c
        self.t = 0
        self.counts = [0]*n_arms
        self.values = [0.0]*n_arms

    def select(self) -> int:
        self.t += 1
        for i in range(self.n):
            if self.counts[i] == 0:
                return i
        ucb = [
            self.values[i] + self.c * math.sqrt(math.log(self.t) / self.counts[i])
            for i in range(self.n)
        ]
        return max(range(self.n), key=lambda i: ucb[i])

    def update(self, arm: int, reward: float) -> None:
        self.counts[arm] += 1
        n = self.counts[arm]
        self.values[arm] += (reward - self.values[arm]) / n
```

---

### 2.3. Thompson Sampling (Бета‑Бернулли)

**Идея:** держим апостериорное распределение параметра (напр. CTR) и сэмплируем из него. В этот шаг играем руку с максимальным сэмплом. Это реализует probability matching — рука выбирается пропорционально вероятности быть оптимальной.

Для Бернулли‑вознаграждений поддерживаем параметры Beta($\alpha_i, \beta_i$):

$$
\theta_i \sim \mathrm{Beta}(\alpha_i, \beta_i), \quad a_t = \arg\max_i \theta_i.
$$

Обновление: при клике $\alpha_i \leftarrow \alpha_i+1$, при отсутствии — $\beta_i \leftarrow \beta_i+1$.

### Как это устроено в Beta–Bernoulli

* Модель: клики/не-клики → Бернулли с параметром $\theta_i$ для руки $i$.
* Приор: $\theta_i \sim \mathrm{Beta}(\alpha_{0},\beta_{0})$ (часто берут несмещённый $\alpha_0=\beta_0=1$, «равномерный»).
* Наблюдения: $s_i$ кликов и $f_i$ не-кликов.
* Апостериор:
  $$
  \theta_i \mid\text{данные}\sim \mathrm{Beta}(\alpha_0+s_i,\;\beta_0+f_i).
  $$
  Обозначим $\alpha_i=\alpha_0+s_i$, $\beta_i=\beta_0+f_i$.
* Шаг выбора:

  1. для каждой руки сэмплируем $\tilde\theta_i \sim \mathrm{Beta}(\alpha_i,\beta_i)$,
  2. берём руку $a_t = \arg\max_i \tilde\theta_i$,
  3. наблюдаем клик (1/0), обновляем $\alpha_i,\beta_i$.

Почему это даёт exploration? Потому что дисперсия Beta при малых $\alpha_i,\beta_i$ большая:
$$
\mathrm{Var}(\theta_i)=\frac{\alpha_i\beta_i}{(\alpha_i+\beta_i)^2(\alpha_i+\beta_i+1)}.
$$
У «мало показанной» руки распределение широкое — сэмпл иногда «взлетает», и мы её пробуем. По мере накопления данных распределение сужается, и алгоритм всё чаще эксплуатирует реально лучшие варианты.

### Мини-пример

* Рука A: $s_A=3$ клика из 10 показов → $\alpha_A=1+3=4$, $\beta_A=1+7=8$, $\mathbb{E}[\theta_A]=4/12=0.333$.
* Рука B: $s_B=6$ кликов из 30 → $\alpha_B=7$, $\beta_B=25$, $\mathbb{E}[\theta_B]=7/32\approx0.219$.

Хотя среднее у A выше, важнее то, что у A распределение шире: нередко $\tilde\theta_A$ окажется очень высоким → A выбирается; у B распределение уже — реже «перепрыгивает» конкурентов. Со временем картина стабилизируется.

### Чем TS отличается от ε-greedy и UCB

* **ε-greedy:** exploration задаётся «сверху» параметром $\varepsilon$ и не смотрит на неуверенность.
* **UCB:** добавляет детерминированный бонус $\propto\sqrt{\ln t/n_i}$ к среднему.
* **Thompson:** делает **probability matching** — выбирает руку пропорционально вероятности быть оптимальной. Exploration «встроен» в ширину постериора; ручная настройка почти не нужна.

### Практические тонкости

* **Выбор приора.**

  * Часто $\alpha_0=\beta_0=1$ (равномерный).
  * Если есть бенчмарк CTR $p_0$, можно взять «псевдонаблюдения» $\alpha_0=\kappa p_0$, $\beta_0=\kappa(1-p_0)$ (например, $\kappa=2$–10).
* **Нестационарность.** Делайте discounting: раз в шаг умножать $\alpha_i,\beta_i \leftarrow \gamma\alpha_i,\gamma\beta_i$ с $\gamma\in(0,1)$ или вести скользящее окно — так «память» о старых данных ослабевает.
* **Не только Бернулли.**

  * Для «континуальных» наград с известной дисперсией используют нормальную модель: $\mu_i\mid\text{данные}\sim\mathcal N(\hat\mu_i,\sigma_i^2)$ и сэмплируют среднее.
  * Для контекстных бандитов — апостериор параметров регрессии (например, линейный TS с нормальным приором).

### Почему это работает интуитивно

Фраза «сэмплируем “правду” и берём максимум» означает: мы *ведём себя так, будто истинные CTR сейчас равны сэмплам из их правдоподобных распределений*. Если рука потенциально лучшая, у неё часто будут высокие сэмплы — она будет выбираться; если рука слабая, но изучена плохо, иногда она всё же «выстрелит» — этого достаточно, чтобы не упустить редкие хорошие варианты.

**Python:**

```python
import random
from math import gamma

# Используем random.betavariate для Beta(α, β)
class ThompsonBernoulli:
    def __init__(self, n_arms: int):
        self.n = n_arms
        self.alpha = [1.0]*n_arms
        self.beta = [1.0]*n_arms

    def select(self) -> int:
        samples = [random.betavariate(self.alpha[i], self.beta[i]) for i in range(self.n)]
        return max(range(self.n), key=lambda i: samples[i])

    def update(self, arm: int, reward: int) -> None:
        if reward:
            self.alpha[arm] += 1
        else:
            self.beta[arm] += 1
```

---

### 2.4. Contextual bandits (LinUCB / логистическая регрессия)

В отличие от классических бандитов, **учитывают контекст** $x_t$ (например, признаки пользователя/ситуации). Выбирают действие условно на $x_t$.

**LinUCB (аппроксимация линейной модели):**

$$
\hat{\mu}_i(x) = w_i^\top x, \quad \text{UCB бонус } \alpha\sqrt{x^\top A_i^{-1} x},
\quad a_t = \arg\max_i \left( w_i^\top x_t + \alpha\sqrt{x_t^\top A_i^{-1} x_t} \right).
$$

Где $A_i = D_i^\top D_i + \lambda I$ аккумулирует информацию, $w_i = A_i^{-1} b_i$.

**PyTorch пример:**

```python
import torch

class LinUCB:
    def __init__(self, n_arms: int, d: int, alpha: float = 1.0, lam: float = 1e-3):
        self.n = n_arms
        self.d = d
        self.alpha = alpha
        self.A = [torch.eye(d) * lam for _ in range(n_arms)]
        self.b = [torch.zeros(d) for _ in range(n_arms)]

    def select(self, x: torch.Tensor) -> int:
        scores = []
        for i in range(self.n):
            Ainv = torch.inverse(self.A[i])
            w = Ainv @ self.b[i]
            mean = torch.dot(w, x)
            bonus = self.alpha * torch.sqrt(x @ Ainv @ x)
            scores.append(mean + bonus)
        return int(torch.argmax(torch.tensor(scores)))

    def update(self, arm: int, x: torch.Tensor, reward: float):
        x = x.detach()
        self.A[arm] += torch.outer(x, x)
        self.b[arm] += reward * x
```

---

## 3. Метрики качества

* **Regret** — разница между суммарным выигрышем стратегии и оптимальной:

$$
R_T = \sum_{t=1}^T \left( \mu^* - \mu_{a_t} \right).
$$

* **CTR uplift / Conversion uplift** — относительное изменение целевой метрики.
* **Скорость сходимости** — за сколько шагов стратегия приближается к оптимуму.
* **Exploration cost** — потери из‑за проб.

---

## 4. Практические сценарии (поиск/рекомендации)

* Ротация кандидатов в выдаче, чтобы собирать сигналы о новых документах.
* Exploration‑слоты в ленте (1 позиция под «новинки/длинный хвост»).
* Онлайн‑обновление модели ранжирования на кликах/конверсиях.
* Ограничения: не портить top‑K позиций целиком → мягкий explore в хвосте.

**Трюки продуктивной эксплуатации:**

* Decaying‑$\varepsilon$ или Annealing UCB (менять $c$) по мере накопления данных.
* Discounting/скользящее окно для **non‑stationarity** (старые наблюдения обесцениваются).
* Разделение трафика по сегментам: bandit в каждом крупном сегменте.
* Безопасный explore: guard‑rails на метриках (не опускаться ниже X% CTR).

---

## 5. Edge cases

* **Cold‑start:** нет данных о новых документах/пользователях → агрессивнее explore, приоритизация новинок.
* **High exploration cost:** в критических доменах (медицина/финансы) — строгие лимиты explore.
* **Non‑stationarity:** предпочтения меняются → sliding window, экспоненциальный discounting.
* **Большое число рук:** иерархические бандиты или предварительный отбор кандидатов (candidate generation → bandit rerank).

---

## 6. Мини‑план внедрения

1. Определить «руки» (кандидаты/стратегии ранжирования/рекламные креативы).
2. Выбрать безопасный алгоритм (начать с Thompson/UCB).
3. Настроить сбор онлайновых сигналов (клики, dwell time, конверсии).
4. Ввести guard‑rails (минимальный CTR/конверсию).
5. Использовать decaying/discounting для адаптации.
6. Построить мониторинг regret/uplift и A/B‑страты.

---

## 7. Выводы

Bandit‑алгоритмы позволяют находить баланс между исследованием и эксплуатацией. $\varepsilon$‑greedy подходит для простых сценариев, UCB — когда нужен контроль regret, Thompson Sampling — для эффективного trade‑off без ручной настройки, а contextual bandits делают систему персонализированной, учитывая признаки пользователей и контекста.
