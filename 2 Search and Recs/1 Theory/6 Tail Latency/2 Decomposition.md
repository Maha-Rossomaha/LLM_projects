# Tail Latency Decomposition by stages

## 1. Типичный retrieval‑пайплайн 

Почти любая современная система поиска/рекомендаций/RAG сводится к каскаду.

1. **Preprocess**

   * нормализация запроса,
   * языковая детекция,
   * спеллчек/лемматизация,
   * построение фильтров.

2. **Query embedding**

   * encoder делает вектор для запроса.

3. **ANN retrieval**

   * kNN по векторной базе,
   * выдаёт top‑N кандидатов.

4. **Rerank**

   * более тяжёлая модель сортирует top‑N,
   * иногда ещё добавляет фичи/бизнес‑правила.

5. **Postprocess / LLM**

   * финальные правила,
   * сбор ответа,
   * (в RAG) генерация текста LLM.

Ключ к декомпозиции хвоста — **логировать время на каждом шаге**:

$$
T_{total} = T_{pre} + T_{emb} + T_{ann} + T_{rerank} + T_{post} + T_{net} + T_{queue}
$$

Где `T_net` и `T_queue` могут быть распределены по шагам, но их стоит выделять отдельно.

---

## 2. Per‑stage latency budget

### 2.1. Что такое budget

Latency budget — это «сколько миллисекунд разрешено каждому этапу», чтобы общий SLO не нарушался.

Если твой SLO:

* p95(T_total) ≤ 300 мс,
* p99(T_total) ≤ 600 мс,

то ты разбиваешь это на бюджеты:

* p95(T_emb) ≤ 40 мс,
* p95(T_ann) ≤ 80 мс,
* p95(T_rerank) ≤ 120 мс,
* p95(T_post) ≤ 30 мс,
* остаток на сеть/очереди.

Это не «правда жизни», а **контрольные границы**.

### 2.2. Как задают budgets на практике

1. **Сверху вниз от SLO**

   * берёшь SLO и раскладываешь по этапам пропорционально их ожидаемой цене.

2. **От базы (baseline)**

   * меряешь текущие p95/p99 по этапам,
   * задаёшь budgets как baseline + допустимый рост.

3. **По типам трафика**

   * отдельно бюджеты для head‑запросов и tail‑запросов.

### 2.3. Как budgets помогают ловить хвост

Когда $p99(T_{total})$ вырос, ты смотришь **Δ по этапам**:

$$
\Delta T_{stage} = p99(T_{stage}^{new}) - p99(T_{stage}^{base})
$$

Обычно 1–2 этапа дают 80–90% прироста.

### 2.4. Где обычно сидит хвост

1. **ANN retrieval**

   * hot‑shard,
   * cold cache,
   * высокая вариативность числа visited nodes,
   * IO‑спайки в vector DB.

2. **Rerank**

   * дорогие cross‑encoder’ы,
   * нерегулируемый N кандидатов,
   * batch‑очереди на GPU.

3. **LLM**

   * длина запроса и контекста → вариативность токенов,
   * очереди у провайдера,
   * стриминг/пост‑процессинг.

4. **Сеть/шлюзы**

   * редкие сетевые jitter‑события дают огромный вклад в p99 даже если p50 норм.

Практическая эвристика:

* если хвост «вырос резко» после смены ANN‑параметров → виноват ANN.
* если растёт вместе с длиной запросов/контекста → embedder или LLM.
* если растёт только в часы пик → очереди/трещит шардирование.

---

## 3. Декомпозиция хвоста по этапам

### 3.1. Минимальный набор метрик

Для каждого этапа логируй:

* `lat_p50, lat_p95, lat_p99`,
* `queue_wait` (если есть очередь),
* `error/timeout rate`.

И дополнительные “профильные” признаки:

* длина запроса (tokens/chars),
* N кандидатов,
* shard_id/replica_id,
* наличие/число фильтров,
* размер контекста LLM.

### 3.2. Базовый разбор инцидента

1. $p99(T_{total})$ нарушил SLO.
2. Смотришь p99 по этапам и находишь
   $$
   \arg\max_{stage} \Delta T_{stage}
   $$
3. Дальше смотришь корреляции:

   * с shard_id → дисбаланс,
   * с N кандидатов → тяжёлые запросы,
   * с tokens → embedder/LLM,
   * с queue_wait → backpressure.

Это почти всегда приводит к корню за 5–10 минут, если логи есть.

### 3.3. Что делать, если хвост «размазан»

Если 2–3 этапа выросли понемногу, а total p99 сильно вырос, это обычно означает:

* одномоментные редкие события совпали,
* backpressure породил очередь,
* тяжёлые запросы одновременно тяжёлые для всех стадий.

Тогда ищи:

* пики очередей,
* рост доли тяжёлых запросов,
* сетевые аномалии.

---

## 4. Мульти‑hop архитектуры

### 4.1. Типовая схема хопов

Запрос часто проходит несколько сетевых прыжков:

1. **Gateway / LB**
2. **API‑шлюз / BFF**
3. **Embedder‑сервис**
4. **Vector DB / ANN‑кластер**
5. **Reranker‑сервис**
6. **LLM‑сервис** (если RAG)
7. **Aggregator / Postprocess**

Каждый хоп добавляет:

* своё среднее время,
* и **свою вариативность**.

### 4.2. Почему хопы увеличивают хвост сильнее среднего

Сеть редко «немного медленнее». Она иногда делает:

* ретраи,
* перенаправления,
* очереди в конкретной AZ,
* потери пакетов.

Поэтому:

* p50 хопа может быть 2–3 мс,
* но p99 — 50–100 мс.

И если таких хопов 4–6, хвост суммируется очень неприятно.

### 4.3. Как отделить сетевой хвост от вычислительного

Нужна трассировка, которая логирует:

* время **внутри сервиса**,
* время **в пути**.

Пример:

* `client_send → server_recv` = network_in,
* `server_recv → server_send` = compute,
* `server_send → client_recv` = network_out.

Когда p99 вырос:

* если вырос compute на одном сервисе → оптимизируешь его.
* если вырос network на всех хопах → проблема в сети/балансере/зоне.

### 4.4. Бизнес‑пример

**RAG‑поиск**.
Ты оптимизировал ANN и rerank, но p99 почти не изменился.
Оказалось, что 70% хвоста давал LLM‑провайдер и сетевые ретраи до него.
Без раздельной трассировки это выглядит как «магия».