# Tail Latency in LLM

## 0. Зачем вообще думать про tail в LLM

LLM — самый тяжёлый этап во многих пайплайнах (RAG, чат‑боты, ассистенты).
Даже если retrieval/ANN быстрый, хвост по времени часто определяется **генерацией токенов**.

Важно различать:
- **истинную latency**: сколько времени занял полный ответ,
- **perceived latency**: как быстро пользователь увидел первые осмысленные токены.

Streaming и speculative decoding по-разному влияют на эти две штуки:
- streaming ≈ улучшает perceived latency,
- speculative decoding ≈ режет настоящую latency (включая хвост).

---

## 1. Streaming LLM

### 1.1. Что даёт streaming

**Streaming** — это когда модель выдаёт токены по мере генерации, а не только готовый текст в конце.

Плюсы:
- **первый токен приходит быстро** → пользователь видит, что система «думает», а не зависла;
- UX заметно приятнее: даже если полный ответ занимает 5 секунд, ощущается как «быстрый», потому что уже через 200–500 мс на экране появляются слова.

При этом:
- **истинный хвост** (полное время до конца ответа) может почти не измениться;
- но **perceived tail** (ощущаемая задержка) резко улучшается.

Интуитивно: лучше пусть ответ «печатается» на глазах, чем 3 секунды пустоты и потом бац — целый абзац.

### 1.2. Варианты стриминга

1. **Streaming из LLM‑сервера сразу в клиент**
   - сервер по мере генерации шлёт токены (например, через WebSocket/SSE);
   - клиент сразу рендерит текст;
   - можно показать индикатор «модель пишет…» и подсвечивать уже готовые части.

2. **Streaming + ранняя остановка**

Поскольку мы видим ответ «в реальном времени», можно:
- дать пользователю кнопку *«остановить»* (user cancel);
- остановиться по эвристике:
  - ответ достиг max tokens,
  - достигнут определённый порог длины,
  - получен уже достаточно полный/шаблонный ответ.

Это снижает хвост:
- многие ответы на самом деле не нуждаются в максимально возможном количестве токенов;
- пользователь часто читает первые пару абзацев и уже принимает решение.

### 1.3. Ограничения стриминга

Streaming:
- **не ускоряет внутренний forward‑pass**;
- увеличивает требования к протоколу и клиенту;
- требует продуманного UX (скролл, дозагрузка, обрывы).

Но как инструмент борьбы за UX и perceived latency — это почти must-have.

---

## 2. Спекулятивная генерация (draft + verifier)

### 2.1. Архитектура

Идея **speculative decoding**:
- есть **маленькая, быстрая модель (draft)**,
- есть **большая, точная модель (verifier)**.

Процесс:
1. Draft‑модель генерирует пачку (chunk) токенов вперёд.
2. Verifier‑модель
   - «прокручивает» эти токены у себя,
   - решает, какие из них принять, а какие отклонить/исправить.
3. Если пачка принята → мы быстро продвигаемся на несколько токенов сразу.
4. Если что‑то не подходит → большая модель дозаполняет/корректирует.

Повторяем блоками до окончания ответа.

Грубо говоря, маленькая модель делает черновик, а большая — **редактирует вместо того, чтобы писать всё с нуля**.

### 2.2. Почему это режет tail

1. **Меньше “долгих” форвардов большой модели**
   - часть работы перехватывает маленькая модель;
   - большая модель делает меньше шагов/токенов в «полноценном» режиме.

2. **Continuous batching легче заполняется**
   - verifier может проверять пачки для нескольких запросов в одном батче;
   - draft‑модель дешёвая, поэтому даже при колебаниях числа запросов она не создаёт такой сильный хвост.

3. **Меньшая чувствительность к outlier‑запросам**
   - даже если один запрос «особенно тяжёлый», значимая часть работы на нём сделана дешёвой моделью;
   - большие шаги verifier’а уменьшают влияние одного странного запроса на всю очередь.

Результат: **p50 и p95 улучшаются**, но особенно заметно — **p99/p999**, потому что именно там много «длинных» forward‑последовательностей.

### 2.3. Вариации спекулятивной генерации

1. **Несколько draft‑веток**
   - можно генерировать несколько гипотез (beam) маленькой моделью;
   - verifier проверяет/выбирает лучшую ветку;
   - потенциально быстрее и качественнее, но сложнее в реализации.

2. **Частичная замена / correction**
   - verifier может не просто «принимать/отклонять» весь блок,
   - но исправлять часть токенов внутри блока;
   - это даёт лучшее качество при сохранении выгоды по скорости.

3. **Спекулятивный prefix**
   - небольшая дешёвая модель генерирует только первые N токенов;
   - дальше большая модель продолжает сама.

### 2.4. Практический эффект

В реальных системах speculative decoding даёт:
- ускорение **в 1.5–3 раза** по средней латентности (зависит от моделей и блок‑размера);
- заметное снижение хвоста (p95/p99), особенно на длинных ответах;
- возможность «выкрутить» более качественную большую модель при тех же SLO по времени.

---

## 3. Другие LLM‑оптимизации, влияющие на хвост

### 3.1. Continuous batching и динамический batch size

**Обычный (naive) batching для LLM**:
- Модель взяла батч из N запросов → сделала шаг генерации → пока эти N запросов не догенерят свой токен, новые запросы ждут снаружи, потом следующий батч, и так далее.
- В итоге новый запрос может ждать в очереди, пока «старый батч» не закончится.

**Continuous batching**:
- генерация идёт пошагово: модель каждый шаг генерит по одному токену для текущего набора запросов;
- на каждом шаге мы можем:
  - убрать из батча те запросы, которые уже закончены,
  - добавить новые запросы, которые только что пришли,
- то есть батч — это не “фиксированный набор запросов от начала до конца”, а живая очередь, которая обновляется на каждом шаге.

**Эффект:**
- новые запросы вклиниваются в следующий шаг генерации, а не ждут, пока отработает целый батч;
- GPU всё время загружен (батч почти всегда плотный),
- хвост по очереди (особенно p99) падает, потому что нет паузы «сидим, ждём, пока прошлый батч получит все свои токены».

**Динамический batch size**:
- уменьшаем батч при высоком p99, увеличиваем при низкой загрузке;
- балансируем throughput и latency.

### 3.2. KV‑кэш

**KV‑кэш** (key-value cache) хранит промежуточные представления для уже сгенерированных токенов.

Особенно важен при multi-turn диалоге:
- без кэша каждый новый ответ пересчитывал бы весь контекст с нуля;
- с кэшем считаем только «хвост» (новые токены).

Полезно для хвоста:
- длинные диалоги не мультиплицируют стоимость,
- если KV‑кэш шарится между запросами пользователя, пиковые задержки меньше.

### 3.3. Ограничение max tokens и ранняя остановка

Очень длинные ответы почти всегда формируют хвост.

Стратегии:
- **max tokens** — жёсткий потолок на длину ответа;
- **early stopping по эвристикам**:
  - достигнут «разумный» размер ответа,
  - модель уже дала summary и повторяется;
  - confidence/логика: нет смысла продолжать.

Эффект:
- снижается вероятность отдельных *монструозных* ответов, которые занимают секунды;
- хвост p99/p999 падает, даже если средняя длина ответа немного уменьшается.

---

## 4. Как всё это складывается в картинку борьбы с tail‑latency LLM

1. **Streaming** улучшает UX прямо сейчас:
   - пользователь видит первые токены быстро,
   - perceived latency падает, даже если «истинный» хвост тот же.

2. **Speculative decoding** уменьшает реальное время генерации:
   - меньше шагов большой модели,
   - лучше batching,
   - сильнее всего помогает на длинных ответах и p99.

3. **Continuous batching, KV‑кэш, ограничение max tokens**
   - делают систему менее чувствительной к одиночным тяжёлым запросам,
   - уменьшает хвост без сильной потери качества.

4. **Всё это должно быть связано с SLO**:
   - p95/p99 для LLM‑этапа,
   - ограничения по длине и режимам,
   - отдельные политики для критичных/обычных запросов.