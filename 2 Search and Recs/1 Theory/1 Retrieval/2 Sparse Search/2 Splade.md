# SPLADE / SPLADE++&#x20;

SPLADE (Sparse Lexical and Expansion Model for First-Stage Ranking) и SPLADE++ — это современные sparse‑retrieval модели, которые используют трансформеры для построения разреженных векторов. Они объединяют идеи **lexical search (BM25)** и **dense retrieval**, выдавая sparse‑сигнатуры большого размера, но с высокой семантической насыщенностью.

---

## 1. Архитектура

- **Базовый энкодер:** трансформер (обычно BERT/RoBERTa).
- **Выход:** токеновые логиты → через softmax / ReLU преобразуются в распределение по словарю.
- **Sparsification:** регуляризация L1 + механизм sparsity (маскирование/обрезка маленьких весов).
- **Результат:** документ/запрос представляется как очень разреженный вектор (словарь до 30–300k термов, активных лишь сотни).

SPLADE++ улучшает SPLADE за счёт:

- оптимизированной регуляризации sparsity (дифференцируемая L1/L0),
- более эффективного обучения (distillation от cross‑encoder),
- лучше сбалансированного recall ↔ sparsity.

---

## 2. Отличие от dense retrieval

- **Dense retrieval:** фиксированный размер вектора (например, 768), ANN‑поиск (FAISS/HNSW).
- **SPLADE:** переменный sparse‑вектор, совпадающий со словарём; поиск через inverted index (BM25‑like).
- **Плюсы SPLADE:**
  - Интерпретируемость (каждое измерение = слово).
  - Совместимость с существующей инфраструктурой (Elastic/Lucene).
  - Хорошо ловит редкие слова и OOV.
- **Минусы:**
  - Огромный словарь → большие индексы.
  - Не всегда хорошо работает на перефразах без совпадения термов.

---

## 3. Проблема sparsity vs recall

- **Слишком sparse:** мало активных термов → падает recall.
- **Слишком dense:** много активных термов → индекс раздувается, падает latency.
- Нужен баланс: регуляризация подбирается так, чтобы вектор был разреженным, но содержал достаточно термов для хорошего покрытия.
- В SPLADE++ используется knowledge distillation от cross‑encoder → помогает повысить recall без потери sparsity.

---

## 4. Edge cases

1. **Очень большие словари (300k+):** индексы занимают десятки гигабайт, нужно агрессивное сжатие.
2. **Мультиязычность:** разные языки → разные словари; можно тренировать мультиязычный SPLADE, но sparsity‑регуляризация усложняется.
3. **Редкие термины:** SPLADE способен активировать их через контекст, но при плохой регуляризации они могут исчезнуть.
4. **Длинные документы:** слишком много токенов → больше активных термов → риск разбухания индекса.

---

## 5. Практические советы

- Использовать SPLADE/SPLADE++ как **lexical‑semantic bridge**: они дают прирост по сравнению с чистым BM25.
- Оптимальный вариант — **гибрид**: BM25 + dense + SPLADE.
- Контролировать sparsity через L1‑регуляризацию (обычно 1e‑5 … 1e‑6).
- Для продакшена использовать distillation от cross‑encoder → улучшает качество.
- Индексировать через Lucene/Elastic — SPLADE совместим со стандартными inverted index.

---

## 6. Примеры 

### 6.1. Логиты и sparsification

```python
import torch
import torch.nn.functional as F

# logits: [batch, vocab_size]
logits = torch.randn(2, 30000)

# sparsification через ReLU и L1 регуляризацию
sparse_vec = F.relu(logits)
sparse_vec = torch.where(sparse_vec > 0.01, sparse_vec, torch.zeros_like(sparse_vec))

print("Активных термов:", (sparse_vec > 0).sum(dim=1))
```

### 6.2. Distillation от cross‑encoder

```python
# teacher_scores: [batch, docs]
# student_scores (SPLADE): [batch, docs]

loss = F.mse_loss(student_scores, teacher_scores)
loss.backward()
```

---

## 7. Чеклист тюнинга

- Подобрать λ для L1‑регуляризации sparsity.
- Проверить recall\@K на dev‑сете при разной sparsity.
- Использовать distillation от cross‑encoder.
- Следить за размером индекса (гигабайты на миллион документов).
- Для мультиязычных корпусов рассмотреть mSPLADE или мультиязычный словарь.

