# Inverted Index


## 1. Базовые понятия и терминология

- **Term (терм):** нормализованная единица текста (токен) после анализа.
- **Dictionary (словарь):** упорядоченный набор всех термов коллекции.
- **Posting list (список вхождений):** отсортированный список документов, содержащих терм, плюс связанная информация.
- **Posting (вхождение):** запись вида `(docID, tf, positions, offsets, payload)`; подмножества выбираются в зависимости от настроек.
- **DAAT/TAAT:** стратегии вычисления: *Document‑At‑A‑Time* (сразу по всем термам идём по документам) vs *Term‑At‑A‑Time* (сканируем термы по очереди и накапливаем скоры).

---

## 2. Классическая структура индекса

### 2.1. Словарь терминов

- Хранится в виде компактной структуры (часто FST/Block‑Tree Terms).
- Для каждого терма хранятся: ссылка на posting list, статистики `df(t)`, `cf(t)`, иногда мини‑максимумы для прунинга.

### 2.2. Списки вхождений (posting lists)

Каждый терм `t` имеет список вхождений, отсортированный по возрастанию `docID`:

- **docIDs:** отсортированные, кодируются через d‑gap (разности соседних ID).
- **tf(t,d):** частота терма в документе (может не храниться при некоторых сценариях).
- **positions:** отсортированные позиции терма в документе; для фразовых запросов.
- **offsets:** смещения для подсветки (highlighting).
- **payloads:** дополнительные байты на вхождение (например, boost).

### 2.3. Индексация полей

- Несколько полей (title, body, tags) индексируются отдельно; возможны field‑specific анализаторы и веса.
- Нормы поля (**norms**) используются в BM25 для нормализации длины: хранятся как один байт на документ на поле.

---

## 3. Построение индекса (pipeline)

1. **Анализ (analyzers):** токенизация, нижний регистр, стемминг/лемматизация, фильтры стоп‑слов, нормализация Unicode.
2. **Инвертирование:** собираем `term → [(docID, positions…)]` в памяти (RAM buffer), периодически «сбрасываем» сегменты на диск.
3. **Сегменты (Lucene‑подобно):** индекс состоит из множества неизменяемых сегментов; новые документы добавляются как новые сегменты.
4. **Слияние сегментов (merge):** фоновая компактизация, пересортировка и перекодирование posting lists; уменьшает фрагментацию и улучшает локальность.
5. **Обновления/удаления:** помечаются как tombstones; физически удаляются при merge.

---

## 4. Оптимизации хранения (compression)

### 4.1. Кодирование docID и позиций

- **d‑gap:** храним разности `docID[i] − docID[i−1]` и `pos[i] − pos[i−1]`.
- **Variable‑Byte (VByte), VarInt:** переменная длина числа, дешёвая декодировка.
- **Gamma/Golomb:** сильно сжимают маленькие числа, дороже по CPU.
- **Frame‑of‑Reference (FOR) + бит‑пэкинг (BP128/SimdBP128):** блоки фиксированного размера (обычно 128) с хранением минимального и ширины в битах.
- **PForDelta/SIMD‑PFor:** FOR с исключениями (outliers) для больших скачков.

### 4.2. Блочное хранение (block posting)

- Разбивка posting list на блоки по `B` элементов (например, 128/256).
- Для каждого блока хранятся метаданные: `maxTF`, `blockMaxScore`, оффсеты для быстрого перехода и прунинга.

### 4.3. Словарь терминов

- **Block‑Tree Terms / FST:** компактное хранение префиксов; быстрый seek по термам.
- **Term indexing:** указатели на чанки posting lists, чтобы не читать весь массив подряд.

---

## 5. Оптимизации поиска

### 5.1. Skip lists

- Вставляем «скачки» каждые `√n` или по размеру блока: `(docID_skip, offset)`.
- Позволяет перескакивать большие куски posting list при пересечении AND/phrase.

### 5.2. MaxScore и WAND (Weak‑AND)

- **Идея:** для каждого терма считаем верхние оценки вклада в скор (`UB_t`). Если текущий частичный скор + сумма `UB_t` по непосещённым термам < текущего `k‑го` результата, документ можно **отсечь**.
- **WAND:** хранит курсоры по спискам и двигает их к «окну» кандидатов, опираясь на верхние границы.
- **Block‑Max WAND (BMW):** использует `blockMaxScore` — верхнюю границу вклада внутри блока; даёт существенно больше прунинга при длинных листах.

### 5.3. Impact‑ordered postings

- Списки упорядочены по «импакту» (квантованному вкладу терма) и разбиты на уровни; позволяет быстро находить документы с высоким суммарным скором.

### 5.4. Tiered/Hybrid

- Короткие posting lists и «горячие» термы держатся в памяти, длинные/холодные — на диске; уменьшает p99.

---

## 6. Краевые случаи (edge cases)

### 6.1. Редкие термы

- Плюс: очень короткие posting lists → быстрый точный матч.
- Минус: переусиление в ранжировании (слишком высокий IDF). Решение: калибровать веса/использовать BM25+/норму‑порог `min_should_match`.

### 6.2. Stop‑words

- Очень частые термы ("и", "the"). В классическом BM25 они почти не влияют (малый IDF), но **нарушают фразовый поиск** при полном удалении.
- Безопасные стратегии:
  - **Query‑time pruning:** удалять стоп‑слова из запроса, но **оставлять в индексе позиции** для корректных фраз.
  - **Index‑time pruning c позициями:** если удаляем — сохранять «дыры»/offests для фразовых запросов (дороже в реализации).
  - **Нулевой вес:** индексируем, но исключаем из счёта (вес≈0), чтобы не ломать фразы.

### 6.3. Длинные документы

- Проблемы: огромные `tf`, длинные списки позиций, большое число уникальных термов.
- Меры:
  - Включать **norms** и подбирать `b` в BM25L (смягчение штрафа длины).
  - Ограничивать `max_positions`/`max_term_freq` на документ.
  - Чанкинг длинных полей (split + overlap), если нужна фразовая точность.

---

## 7. Распределённая архитектура (Elasticsearch/OpenSearch)

### 7.1. Логическая модель

- **Index → shards → segments**: индекс делится на `P` primary‑шардов; у каждого `R` реплик.
- Каждый шард — независимый Lucene‑индекс с собственными сегментами и кешами.

### 7.2. Запись

- Документы пишутся в primary‑шард (по роутингу `_id`/custom), реплицируются на `R` шардов.
- **Refresh** (по умолчанию \~1с): делает новые сегменты видимыми для поиска.
- **Flush/Translog:** надёжность и восстановление после сбоев.

### 7.3. Поиск (fan‑out / gather)

- Координатор рассылает запрос на все релевантные шарды, каждый возвращает локальный `top‑k'` (`k' ≥ k`, обычно `k' = α·k`, `α≈2..5`).
- Координатор выполняет глобальный `k‑way merge` по скору.
- Внутри шарда применяются WAND/BMW/skip; кэшируются структуры запросов.

### 7.4. Кеши и данные полей

- **Query cache / request cache**: повторно используемые части запроса.
- **Doc Values**: колоночные данные для сортировки/агрегаций; отдельны от инвертированного индекса.

### 7.5. Масштабирование

- **Shard rebalancing, allocation awareness**, hot/warm/cold узлы.
- **Routing** для многотенантности (index per tenant / shard per tenant).

---

## 8. Pruning stop‑words (подробно)

### 8.1. Подходы

- **Static stoplist:** фиксированный набор термов на язык/домен. Риск: потеря качества фраз/проксимити.
- **Dynamic stoplist:** вычисляем по `df(t)`/`cf(t)` (например, top‑x% самых частых термов) — адаптируется к корпусу.
- **Per‑query pruning:** удаляем часто‑встречающиеся термы именно в текущем запросе, если они не несут сигнала.

### 8.2. Рекомендации

- Для фразовых/позиционных запросов не удалять полностью: хранить позиции/заглушки.
- Оставлять стоп‑слова в индексе, но с нулевым весом в BM25 (без влияния на скор), чтобы не рушить семантические конструкции.
- Обязательно валидировать на наборах фраз и коротких запросов ("to be or not to be").

---

## 9. Практические рекомендации и чеклист

- **Анализаторы:** под язык/домен; лемматизация для славянских языков, стемминг для германских — тестировать обе опции.
- **Нормы:** включать для текстовых полей (BM25). Выключать для `keyword`.
- **Компрессия:** BP128/SIMD‑PFor для длинных списков, VByte для коротких.
- **Блоки:** `B=128/256`, хранить `blockMaxScore` для BMW.
- **Прунинг:** включить WAND/BMW; подобрать `α` для `k'` на шард (обычно 2–5).
- **Стоп‑слова:** использовать per‑query pruning; избегать полного удаления, если важны фразы.
- **Длинные поля:** рассмотреть чанкинг + BM25L.
- **Мердж‑политика:** tiered merge policy; ограничить число сегментов per shard для стабильной p95/p99.

---

## 10. Пример кода

\
Инвертирование с позициями и поиск фразы

```python
from collections import defaultdict

def analyze(text):
    # очень упрощённый анализатор: lowercase + split
    return [t for t in text.lower().split() if t]

# Построение индекса с позициями
corpus = {
    1: "the quick brown fox jumps over the lazy dog",
    2: "the quick blue fox jumps high",
    3: "blue whale and quick fish"
}

postings = defaultdict(lambda: defaultdict(list))  # term -> docID -> [positions]
for doc_id, text in corpus.items():
    for pos, term in enumerate(analyze(text)):
        postings[term][doc_id].append(pos)

# Фразовый поиск: term1 term2 (соседние позиции)
def phrase_search(term1, term2):
    res = []
    docs1 = set(postings.get(term1, {}))
    docs2 = set(postings.get(term2, {}))
    for d in docs1 & docs2:
        p1 = postings[term1][d]
        p2 = postings[term2][d]
        i = j = 0
        while i < len(p1) and j < len(p2):
            if p2[j] - p1[i] == 1:
                res.append(d); break
            if p1[i] < p2[j]-1: i += 1
            else: j += 1
    return sorted(set(res))

print(phrase_search("quick", "brown"))  # [1]
```

---

## 11. Метрики качества и производительности

- **Качество:** Recall\@K, nDCG\@K, точность фразовых запросов.
- **Производительность:** p50/p95/p99 latency, пропускная способность QPS, размер индекса, скорость refresh/merge.
- **Надёжность:** время восстановления, устойчивость к hotspot шардов.

---

## 12. Краткий глоссарий

- **df(t):** document frequency — число документов, где встречается `t`.
- **cf(t):** collection frequency — общее число вхождений `t` в коллекции.
- **Posting:** запись о присутствии терма в документе.
- **Skip list:** структура «перепрыгивания» блоков в posting list.
- **WAND/BMW:** алгоритмы раннего прунинга по верхним границам вклада.
- **Norms:** нормализации для полей (длина документа и т.п.).

