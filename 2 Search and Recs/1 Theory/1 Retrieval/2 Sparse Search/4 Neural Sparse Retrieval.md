# Neural Sparse Retrieval — подробный разбор (DeepImpact, uniCOIL и др.)

Нейросетевые sparse-подходы — это семейство методов, которые используют трансформеры для формирования **разреженных представлений документов и запросов**. В отличие от классического BM25 и TF-IDF, веса термов вычисляются нейросетью. Эти методы стоят на промежуточной позиции между SPLADE и dense retrieval.

---

## 1. Архитектуры

### DeepImpact

- **Идея:** вместо бинарного присутствия терма или фиксированного IDF-веса документ получает непрерывные веса для термов, рассчитанные нейросетью. Это позволяет тонко различать значимость разных слов.
- **Модель:** BERT-энкодер, дообученный на query–document парах, выдаёт представления токенов; через линейный слой прогнозируются числовые веса термов.
- **Особенность:** важные слова усиливаются, второстепенные получают близкие к нулю веса; тем самым улучшается дискриминация в рамках inverted index.
- **Обучение:** часто используется distillation от более мощного cross-encoder, чтобы приблизить веса к релевантностным скорам.
- **Инфраструктура:** индекс формируется как стандартный inverted index, но вместо TF/IDF хранит веса из модели.

### uniCOIL

- **Идея:** «contextualized IDF» — каждый терм в документе получает контекстно-зависимый вес, зависящий от окружения слов.

- **Модель:** BERT-энкодер с классификационной головой предсказывает значимость каждого токена документа.

- **Особенность:** один и тот же терм может иметь разные веса в разных документах или даже в разных позициях, что повышает чувствительность к контексту.

- **Обучение:** оптимизируется на парных данных (query–relevant/irrelevant doc), часто также с дистилляцией.

- **Индекс:** полностью совместим со стандартными системами (Lucene/Anserini), хранит веса как расширение inverted index.

- **Идея:** «contextualized IDF». Каждое вхождение терма получает контекстно-зависимый вес.

- **Модель:** BERT обучается предсказывать значимость терма в документе с учётом контекста.

- **Особенность:** один и тот же терм может иметь разные веса в разных документах.

- **Индекс:** совместим со стандартным Lucene/Anserini.

### Другие модели

- **COIL:** query-term ↔ doc-term взаимодействия, но хранение в sparse-формате.
- **ColBERT-Sparse:** модификация ColBERT с ограничением на количество активных термов.
- **Distill-Sparse:** дистилляция от cross-encoder в sparse-пространство.

---

## 2. Отличие от SPLADE

- **SPLADE:** генерирует огромный словарь (30–300k термов) и активирует множество токенов → сильное расширение документа.
- **DeepImpact/uniCOIL:** работают только с термами, реально присутствующими в тексте, но меняют их веса.
- **Сравнение:**
  - SPLADE = query/document expansion (генерация новых признаков).
  - DeepImpact/uniCOIL = reweighting (переоценка существующих термов).
- **Инфраструктура:** uniCOIL и DeepImpact проще интегрировать, так как используют тот же inverted index.

---

## 3. Когда применять

- **DeepImpact:**
  - Когда нужно повысить качество поиска без изменения словаря.
  - Хорош для доменов с устоявшейся терминологией.
- **uniCOIL:**
  - Когда важен контекст слов (например, «Apple» = фрукт или компания).
  - Полезен для коллекций с высокой полисемией.
- **SPLADE:**
  - Лучше при необходимости генерации новых формулировок (expansion).
  - Но требует больше памяти.

---

## 4. Проблемы и edge cases

1. **Рост индекса:** хотя меньше, чем у SPLADE, веса термов увеличивают размер inverted index.
2. **Шумные веса:** при плохом обучении нейросеть может присвоить высоким весам нерелевантные слова.
3. **Мультиязычие:** для разных языков нужны отдельные модели.
4. **Длинные документы:** BERT ограничен 512 токенами → требуется чанкинг.

---

## 5. Практические советы

- Для внедрения в продакшн лучше начинать с **uniCOIL**: простая интеграция и хорошие результаты.
- DeepImpact полезен для доменов с короткими документами (новости, product descriptions).
- Для больших коллекций использовать гибрид: BM25 + uniCOIL.
- Для мультиязычия — mBERT или XLM-R в качестве энкодера.

---

## 6. Примеры (псевдокод)

### 6.1. Генерация весов термов (uniCOIL)

```python
from transformers import AutoTokenizer, AutoModel
import torch

tok = AutoTokenizer.from_pretrained("castorini/unicoil")
model = AutoModel.from_pretrained("castorini/unicoil")

text = "Apple releases new iPhone"
inputs = tok(text, return_tensors="pt")
outputs = model(**inputs).last_hidden_state  # [seq_len, hidden]

# линейный слой для предсказания веса терма
term_weights = torch.sigmoid(outputs @ torch.randn(outputs.size(-1), 1))
```

### 6.2. Интеграция в inverted index

```python
# Для каждого терма документа сохраняем его вес
index[doc_id][term] = weight
```

---

## 7. Чеклист тюнинга

- Подобрать функцию активации (sigmoid/softmax) для весов.
- Контролировать sparsity (обнулять низкие веса).
- Валидировать recall/precision против BM25 и SPLADE.
- Для чанкинга документов использовать overlap = 128 токенов.
- Мониторить рост индекса при масштабировании.

