# Edge cases в sparse‑поиске

## 1. Почему это важно

Sparse‑поиск опирается на пересечение термов. В реальных корпусах распределение слов и документов **несбалансировано**: встречаются редкие и сверхчастотные термы, документы сильно отличаются по длине, данные многоязычны и динамичны. Без специальных приёмов это ведёт к падению recall/nDCG, tail‑latency и росту индекса.

---

## 2. Редкие термины (Rare Terms)

**Симптомы**

- Очень высокий $IDF(t)$ «перебивает» остальные сигналы → всплывают документы с единичным, но нерелевантным совпадением.
- Высокая чувствительность к опечаткам и морфологии.

**Риски**

- Нестабильность ранжирования для однословных запросов.
- Утечка качества при появлении новых редких терминов (сленг, неологизмы).

**Митигаторы**

- **IDF‑clipping:** ограничить максимум $IDF_{max}$.
- **BM25+:** положить «базовый вклад» через $\delta$ и не завышать редкие термы в коротких документах.
- **min\_should\_match:** требовать минимум совпадений термов в длинных запросах.
- **Query expansion:** словари синонимов, DocT5Query (псевдозапросы), char n‑граммы для робастности к опечаткам.
- **Hybrid fusion:** смешивать с dense‑сигналом, чтобы редкий терм не доминировал.

**Мини‑пример (IDF‑clipping + min\_should\_match)**

```python
import math

def idf(df, N, idf_max=12.0):
    val = math.log((N - df + 0.5) / (df + 0.5))
    return min(val, idf_max)

# heurstic: требуем минимум m совпадений для длинных запросов
def min_should_match(q_terms):
    L = len(q_terms)
    if L <= 2: 
        return L  # все термы
    if L <= 5: 
        return L - 1
    return int(0.8 * L)
```

---

## 3. Очень длинные документы (Long Docs)

**Симптомы**

- Большие $tf$ и длинные списки позиций → перегруз индекса и bias в сторону длинных документов.
- Падение точности фразовых запросов из‑за «шума».

**Митигаторы**

- **BM25L:** смягчить штраф за длину; подобрать $b$ и $\epsilon$.
- **Чанкинг полей:** делить body на куски фиксированной длины с overlap; ранжировать по лучшему чанку, а не по сумме.
- **Ограничения:** `max_term_freq`, `max_positions` на документ.
- **Полевая модель:** выносить «title/abstract» в отдельные поля с повышенным весом.

**Мини‑пример (чанкинг + best‑chunk)**

```python
def chunks(tokens, size=256, overlap=64):
    i = 0
    while i < len(tokens):
        yield tokens[i:i+size]
        i += size - overlap

# скор документа как максимум по чанкам
scores = [max(score_chunk(c) for c in chunks(doc_tokens)) for doc_tokens in corpus]
```

---

## 4. Stop‑words и High‑Frequency Bias

**Симптомы**

- Сверхчастые слова не несут смысла, но:
  - при полном удалении ломается фразовый/проксимити‑поиск,
  - длинные запросы «разбавляются» шумом.

**Митигаторы**

- **Query‑time pruning:** удалять стоп‑слова из запроса, **оставляя их в индексе с позициями**.
- **Нулевые веса:** индексировать, но давать вес $\approx 0$ в скоре.
- **Dynamic stoplist:** формировать стоп‑список по порогу $df$ или квантилю частот.
- **Block‑max WAND / impact‑ordered:** агрессивный прунинг по верхним границам вкладов для head‑термов.

**Мини‑пример (динамический стоп‑лист)**

```python
import numpy as np

def dynamic_stoplist(df_dict, top_pct=0.01):
    # df_dict: term -> df
    dfs = np.array(list(df_dict.values()))
    thr = np.quantile(dfs, 1 - top_pct)  # верхний 1%
    return {t for t, df in df_dict.items() if df >= thr}
```

---

## 5. Мультиязычие (Multilingual)

**Симптомы**

- Разные языки → разные словари; BM25 не «склеивает» языки.
- Морфология/склонения → взрыв вариантов термов.

**Митигаторы**

- **Language detection → per‑language analyzers:** отдельные индексы/поля с языковыми анализаторами.
- **Лемматизация/стемминг по языку:** уменьшить вариативность форм.
- **Транслитерация/нормализация Unicode:** для смешанного письма.
- **Hybrid:** мультиязычный dense‑сигнал (mBERT/e5‑m) + BM25.
- **mSPLADE/uniCOIL‑модели:** как нейроспарс‑надстройка над BM25.

**Мини‑пример (роутинг по языку)**

```python
def route_by_lang(doc):
    lang = detect_lang(doc)  # fasttext/char-ngrams heuristic
    index = f"docs_{lang}"
    add_to_index(index, doc)
```

---

## 6. Drift корпуса (новые термины, сленг)

**Симптомы**

- Появление новых слов/значений; сдвиг $df(t)$ и распределений длин.
- Падение recall для свежих тем.

**Мониторинг**

- **Term‑DF shift:** отслеживать долю термов с $|df_{new} - df_{old}|$ выше порога.
- **PSI/KL по гистограммам df/длин документов.**
- **New‑term rate:** доля документов с $\geq 1$ термом, не встречавшимся ранее.

**Митигаторы**

- **Переиндексация словаря:** пересчитать IDF после значимых сдвигов.
- **DocT5Query / query expansion:** покрыть новые формулировки.
- **SPLADE:** контекстуально «поднимает» свежие термы.
- **Shadow index → alias switch:** безопасный кат новых настроек анализа/индекса.

**Мини‑пример (PSI на df‑распределении)**

```python
import numpy as np

def psi(old, new, bins=20):
    h1, b = np.histogram(old, bins=bins, density=True)
    h2, _ = np.histogram(new, bins=b, density=True)
    h1 += 1e-9
    h2 += 1e-9
    return np.sum((h1 - h2) * np.log(h1 / h2))

psi_val = psi(old_df_values, new_df_values)
```

---

## 7. Дополнительные edge cases

- **Короткие запросы (1–2 терма):** высокое влияние IDF → используйте hybrid и IDF‑clipping.
- **Сверхдлинные запросы:** включайте `min_should_match`, нормализуйте веса по длине запроса.
- **Числа/даты/ID:** отдельные поля с типами, нормализованные форматы, токенизация по правилам.
- **Опечатки/вариативность письма:** char n‑граммы, фонетические ключи, fuzzy‑match только на узком пред‑кандидатном множестве.
- **Пунктуация/эмодзи:** явные фильтры/нормализация; хранить offsets для корректного highlighting.
- **Фразовые/проксимити‑запросы:** не удаляйте стоп‑позиции; храните positions.
- **Дедупликация:** MinHash/SimHash снижает индекс и шумовые совпадения.

---

## 8. Операционные риски и производительность

- **Index bloat:** контролировать рост через чанкинг, pruning, компрессию (BP128/PForDelta).
- **Shard hotspots:** равномерный роутинг, ребаланс, кэш «горячих» posting lists.
- **Tail‑latency:** BMW/WAND, tiered storage, кэш запросов, ограничение `k'` per shard.
- **Согласованность:** refresh/merge тайминги, правила удаления/обновления.

## 9. Чеклист тюнинга

- Настроить анализаторы по языку; включить лемматизацию/стемминг.
- Включить positions для фразовых запросов; не удалять stop‑слова из индекса целиком.
- Ввести IDF‑clipping и `min_should_match` для длинных запросов.
- Для длинных документов — чанкинг + BM25L; ограничить `max_term_freq`.
- Включить BMW/WAND; хранить `blockMaxScore`.
- Мониторить drift (PSI/KL), new‑term rate; применять shadow‑reindex.
- Использовать hybrid fusion как страховку против редких/ошибочных термов.

---

## 10. Примеры

### 10.1. Нормализация скоринга для гибрида

```python
import numpy as np

def z_norm(x):
    x = np.asarray(x)
    return (x - x.mean()) / (x.std() + 1e-9)

def fused_score(bm25_scores, dense_scores, w_lex=0.6, w_dense=0.4):
    return w_lex * z_norm(bm25_scores) + w_dense * z_norm(dense_scores)
```

### 10.2. Ограничение tf и позиций на документ

```python
def cap_tf(tf, cap=20):
    return min(tf, cap)

MAX_POSITIONS = 10000  # на поле/документ
```

### 10.3. Быстрая эвристика min\_should\_match

```python
def msm(L):
    if L <= 2: 
        return L
    if L <= 5: 
        return L - 1
    return int(0.8 * L)
```

---

## 11. Метрики и мониторинг

- **Качество:** Recall\@K, nDCG\@K, Precision\@K, точность фраз.
- **Распределения:** гистограммы $df(t)$, длин документов, доля стоп‑слов в запросах.
- **Drift:** PSI/KL; new‑term rate; распределение IDF до/после.
- **Производительность:** p50/p95/p99 latency, QPS, размер индекса, частота merge/refresh.
