# Retrieval на очень малых корпусах

Когда мы работаем с retrieval-системой на малом корпусе (от 10 до 1000 документов), привычные стратегии начинают давать сбои:

- retriever переобучается на специфике корпуса,
- dense-модели не могут обучиться или переобобщают,
- частотные признаки (TF-IDF) теряют смысл,
- высокая чувствительность к шуму.

---

## 1. Проблемы малых корпусов

### 1.1 Отсутствие статистики

- TF-IDF не работает: редкие слова могут стать слишком важными.
- BM25 теряет смысл, если документы <100.
- Dense retrievers (e.g. Contriever, GTR) не обучаются нормально.

### 1.2 Переобучение

- Fine-tune retriever легко переобучается на тривиальные паттерны.
- Модель учит запоминание, а не семантику.

### 1.3 Семплирование негативов не работает

- Негативы слишком похожи на позитивы.
- Нет "достаточно трудных" негативов для тренировки.

### 1.4 Высокая чувствительность к шуму

- Даже один мусорный документ может искажать ранжирование.

---

## 2. Подходы к улучшению

### 2.1 Augmentation: генерация синтетических запросов

#### Способ 1: docT5query

- Для каждого документа генерируем 3–5 псевдо-запросов.
- Используем их как (query, doc) пары для обучения.

#### Способ 2: GPT-подсказки

```python
prompt = f"Придумай 5 возможных вопросов, на которые отвечает следующий документ: {doc}"
```

### 2.2 External injection (внешние знания)

- Добавляем в индекс справочную информацию, определение терминов, топики из Википедии.
- Полезно, если в документах используется внутренняя терминология.

### 2.3 Pseudo-documents

- Генерация дополнительных документов на основе существующих (расширение, резюме, переформулировка).
- Увеличивает разнообразие и снижает переобучение.

### 2.4 Ручное покрытие intent'ов

- Задаём вручную перечень возможных вопросов / интентов.
- Создаём соответствующие документы или chunk-и.

---

## 3. Обучение retriever'а на малом корпусе

### 3.1 Использовать предварительно обученную модель

- Не fine-tune с нуля, а адаптировать (например, GTE, E5-small).

### 3.2 Использовать triplet loss с margin

- anchor = вопрос, positive = релевантный док, negative = нерелевантный.
- Margin позволяет избежать полного сжатия пространства.

### 3.3 Не доверять автоматическому семплингу негативов

- Лучше выбрать негативы вручную или использовать distant negatives (из внешнего корпуса).

### 3.4 Выравнивание распределения

- Использовать t-SNE / PCA, чтобы проверить, как ведут себя эмбеддинги.
- Обнаруживать кластеризацию noise vs signal.

---

## 4. Методы без обучения

### 4.1 Sparse baseline (BM25)

- Даже при маленьком корпусе BM25 остаётся разумной стартовой точкой.
- Можно комбинировать с ручной фильтрацией.

### 4.2 Rerank вручную написанными правилами

- Например: если query содержит "что такое", то приоритет документам с определениями.

### 4.3 One-shot embedding search

- Векторизуем все документы (через E5, bge-small).
- Используем cosine similarity без обучения.

---

## 5. Общие советы

- Убедись, что в корпусе нет дубликатов / мусора.
- Добавляй метаданные: source, категория, topic.
- Лучше 20 продуманных документов, чем 200 случайных.
- Не пытайся обучать retriever на 50 документах — просто сделай качественный поиск + rerank.

---

## 6. Что не работает

| Метод                      | Почему не работает         |
| -------------------------- | -------------------------- |
| Fine-tune dense retriever  | Сильное переобучение       |
| TF-IDF weighting           | Нет статистики             |
| Автосемплинг негативов     | Все документы близки       |
| Clustering-based negatives | Слишком малое разнообразие |

---

## 7. Что работает

| Метод                         | Эффект                        |
| ----------------------------- | ----------------------------- |
| GPT-генерация запросов        | Увеличивает покрытие интентов |
| Pseudo-documents              | Повышает устойчивость         |
| External injection            | Повышает recall               |
| Pretrained retriever + rerank | Работает даже без дообучения  |
