# Vector Imbalance

Несбалансированное распределение векторов (vector imbalance) — типичная проблема dense-поиска и эмбеддеров. Возникает, когда векторы в пространстве распределены неравномерно или «сваливаются» в узкое подпространство. Это приводит к падению качества поиска и деградации метрик.

---

## 1. Идея проблемы

- Эмбеддеры обучаются так, что разные объекты должны иметь разные представления.
- Иногда векторы распределяются **несбалансированно**:
  - Нормы сильно различаются (одни векторы «длинные», другие «короткие»).
  - Почти все векторы лежат в узком угле (cosine similarities ≈ 0.9 для всех).
  - Коллапс: векторы концентрируются вокруг одного центра.

---

## 2. Причины

1. **Проблемы обучения эмбеддера:**

   - Слишком простой loss (например, MSE вместо contrastive).
   - Отсутствие регуляризации норм.
   - Мало негативных примеров при contrastive learning.

2. **Особенности корпуса:**

   - Данные однотипные (например, только короткие заголовки).
   - Дисбаланс классов/языков.

3. **Без нормализации:**

   - При использовании cosine, но без L2-нормализации, нормы векторов растут и искажают метрику.

---

## 3. Последствия

- **Падение Recall\@K**: ANN индекс не различает документы.
- **Плохая дискриминация**: сходства всех векторов почти одинаковы.
- **Сложность в кластеризации**: k-means работает хуже, так как все точки «слиплись».

---

## 4. Методы обнаружения

- Гистограмма норм векторов (ожидаемая форма — колокол, проблемы: узкий пик или длинный хвост).
- Распределение cosine similarities между случайными парами (должно быть около 0, при проблеме → сдвинуто к 1).
- PCA/UMAP визуализация: векторы образуют «шарик» или «тонкий диск» вместо равномерного облака.

---

## 5. Методы решения

1. **L2-нормализация**:

   - Приведение всех векторов к единичной длине.
   - Устраняет перекос норм, корректна для cosine/IP.

2. **Регуляризация в обучении**:

   - Добавить penalty за большую норму.
   - Использовать contrastive / triplet loss с hard negatives.

3. **Data augmentation**:

   - Добавлять разнообразные примеры (разные языки, домены).
   - Балансировать датасет.

4. **Понижение размерности (PCA/SVD)**:

   - Убирает коррелированные измерения.
   - Делает пространство более равномерным.

5. **Temperature scaling в softmax**:

   - Контролирует распределение вероятностей и норм при обучении.

---

## 6. Практические советы

- Всегда **нормализовать эмбеддинги** при cosine/IP поиске.
- Проверять распределение норм после обучения новой модели.
- При сильном дисбалансе использовать PCA до 256–384 dim.
- Мониторить drift распределения при обновлении эмбеддеров.

---

## 7. Примеры 

### 7.1. Проверка норм

```python
import torch

emb = torch.randn(10000, 768)  # эмбеддинги
norms = emb.norm(dim=1)
print(norms.mean().item(), norms.std().item())
```

### 7.2. L2-нормализация

```python
import torch.nn.functional as F

emb_norm = F.normalize(emb, p=2, dim=1)
print(emb_norm.norm(dim=1).mean().item())  # ≈ 1.0
```

### 7.3. PCA для снижения размерности

```python
import numpy as np
from sklearn.decomposition import PCA

X = emb.numpy()
pca = PCA(n_components=256)
X_reduced = pca.fit_transform(X)
print(X_reduced.shape)  # (10000, 256)
```

---

## 8. Чеклист тюнинга

- Проверить распределение норм (mean ≈ 1, std > 0.05).
- Нормализовать векторы перед индексом.
- Применить PCA для устранения корреляций.
- Использовать contrastive loss с hard negatives.
- Балансировать тренировочный корпус.

