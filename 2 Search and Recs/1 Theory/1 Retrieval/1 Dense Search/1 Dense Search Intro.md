# Dense-поиск 

Dense search (плотный поиск, векторный поиск) — это метод поиска ближайших соседей в **плотном (dense) векторном пространстве**, где каждый объект (текст, изображение, пользователь и т.д.) представлен эмбеддингом фиксированной размерности.

---

## 1. Идея

* Объекты кодируются векторами $x \in \mathbb{R}^d$ с помощью нейросетевых моделей (Sentence-BERT, E5, BGE, CLIP, и т.д.).
* Поиск сводится к задаче **Nearest Neighbors Search (NNS)** по метрике:

  * L2 (евклидово расстояние),
  * Inner Product (dot product),
  * Cosine similarity (IP + L2-нормализация).
* В отличие от sparse (BM25, TF-IDF), dense search учитывает семантическую близость.

---

## 2. Архитектура пайплайна

1. **Эмбеддер**: модель, которая преобразует объект (текст, картинку) в вектор $d$-размерности.
2. **Индекс**: структура для поиска ближайших соседей (Flat, IVF, IVFPQ, HNSW, ScaNN, Annoy, Multi-shard и т.п.).
3. **Поиск**: запрос $q$ тоже преобразуется в вектор, после чего ищутся ближайшие соседи в индексе.
4. **Фильтры и метаданные**: иногда поиск ограничивается подмножеством (например, по языку или дате).
5. **Reranker (опционально)**: модель, которая пересортировывает top-k кандидатов для повышения точности.

---

## 3. Метрики близости

* **Cosine similarity:** $\cos(x, y) = \frac{x \cdot y}{|x||y|}$.
* **Inner product (IP):** $x \cdot y$. Для cosine нужна нормализация векторов.
* **L2 distance:** $|x - y|^2$.

Выбор метрики зависит от того, как обучен эмбеддер.

---

## 4. Сложность

* Flat (brute-force): $O(Nd)$ на запрос.
* ANN-алгоритмы снижают сложность до $O(\log N)$ или $O(\sqrt{N})$ на практике.
* Dense search почти всегда реализуется через ANN (FAISS, ScaNN, HNSW).

---

## 5. Достоинства

* Улавливает семантику (синонимы, перефразировки).
* Хорошо работает на мультимодальных данных (текст ↔ картинка).
* Поддерживает zero-shot (без учёта словаря).

---

## 6. Недостатки

* Требует обучения/дообучения эмбеддеров.
* Обновление индекса дороже, чем в sparse.
* Сложнее объяснять пользователям (непрозрачность по сравнению с BM25).

---

## 7. Практические советы

* Для текста использовать модели типа **E5, BGE, SBERT**.
* Для изображений/мультимодальности — **CLIP, SigLIP**.
* Нормализовать эмбеддинги для cosine.
* Хранить эмбеддинги в **векторной БД** (Qdrant, Milvus, Weaviate, Pinecone) или FAISS.
* Для больших коллекций использовать **IVFPQ, HNSW, ScaNN** и multi-shard.

---

## 8. Пример кода 

```python
import torch
import faiss

# Эмбеддер (например, простая линейная модель)
class Encoder(torch.nn.Module):
    def __init__(self, in_dim=768, out_dim=256):
        super().__init__()
        self.fc = torch.nn.Linear(in_dim, out_dim)
    def forward(self, x):
        return torch.nn.functional.normalize(self.fc(x), dim=-1)

model = Encoder()

# Генерим базу
N, d_in = 10000, 768
xb = torch.randn(N, d_in)
emb = model(xb).detach().cpu().numpy().astype('float32')

# Строим FAISS IndexFlatIP (cosine через нормализацию)
index = faiss.IndexFlatIP(256)
index.add(emb)

# Запрос
xq = model(torch.randn(5, d_in)).detach().cpu().numpy().astype('float32')
D, I = index.search(xq, 5)
print(I)
```

---

## 9. Чеклист тюнинга

* Проверить метрику: cosine vs IP vs L2.
* Нормализовать эмбеддинги для cosine.
* Выбрать ANN-алгоритм под требования SLA (Flat, IVF, HNSW, ScaNN).
* Для миллиардов векторов → IVFPQ/ScaNN + multi-shard.
* Добавить reranker для повышения качества top-k.
