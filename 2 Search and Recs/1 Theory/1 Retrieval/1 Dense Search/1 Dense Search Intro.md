# Dense поиск

## Мотивация

Dense поиск появился как альтернатива классическим **sparse методам** (BM25, TF‑IDF), которые опираются на лексическое совпадение слов. Главная проблема sparse‑подхода — он плохо работает при синонимах, морфологии и в случаях, когда важный документ не содержит тех же слов, что и запрос. Dense поиск позволяет уйти от точного совпадения токенов к **семантическому поиску** — сопоставлению значений.

Идея: обучить модель преобразовывать запросы и документы в векторы фиксированной размерности, так чтобы **семантически близкие пары имели близкие векторы**. Дальше задача сводится к поиску ближайших соседей (ANN).

## Основные шаги

1. **Выбор эмбеддера**

   - Bi‑encoder (Sentence‑Transformers, E5, BGE).
   - Размерность, pooling, L2‑нормализация.

2. **Индексация**

   - Хранение эмбеддингов в vector DB (FAISS, Qdrant, Weaviate, Milvus, pgvector).
   - ANN‑структуры: IVF‑PQ, HNSW, ScaNN.

3. **Поиск**

   - Запрос кодируется в вектор.
   - ANN возвращает top‑K ближайших документов по cosine/L2/IP.

4. **Каскад reranking (по желанию)**

   - Первичный dense поиск даёт кандидатов.
   - Далее может применяться cross‑encoder или late interaction (ColBERT) для уточнения.

5. **Жизненный цикл**

   - Drift‑мониторинг: PSI, KL‑div на распределениях.
   - Shadow re‑index + alias switch при обновлении эмбеддеров.

## Плюсы

- Семантический поиск (работает с синонимами, перефразами).
- Гибкость: можно использовать в RAG, рекомендациях, дедупликации.
- Совместимость с мультимодальными эмбеддерами (текст, изображение, аудио).

## Минусы

- Требует GPU/CPU‑ресурсов для обучения и инференса эмбеддеров.
- Индексы ANN потребляют много памяти, особенно при миллиардах документов.
- Падение качества при domain shift (нужно обновлять эмбеддеры).
- Более сложная инфраструктура (vector DB, pipeline, мониторинг drift).

## Что обычно требует

- **Данные**: качественный корпус для обучения/адаптации эмбеддера.
- **Инфраструктура**: vector DB с поддержкой ANN и метаданных.
- **Мониторинг**: метрики качества (nDCG, Recall\@K), latency, drift.
- **Интеграция**: каскад retrieval → rerank → генерация.
- **Команда**: ML‑инженеры + MLOps для поддержки жизненного цикла индексов.

