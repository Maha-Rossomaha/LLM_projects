# Late Interaction и ColBERT 

Late Interaction (позднее взаимодействие) — класс архитектур для поиска, в которых **запрос и документ кодируются раздельно**, но их **взаимодействие учитывается при скоринге** на уровне токенов. Самый известный представитель — **ColBERT** (Contextualized Late Interaction over BERT). Подход занимает промежуточную позицию между bi‑encoder (быстро, но грубо) и cross‑encoder (медленно, но точно).

---

## 1. Интуиция и место в семействе
- **Bi‑encoder:** $q=f_q(y)$, $d=f_d(x)$, затем $sim(q,d)$ (один вектор на текст). Высокая скорость, но потеря тонких взаимодействий.
- **Cross‑encoder:** $f([y;x]) \to score$ (модель видит токен‑к‑токен взаимодействия), но дорого на онлайне.
- **Late Interaction / ColBERT:** кодируем **все токены** отдельно, а потом считаем токен‑уровневое взаимодействие через простую операцию (MaxSim). Это сохраняет большую часть сигналов cross‑encoder, но оставляет возможность **предвычислить документные представления**.

---

## 2. Архитектура (ColBERT)
1. **Энкодеры** (обычно общий BERT/roBERTa):
   - Запрос $y \to$ токен‑эмбеддинги $Q=[q_1,\dots,q_m]$.
   - Документ $x \to$ токен‑эмбеддинги $D=[d_1,\dots,d_n]$.
2. **Проекция и нормализация:** линейный слой $W \in \mathbb{R}^{h\times d}$ и L2‑норма:
   $$q_i=\mathrm{norm}(W\,\mathrm{BERT}(y)_i),\quad d_j=\mathrm{norm}(W\,\mathrm{BERT}(x)_j).$$
3. **Хранение:** все $d_j$ документа сохраняются (или их сжатая версия) в индексе.
4. **Скоринг (MaxSim):** на запросе считаем матрицу скалярных произведений и берём максимум по документным токенам для каждого токена запроса.

> Ключевая идея: **разделить «понимание контекста» (BERT) и «взаимодействие» (MaxSim)** так, чтобы второе было дешёвым и позволило предвычислять документные представления.

---

## 3. Математика (MaxSim‑скор)
Пусть $Q=[q_1,\dots,q_m]$, $D=[d_1,\dots,d_n]$, все L2‑нормализованы. Тогда
$$
S(y,x)=\sum_{i=1}^{m} \max_{1\le j\le n} q_i^\top d_j.
$$
Часто используют взвешивание токенов запроса (например, по IDF):
$$
S(y,x)=\sum_{i=1}^{m} w_i\, \max_j q_i^\top d_j,\quad w_i\ge 0.
$$
Интерпретация: каждый токен запроса «ищет» **лучше всего соответствующий** ему токен документа, а итог — сумма таких наилучших совпадений.

---

## 4. Индексация и хранение
- **Документные токены:** сохраняем $d_j$ для всех токенов (или чанков) документа. Память $\approx N\cdot T_d\cdot d$ (с компрессией — существенно меньше).
- **Компрессия:**
  - FP16/INT8 квантование.
  - PQ/RQ (product/residual quantization) для $d_j$.
- **Поисковый движок:**
  - На практике используют двухэтапную схему: (а) **кандидаты** через быстрый отбор (BM25/IVF‑ANN/термовые списки), (б) **точный MaxSim‑скор** для top‑$K$ кандидатов по всем их токенам.
  - Оптимизации на уровне индекса: блоки токенов документа, прунинг «слабых» токенов, кэширование матриц $q^\top d$.

> Варианты реализации позднего взаимодействия для продакшена включают движки с блок‑максимумами и инвертированные структуры по кластерным центроидам токен‑векторов (для быстрой маршрутизации кандидатов).

---

## 5. Обучение
- **Лосс:** обычно pairwise/listwise ранжирование на уровне документов, используя $S(y,x)$ как скор; популярны InfoNCE/NLL с in‑batch negatives.
- **Hard negatives:** важны близкие по смыслу, но нерелевантные документы.
- **Дистилляция:** cross‑encoder как teacher, $S(y,x)$ подгоняется под teacher‑скоры.
- **Регуляризация:** дропаут токенов, L2‑норма, ограничение длины (truncate/stride) для очень длинных документов.

---

## 6. Инференс и пайплайн
1. **Прединдексация документов:** считать $D$ offline и сохранить (возможно, сжатые $d_j$).
2. **Онлайн шаг:**
   - Закодировать запрос → $Q$.
   - Выбрать **кандидатов** (BM25/ANN/гибрид).
   - Посчитать **MaxSim** на top‑$K$ и вернуть top‑$R$.
3. **Гибридизация:** легко добавляется rerank cross‑encoder на top‑$R$.

---

## 7. Параметры
- **$d$ (размерность после проекции):** 64–128–192. Меньше $d$ → быстрее/компактнее, но ниже точность.
- **$m,n$ (длины):** ограничивать max токенов для запроса/документа; для документов часто используют **чанкинг** (например, 180–300 токенов с overlap).
- **$K$/$R$:** размер кандидатов для MaxSim и возвращаемого списка.
- **Компрессия:** тип (FP16/INT8/PQ), параметры PQ ($m$‑подпространств, $nbits$), глубина RQ.

---

## 8. Оптимизации
- **IDF‑веса $w_i$**: подавлять стоп‑слова в запросе.
- **Прунинг токенов документа:** удалять токены с низкой полезностью (часто стоп‑слова/пунктуация) или хранить их сильно сжато.
- **Block‑max/impact‑ordered:** хранить верхние границы вклада по блокам токенов для раннего отсечения кандидатов.
- **Кэширование $Q$:** для частых запросов; кэш скалярных произведений по блокам.
- **Параллелизм:** батчи запросов и кандидатов; SIMD для матриц $q^\top d$; GPU‑ускорение.

---

## 9. Сравнение с альтернативами
| Критерий | Bi‑encoder | Late Interaction (ColBERT) | Cross‑encoder |
|---|---|---|---|
| Качество | ниже | близко к cross | выше всего |
| Скорость онлайн | очень высокая | средняя (дороже bi, дешевле cross) | низкая |
| Память | низкая (1 вектор/док) | средняя/высокая (все токены) | низкая |
| Инфраструктура | ANN | специальный индекс токенов + кандидатный слой | без индекса, но тяжёлый онлайн |

---

## 10. Edge cases и риски
- **Очень длинные документы:** число токенов \uparrow → память/latency. Решение: чанкинг + best‑chunk агрегация.
- **Мультиязычие:** общий энкодер (mBERT/XLM‑R) + нормализация токенизации.
- **Стоп‑слова:** вес $w_i\approx0$ в запросе, прунинг в документах.
- **Повторяющиеся токены:** MaxSim может «насыщаться» на повторениях; полезны IDF‑веса.
- **Дрейф домена:** обновления индекса при смене модели/корпуса.

---

## 11. Практические советы
- Начать с $d=128$, max_doc_tokens=180–300 (с overlap), FP16; проверить INT8/PQ.
- Кандидаты: BM25 или быстрый ANN по токен‑векторам; $K=200\dots1000$, $R=50\dots200$.
- Обязательно **hard negatives** и дистилляция от cross‑encoder.
- Отдельные поля (title/abstract/body) — полезно давать более высокий вес заголовку.
- Мониторить p95 latency и размер индекса; включить block‑max прунинг.

---

## 12. Примеры

### 12.1. MaxSim‑скоринг для пары (без батча)
```python
import torch
import torch.nn.functional as F

# Q: [m, d], D: [n, d] — L2-нормализованы
Q = F.normalize(torch.randn(16, 128), dim=-1)
D = F.normalize(torch.randn(200, 128), dim=-1)

# Матрица скоров [m, n]
S = Q @ D.t()
# MaxSim по документным токенам для каждого токена запроса
max_per_q, _ = S.max(dim=1)
score = max_per_q.sum().item()
print(score)
```

### 12.2. Взвешенный MaxSim (IDF‑веса токенов запроса)
```python
w = torch.rand(Q.size(0))  # например, нормированные IDF
score_w = (max_per_q * w).sum().item()
```

### 12.3. Чанкинг документа и «лучший чанк»
```python
def chunks(X, size=196, overlap=32):
    i = 0
    while i < X.size(0):
        yield X[i:i+size]
        i += size - overlap

best = -1e9
for D_chunk in chunks(D, size=196, overlap=32):
    s = (Q @ D_chunk.t()).max(dim=1).values.sum()
    best = max(best, s.item())
print(best)
```

---

## 13. Чеклист тюнинга
- $d \in \{64,128,192\}$; компрессия FP16→INT8/PQ.
- max длины для запросов/документов, чанкинг и overlap.
- Кандидатный слой: BM25 vs ANN по токенам; подобрать $K$/$R$.
- IDF‑веса для запросов; прунинг документных токенов.
- Дистилляция и hard negatives.
- Block‑max/impact‑ordered индекс для прунинга.

---

## 14. Метрики и мониторинг
- **Качество:** Recall@K, nDCG@K, MRR; раздельно по длинам запросов.
- **Производительность:** p50/p95/p99 latency; QPS; размер индекса.
- **Стабильность:** дрейф (распределения норм и токенов), регрессии при обновлении модели.

---

## 15. Глоссарий
- **Late Interaction:** класс методов с раздельным кодированием и токен‑уровневым взаимодействием на скоринге.
- **ColBERT:** реализация Late Interaction с MaxSim‑скорингом и проекцией на низкую размерность.
- **MaxSim:** $\sum_i \max_j q_i^\top d_j$.
- **Чанкинг:** разбиение документа на фрагменты для контроля памяти/latency.
- **Block‑max:** верхние границы вкладов токенов в блоке для раннего отсечения.

