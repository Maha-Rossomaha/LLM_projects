# Dimension, pooling and L2-нормализация 

Эти три аспекта — ключевые настройки при работе с эмбеддингами для dense-поиска и retrieval-систем: **размерность вектора, стратегия pooling и L2-нормализация**. Они напрямую влияют на качество поиска, память и скорость.

---

## 1. Размерность эмбеддингов
- Эмбеддеры (BERT, SBERT, E5, BGE, CLIP и др.) обычно выдают векторы размерности $d = 256, 384, 512, 768, 1024$.
- **Высокая размерность** (768, 1024):
  - **Преимущества**: лучшее качество (больше информации).
  - **Недостатки**: больше памяти ($4 \times d$ байт/вектор) и медленнее поиск.
- **Низкая размерность** (128–256):
  - **Преимущества**: быстрее, меньше памяти.
  - **Недостатки**: возможная потеря точности.
- **Снижение размерности:**
  - PCA / SVD / autoencoder — уменьшаем $d$, сохраняя максимум дисперсии.
  - Часто берут $d=256$ или $d=384$ для продакшена.

---

## 2. Pooling
Pooling нужен, чтобы получить фиксированный вектор из токенов (BERT-выход: [seq_len × hidden_dim]).

### Варианты:
- **[CLS] pooling:** берём эмбеддинг CLS-токена.
  - Просто, но не всегда оптимально.
- **Mean pooling:** усредняем эмбеддинги всех токенов.
  - Лучше захватывает общий смысл.
- **Max pooling:** берём максимум по каждому измерению.
  - Хорошо для задач «выбор наиболее ярких признаков».
- **Weighted pooling:** взвешенное усреднение (attention weights, tf-idf, BM25, др.).
- **Concat pooling:** комбинация нескольких стратегий (например, mean+max).

### Практика:
- В Sentence-BERT используется **mean pooling**.
- В E5/BGE — CLS-токен (специально обучен под retrieval).

---

## 3. L2-нормализация
- Делает векторы единичной длины: $x' = \frac{x}{\|x\|_2}$.
- Для cosine similarity: $cos(x,y) = x' \cdot y'.$
- **Зачем:**
  - Стабилизирует распределение норм (без нормализации часть векторов может иметь огромные нормы → искажённый поиск).
  - Ускоряет обучение (loss меньше зависит от scale).
  - Делает cosine и dot product эквивалентными.
- **Когда нужна:**
  - При cosine-поиске (почти всегда в retrieval).
  - При contrastive learning (triplet, InfoNCE).
- **Когда не нужна:**
  - Если метрика — L2 distance (но вектор может быть нормализован и тут, тогда это уже spherical distance).

---

## 4. Взаимосвязь
- **Размерность $d$** ↔ память и latency.
- **Pooling** ↔ качество эмбеддера.
- **L2-нормализация** ↔ корректная метрика и стабильность.

Пример: SBERT (mean pooling, $d=768$, L2-normalize) → обычно дают recall@10 ≈ 0.9 на MS MARCO.

---

## 5. Практические советы
- Если важна память и latency → уменьшить $d$ (PCA до 256–384).
- Проверять pooling: CLS vs mean (в зависимости от модели).
- Для cosine similarity всегда нормализовать векторы.
- Для обучения bi-encoder: использовать L2-normalization + margin loss.
- Для inference: хранить только нормализованные векторы (уменьшает вариативность).

---

## 6. Примеры 

### 6.1. Mean pooling + L2-нормализация
```python
import torch
from torch.nn import functional as F

# Эмбеддинги токенов (batch=2, seq_len=5, hidden=768)
tok_emb = torch.randn(2, 5, 768)
mask = torch.tensor([[1,1,1,1,0],[1,1,1,0,0]])  # attention mask

# Mean pooling
sum_emb = (tok_emb * mask.unsqueeze(-1)).sum(1)
len_emb = mask.sum(1, keepdim=True)
mean_emb = sum_emb / len_emb

# L2-нормализация
norm_emb = F.normalize(mean_emb, p=2, dim=1)
print(norm_emb.shape)  # torch.Size([2, 768])
```

### 6.2. PCA для уменьшения размерности
```python
import torch
import numpy as np
from sklearn.decomposition import PCA

# Эмбеддинги (1000 документов, dim=768)
emb = torch.randn(1000, 768).numpy()

# PCA → 256
pca = PCA(n_components=256)
emb_reduced = pca.fit_transform(emb)
print(emb_reduced.shape)  # (1000, 256)
```

---

## 7. Чеклист тюнинга
- Проверить, какое pooling лучше для конкретного эмбеддера.
- Нормализовать векторы для cosine/IP.
- При больших $d$ сделать PCA до 256–384.
- Тестировать Recall@K vs Latency при разных $d$.
- Для мультиязычных моделей — всегда проверять распределение норм (обязательная нормализация).

