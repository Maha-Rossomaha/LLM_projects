# Multi-shard 

Multi-shard (многошардовая архитектура) — это способ горизонтального масштабирования ANN‑индексов и векторных баз данных. Вместо одного огромного индекса база делится на несколько **шардов** (частей), которые обрабатываются параллельно на разных узлах или процессах.

---

## 1. Идея
- Делим всю коллекцию векторов $X = \{x_i\}$ на $S$ частей.
- Каждый шард хранит свой индекс (IVF, HNSW, ScaNN, Annoy и т.д.).
- Запрос (query) отправляется на все шарды или на подмножество шардов.
- На каждом шарде вычисляется локальный top‑k кандидатов.
- Результаты мёрджатся глобально (k‑way merge).

---

## 2. Отношение к ANN-алгоритмам

### 2.1 Multi-shard ≠ отдельный алгоритм ANN

* **ANN-алгоритмы** (IVF, PQ, IVFPQ, HNSW, OPQ, ScaNN, Annoy и др.) определяют, **как именно** искать ближайшие соседи внутри одной базы/индекса.
* **Multi-shard** — это архитектурный приём: деление огромной базы на части (шарды), которые могут использовать любой из этих алгоритмов.

### 2.2 Как это работает вместе

* Каждый шард — отдельный ANN-индекс (например, IVFPQ или HNSW).
* Координатор → получает запрос → отправляет на все или часть шардов → собирает локальные top-k → делает глобальный merge.
* Таким образом, multi-shard — это «слой над ANN».

### 2.3 Примеры

* Векторная база **Qdrant**: внутри шарда может использовать HNSW, а вся база масштабируется шардингом.
* **Milvus / Weaviate / Pinecone**: то же самое — внутри шарда один ANN-алгоритм, сверху — распределённая архитектура.
* В FAISS: можно вручную собрать пул индексов (по шардам) и координировать поиск (пример с ThreadPoolExecutor).

### 2.4 Ключевая мысль

Multi-shard — это не алгоритм поиска, а **архитектурный паттерн горизонтального масштабирования**, который применяется **поверх любого ANN-индекса** для работы с миллиардами объектов и высокой нагрузкой.

---

## 3. Архитектура
1. **Sharding strategy:**
   - **Hash‑based** (по ID или по хэшу): равномерное распределение нагрузки.
   - **Semantic (vector partition)**: шард соответствует кластеру (k‑means partitioning).
   - **Range‑based**: по признаку (например, по времени или по пользователю).

2. **Replication:**
   - **Primary + replicas**: для высокой доступности и уменьшения latency.
   - Replicas обслуживают запросы параллельно, p99 latency снижается.

3. **Fan‑out query:**
   - Координатор отправляет запрос на все релевантные шарды.
   - Каждый шард возвращает $k'$ кандидатов.
   - Координатор объединяет в глобальный top‑k.

---

## 4. Алгоритм объединения (merge)
- Используется **k‑way merge** (аналог многопутевого слияния в сортировке).
- Если каждый шард возвращает $k'$ кандидатов, то глобальный merge стоит $O(S \cdot k' \cdot \log S)$.
- Обычно $k' = \alpha k$, где $\alpha \in [2,5]$, чтобы не потерять хорошие кандидаты при merge.

---

## 5. Параметры
- **S (число шардов):** больше шардов → меньше latency на шард, но выше накладные расходы на merge.
- **Replication factor (R):** больше реплик → выше отказоустойчивость, но выше расходы по памяти.
- **Shard selection:** отправляем запрос на все $S$ шардов или только на top‑L релевантных (semantic routing).

---

## 6. Сложность и ресурсы
- Память: $O(Nd)$ делится на $S$, но при репликации умножается на $R$.
- Поиск: $O((N/S) \cdot cost\_index)$ параллельно на $S$ узлах.
- Merge: $O(S \cdot k' \cdot \log S)$, обычно мало по сравнению с ANN‑поиском.

---

## 7. Достоинства
- Масштабирование до миллиардов векторов.
- Распределение нагрузки и отказоустойчивость.
- Можно сочетать разные типы индексов на разных шардах.

---

## 8. Недостатки
- Сложнее инфраструктура (координатор, балансировка).
- Tail latency: общий SLA зависит от самого медленного шарда.
- Перераспределение при добавлении/удалении шарда (resharding).

---

## 9. Практические советы
- Использовать **hash‑based sharding** как дефолт: простота и равномерность.
- При семантическом шардинге использовать роутинг (query → top‑L кластеров).
- Следить за p99 latency: дублировать горячие шарды (replication).
- Делать **shadow‑resharding**: построить новый слой шардов и переключить alias.

---

## 10. Примеры

### 10.1. Координатор multi‑shard с FAISS
```python
import faiss
import numpy as np

# Допустим, есть S=4 шарда
indices = [faiss.IndexFlatL2(128) for _ in range(4)]

# Добавляем данные (по shard_id)
for i, vec in enumerate(vectors):
    shard_id = i % 4  # простое hash‑разбиение
    indices[shard_id].add(vec.reshape(1, -1))

# Поиск
k = 10
query = np.random.randn(1, 128).astype('float32')

all_results = []
for idx in indices:
    D, I = idx.search(query, k*2)  # возвращаем с запасом
    all_results.extend(zip(D[0], I[0]))

# Глобальный top‑k
all_results.sort(key=lambda x: x[0])
global_topk = all_results[:k]
print(global_topk)
```

### 10.2. Распределённый вариант (fan‑out)
```python
from concurrent.futures import ThreadPoolExecutor

# Функция поиска на шарде
def search_shard(index, q, k):
    D, I = index.search(q, k*2)
    return list(zip(D[0], I[0]))

with ThreadPoolExecutor(max_workers=len(indices)) as ex:
    futures = [ex.submit(search_shard, idx, query, k) for idx in indices]

all_results = []
for f in futures:
    all_results.extend(f.result())

all_results.sort(key=lambda x: x[0])
global_topk = all_results[:k]
print(global_topk)
```

---

## 11. Чеклист тюнинга
- Выбрать стратегию шардирования (hash vs semantic).
- Решить, отправляем ли запросы на все шарды или только на top‑L.
- Настроить $k'$ (сколько возвращать кандидатов с шарда) и глобальный merge.
- Контролировать p95/p99 latency (replication + hedged queries).
- При изменении количества шардов использовать shadow‑resharding.

