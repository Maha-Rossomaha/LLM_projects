# Hierarchical Navigable Small World (HNSW)

Hierarchical Navigable Small World (HNSW) — графовый метод поиска ближайших соседей, реализованный в FAISS. Отличается высокой точностью (близко к exact) и хорошей масштабируемостью.

---

## 1. Идея

* Строим многослойный граф (иерархию), где вершины — векторы.
* Верхние уровни содержат мало вершин и длинные связи (далёкие соседи).
* Нижние уровни более плотные, содержат связи с ближайшими соседями.
* Поиск: двигаемся сверху вниз. На каждом уровне стартуем из лучшей найденной точки и продолжаем жадный поиск к запросу.

---

## 2. Архитектура

1. **Уровни:** каждый вектор случайным образом назначается на уровни (по распределению, убывающему экспоненциально). Верхний уровень содержит мало точек.
2. **Связи:** каждый узел соединён с $M$ соседями (обычно $M=16$–$64$), отобранными по эвристикам «diversified connectivity» (не только ближайшие).
3. **Параметры построения:**

   * `M`: максимальное число связей у каждой вершины.
   * `efConstruction`: число кандидатов в буфере при построении.
4. **Поиск:**

   * `efSearch`: число поддерживаемых кандидатов при поиске.

---

## 3. Построение графа

1. Для каждой новой точки случайно выбирается максимальный уровень (геометрическое распределение).
2. На верхнем уровне добавляем связей к ближайшим точкам.
3. Спускаемся по уровням: для каждого уровня находим кандидатов (размер буфера `efConstruction`) и выбираем $M$ связей.

---

## 4. Поиск

1. Начинаем с случайного entry-point на верхнем уровне.
2. Жадно движемся к ближайшему соседу относительно запроса.
3. Когда улучшений нет — спускаемся на уровень ниже, используя текущую лучшую точку.
4. На нижнем уровне выполняем поиск с буфером размера `efSearch`.
5. Возвращаем top-$k$ ближайших.

---

## 5. Параметры

* `M`: число рёбер у вершины. Большее $M$ → лучше качество (связность), но больше память. Обычно 16–64.
* `efConstruction`: качество построения. Чем больше, тем лучше recall, но дороже индексирование.
* `efSearch`: баланс recall/latency. Большее `efSearch` даёт выше recall, но больше задержку. Обычно 32–256.

---

## 6. Сложность и память

* Память: $O(NM)$ рёбер + сами векторы. Обычно больше, чем у IVF/IVFPQ.
* Поиск: $O(M \log N)$, на практике очень быстро (миллисекунды при миллионах точек).
* Построение: дороже, чем у IVF (из-за связей и буфера).

---

## 7. Достоинства

* Очень высокая точность (близко к exact kNN).
* Быстрый поиск при умеренных $N$ (до сотен миллионов).
* Гибкая настройка через `efSearch`.

---

## 8. Недостатки

* Память выше, чем у IVFPQ/IVF.
* Вставка и удаление дороже, чем у IVF.
* Построение индекса занимает больше времени.

---

## 9. Практические советы

* Для recall > 0.95 берут `efSearch` ≥ 64.
* `M=32` и `efConstruction=200` — хорошая стартовая конфигурация.
* Для больших $N$ и ограниченной памяти лучше использовать IVFPQ.
* На GPU HNSW не всегда поддерживается — чаще используют CPU-индекс.

---

## 10. Примеры кода 

### 10.1. Построение HNSW под L2

```python
import numpy as np
import faiss

# Параметры
d = 128
N = 100_000
M = 32
k = 10

# База
xb = np.random.randn(N, d).astype('float32')

# Создаём HNSW
index = faiss.IndexHNSWFlat(d, M)  # Flat storage + HNSW граф

# Параметры построения
index.hnsw.efConstruction = 200

# Добавление векторов
index.add(xb)

# Поиск
xq = np.random.randn(10, d).astype('float32')
index.hnsw.efSearch = 64
D, I = index.search(xq, k)
print(I[:5])
```

### 10.2. HNSW с Inner Product (cosine)

```python
import numpy as np
import faiss

d = 256
N = 50_000
M = 32

xb = np.random.randn(N, d).astype('float32')
faiss.normalize_L2(xb)  # для cosine

index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_INNER_PRODUCT)
index.hnsw.efConstruction = 100

index.add(xb)

xq = np.random.randn(5, d).astype('float32')
faiss.normalize_L2(xq)
index.hnsw.efSearch = 64
D, I = index.search(xq, 5)
print(I)
```

---

## 11. Чеклист тюнинга

* Начинать с `M=32`, `efConstruction=200`, `efSearch=64`.
* Тюньте `efSearch` под SLA: выше → лучше recall, но выше latency.
* При очень больших $N$ оценить память: HNSW может быть тяжёлым.
* Для GPU использовать альтернативы (IVFPQ, ScaNN) или гибридные подходы.
