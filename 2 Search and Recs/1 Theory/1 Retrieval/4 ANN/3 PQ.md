# Product Quantization (PQ)

Product Quantization (PQ) — метод сжатия векторов и ускорения поиска ближайших соседей за счёт аппроксимации расстояний.

---

## 1. Идея

* Имеем вектор $x \in \mathbb{R}^d$.
* Разбиваем $x$ на $M$ под-векторов: $x = (x^{(1)}, …, x^{(M)})$, где каждый $x^{(m)} \in \mathbb{R}^{d/M}$.
* Для каждого подпространства обучаем кодбук (обычно $2^{nbits}$ центров через k-means):
  * Берём все $x^{(m)}$ из базы.
  * Запускаем на них k-means с $K=2^{nbits}$ кластерами.
  * Получаем $K$ центров: ${c^{(m)}_1, …, c^{(m)}_K}$.
  * Это и есть codebook для $m$-го подпространства.
* Кодируем $x^{(m)}$ индексом ближайшего центра. Итоговый PQ-код вектора — $M$ индексов: **Если nbits=8 → $K=256$ центров.**

---

## 2. Обучение кодбуков

1. Берём обучающую выборку $X_{train}$ из базы.
2. Для каждого подпространства $m=1…M$:

   * Собираем соответствующие под-векторы $x^{(m)}$.
   * Запускаем k-means на $2^{nbits}$ кластеров.
   * Сохраняем центры $C^{(m)} = {c^{(m)}_j}$.
3. Теперь каждый вектор кодируется как $(i_1, …, i_M)$, где $i_m$ — индекс ближайшего $c^{(m)}_j$.

---

## 3. Поиск (Asymmetric Distance Computation, ADC)

Для запроса $q$:

1. Разбиваем $q$ на $M$ под-векторов $q^{(m)}$.
2. Для каждого $m$ строим LUT (lookup table) расстояний $[|q^{(m)}-c^{(m)}_j|^2]$ для всех $j$.
3. Для кандидата с кодом $(i_1,…,i_M)$ аппроксимированное расстояние считается как
   $\operatorname{dist}(q,x) \approx \sum_{m=1}^M LUT^{(m)}[i_m].$
4. Сохраняем top-$k$ кандидатов.

---

## 4. Память

* Код: $M \cdot nbits/8$ байт.
* ID вектора: 4–8 байт.
* Кодбуки: $M \cdot 2^{nbits} \cdot (d/M)$ float-чисел.

**Пример:** $d=768, M=96, nbits=8$. Код: $96$ байт. Для $N=10^7$ → всего \~0.96 ГБ кодов (+ кодбуки и ID).

---

## 5. Параметры

* $M$ — число под-векторов. Большее $M$ → тоньше аппроксимация, выше качество, но больше память.
* $nbits$ — бит на под-вектор (обычно 4, 6, 8). Большее $nbits$ → больше кодбук, лучше recall, дороже LUT.
* $d$ должно делиться на $M$.

---

## 6. Варианты поиска

* **ADC (Asymmetric Distance Computation):** кодируются только база, запрос остаётся float. Это основной вариант (лучше качество).
* **SDC (Symmetric Distance Computation):** кодируется и база, и запросы. Быстрее, но хуже recall.
* **RDC (Reconstruction Distance Computation):** восстанавливаем вектор из кодов и считаем точное расстояние. Лучше качество, дороже вычислительно.

---

## 7. Достоинства

* Экономия памяти (сотни МБ вместо десятков ГБ).
* Быстрый поиск (суммирование $M$ чисел вместо $d$).
* Хорошо параллелится (LUT по блокам).

---

## 8. Недостатки

* Потеря точности (аппроксимация).
* Требует репрезентативной выборки для обучения кодбуков.
* При изменении распределения данных возможен drift — падение качества.

---

## 9. Практические советы

* Брать $M$ так, чтобы $d/M \in [8,16]$ (сбалансированные подпространства).
* Обычно $nbits=8$ даёт хороший баланс; $nbits=4$ — очень сжато, но качество страдает.
* Для cosine использовать нормализацию (L2 для базы и запросов).
* Обновлять кодбуки при сильном drift.
* Для повышения качества использовать OPQ (предобученный поворот), но это уже отдельная техника.

---

## 10. Пример кода 

```python
import numpy as np
import faiss

d = 128
M = 16      # число под-векторов
nbits = 8   # бит на под-вектор (кодбук из 256 центров)

xb = np.random.randn(100000, d).astype('float32')

# Создаём PQ-индекс для L2
index = faiss.IndexPQ(d, M, nbits)

# Тренировка кодбуков
index.train(xb[:20000])

# Добавление базы
index.add(xb)

# Поиск
xq = np.random.randn(10, d).astype('float32')
D, I = index.search(xq, 5)
print(I[:5])
```

--- 

## 11. Применение

* Подходит для миллиардных корпусов при ограниченной памяти.
* Можно использовать как standalone (IndexPQ) или как часть IVF (IndexIVFPQ).

---

## 12. Чеклист тюнинга

* Начать с $M=64$–$96$, $nbits=8$.
* Проверить recall vs latency на целевых $N$.
* Для больших $N$ почти всегда используют IVFPQ (чтобы не сканировать весь массив).