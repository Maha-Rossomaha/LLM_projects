# Cold-start users

## 0. Описание задачи: новые пользователи без истории

**Cold-start user** — ситуация, когда в систему приходит пользователь, у которого:

- ещё нет взаимодействий с объектами (товарами, статьями, видео),
- либо история настолько мала, что на неё нельзя опереться (1–2 клика без структуры).

Формально:

- есть множество пользователей $U$ и объектов $I$;
- матрица взаимодействий $R \in \mathbb R^{|U| \times |I|}$;
- для нового пользователя $u_{\text{new}}$ строка $R_{u_{\text{new}}}$ пуста или почти пуста.

Бизнес-проблема:

1. Нужно **сразу** показать что-то разумное, чтобы не потерять пользователя на первом экране.
2. При этом у нас **нет индивидуальной истории**, только общий контекст/мета-информация (если повезёт).
3. Желательно как можно быстрее превратить cold-start user в **warm user** — то есть набрать первые взаимодействия и построить персональный профиль.

Типичные источники информации на старте:

- популярность/тренды по системе в целом,
- демография и канал (возраст, пол, регион, источник трафика, устройство),
- текстовый профиль/анкета,
- история из других продуктов экосистемы или on-device данные.

---

## 1. Popular / trending fallback

### 1.1. Global popular: базовый спасательный круг

Самый простой и всегда рабочий слой — **глобально популярные** объекты.

Определяем глобальный CTR/конверсию объекта $i$:

$$
CTR_i = \frac{clicks_i}{shows_i}, \quad CR_i = \frac{purchases_i}{shows_i}.
$$

Наивный топ-
$K$:

- отсортировать по $CTR_i$ или $CR_i$,
- отфильтровать по минимальному числу показов, чтобы не ловить шум,
- взять $K$ лучших.

Для совсем нового пользователя $u_{\text{new}}$ можно просто показать **global popular**. Это уже лучше, чем случайный список.

### 1.2. Segment popular: популярность в сегменте

Чаще всего есть хотя бы грубая сегментация:

- страна / регион,
- язык интерфейса,
- категория входа (например, пришёл в раздел "фильмы" или "одежда"),
- тип устройства (мобильный/десктоп),
- источник трафика (SEO, реклама, push и т.п.).

Обозначим сегмент пользователя как $s(u)$. Тогда считаем статистики в каждом сегменте:

$$
CTR_{i,s} = \frac{clicks_{i,s}}{shows_{i,s}}.
$$

Проблема: в маленьких сегментах данные шумные. Используем **байесовское сглаживание** с глобальным prior:

$$
\hat p_{i,s} = \frac{clicks_{i,s} + \alpha \cdot p_i^{global}}{shows_{i,s} + \alpha},
$$

где $p_i^{global}$ — глобальный CTR объекта, $\alpha$ — параметр силы prior.

Тогда **segment popular** для сегмента $s$ — это объекты с наибольшим $\hat p_{i,s}$.

### 1.3. Trending: рост популярности во времени

Кроме статичной популярности важны **тренды** — объекты, у которых резко растёт активность за последние $N$ часов или дней.

Пример простого тренд-скора:

$$
trend_i = \frac{shows_i^{(recent)} + \beta}{shows_i^{(past)} + \beta},
$$

где $shows_i^{(recent)}$ — показы элемента $i$ за последний интервал (например, 3 часа), $shows_i^{(past)}$ — показы до этого, $\beta$ — сглаживание (чтобы избежать деления на ноль и уменьшить шум на элементах с очень низкой активностью).

* Если $trend_i > 1$ → показы растут, элемент “в тренде”.
* Если $trend_i \approx 1$ → показы стабильны.
* Если $trend_i < 1$ → показы падают

Можно комбинировать тренд с CTR/CR, чтобы не пушить чистый шум:

$$
score_i^{trend} = CTR_i^{(recent)} \cdot \log(1 + shows_i^{(recent)}) \cdot f(trend_i).
$$

1. $CTR_i^{(recent)}$ — кликабельность (или конверсия) за последний интервал.
    * Позволяет учитывать качество трафика. Элемент может быть в тренде по показам, но если никто не кликает — нет смысла его продвигать.
2. $\log(1 + shows_i^{(recent)})$ — модификатор по объёму показов:
    * Увеличивает вес для элементов, которые реально показываются больше,
    * Логарифм сглаживает эффект: элемент с 1000 показов не будет в 10 раз сильнее, чем с 100 показами.
3. $f(trend_i)$ — функция тренда. Может быть:
    * Просто $trend_i$ (линейная),
    * $log(trend_i)$,
    * какая-то кастомная функция, усиливающая рост тренда и подавляющая падение.
  
**Идея:**
* Мы не хотим просто пушить элементы с резким всплеском показов — это может быть шум.
* Поэтому score учитывает качество (CTR) и популярность (логарифм показов), плюс тренд, чтобы отдавать предпочтение элементам, которые реально набирают обороты.

### 1.4. Комбинация global + segment + trending для нового пользователя

Для cold-start user на первом экране обычно используют **микс**:

- глобально популярные объекты,
- популярные в конкретном сегменте,
- текущие тренды.

Простейшая линейная комбинация для объекта $i$ и сегмента $s$:

$$
score_{i,s}^{fallback} = w_1 \hat p_{i,s} + w_2 p_i^{global} + w_3 score_i^{trend},
$$

где $w_1, w_2, w_3$ — веса (заданы руками или подобраны моделью).

Для **совсем нового пользователя** $u_{\text{new}}$ в сегменте $s(u_{\text{new}})$:

1. Берём список объектов $I$ с ненулевыми показами.
2. Считаем $score_{i,s(u_{\text{new}})}^{fallback}$.
3. Выдаём топ-$K$.

Этого уже достаточно, чтобы собрать **первые клики/просмотры**, то есть сделать пользователя не совсем cold.

### 1.5. Пример: популярный fallback с сегментами

```python
from collections import defaultdict, Counter

# interactions: (user_id, item_id, clicked, segment)
interactions = [
    (1, 10, 1, "ru_mob"),
    (2, 10, 0, "ru_mob"),
    (3, 11, 1, "ru_web"),
    (4, 12, 1, "ru_mob"),
]

shows_global = Counter()
clicks_global = Counter()

shows_seg = defaultdict(Counter)
clicks_seg = defaultdict(Counter)

for _, item, clicked, seg in interactions:
    shows_global[item] += 1
    clicks_global[item] += clicked
    shows_seg[seg][item] += 1
    clicks_seg[seg][item] += clicked

alpha = 20.0  # сила глобального prior


def smoothed_ctr(item: int, seg: str) -> float:
    sg = shows_seg[seg]
    cg = clicks_seg[seg]
    s_seg = sg[item]
    c_seg = cg[item]

    s_glob = shows_global[item]
    c_glob = clicks_global[item]

    p_global = c_glob / s_glob if s_glob > 0 else 0.0
    return (c_seg + alpha * p_global) / (s_seg + alpha)


def recommend_for_new_user(segment: str, k: int = 10):
    items = list(shows_global.keys())
    scored = [(smoothed_ctr(i, segment), i) for i in items]
    scored.sort(reverse=True)
    return [i for _, i in scored[:k]]

print(recommend_for_new_user("ru_mob"))
```

---

## 2. Contextual priors / демография / канал

### 2.1. Какие контекстные признаки бывают

Даже для нового пользователя часто известен **контекст**:

- возрастная группа, пол,
- регион/страна,
- язык системы,
- устройство (iOS/Android/Web), тип сети,
- источник трафика (organic, рекламная кампания X, push Y),
- время суток, день недели.

Эти признаки можно использовать, чтобы строить **контекстно-зависимый скор**.

### 2.2. Модель $score(u, i) = f(context(u), features(i))$

Обозначим:

- $context(u)$ — вектор контекстных/демографических признаков пользователя;
- $features(i)$ — признаки объекта (meta-features: категория, цена, бренд, текстовые фичи и т.п.).

Строим общий вектор признаков пары $(u, i)$:

$$
z_{u,i} = [context(u), features(i)].
$$

И обучаем модель (логистическая регрессия или GBDT):

$$
P(\text{click}=1 \mid u, i) = \sigma(w^\top z_{u,i}) \quad \text{или} \quad P = f_{GBDT}(z_{u,i}).
$$

Важно: **эта модель может обучаться на warm-пользователях**, но использоваться и для cold-start user, потому что $context(u)$ нам известен сразу (возраст, регион, канал и т.п.).

### 2.3. Пример признаков

Для пары $(u, i)$ можно использовать:

- признаки пользователя:
  - one-hot пола (M/F/unknown),
  - one-hot/embedding региона,
  - возрастной бин (18–24, 25–34, ...),
  - индикаторы канала (utm_source / utm_campaign);
- признаки объекта:
  - категория, подкатегория,
  - цена (лог-цена, биннинг),
  - свежесть (время с момента публикации),
  - текстовые фичи (embedding статьи/товара);
- признаки контекста:
  - час суток, день недели,
  - платформа (iOS/Android/Web),
  - лифт по позиции (feature position).

### 2.4. Что даёт контекстная модель

- Разные **топ-листы** для разных демографий и каналов:
  - одному сегменту чаще показываем товары с ценой до X,
  - другому — премиум,
  - разным регионам — локальные товары/контент.
- Персонализацию **до первого действия**: пользователь ещё ничего не кликнул, но его контекст уже задаёт priors.
- Возможность **управлять бизнес-целями** (например, ограничить показ дорогих товаров пользователям с низкой доходностью сегмента).

Фактически это следующий слой над popular/trending fallback: вместо одного global popular мы имеем **context-conditioned popular**.

---

## 3. Persona embeddings

### 3.1. Мотивация

Иногда у нас есть информация о пользователе, даже если он ничего не успел посмотреть:

- он заполнил анкету (любимые жанры фильмов, интересы),
- у него есть биография/описание профиля,
- он пришёл из соцсети, где есть публичные данные (если это легально используется),
- он вводил поисковые запросы ещё до кликов по выдаче.

Эту информацию можно превратить в **persona embedding** — вектор, описывающий его лайфстайл/интересы.

### 3.2. Persona как текст + Encoder

Если persona-профиль можно описать текстом $persona\_text(u)$, то:

$$
e_u = \text{Encoder}(persona\_text(u)),
$$

где Encoder — текстовый эмбеддер (тот же E5/BGE/GTE, что и для объектов).

Примеры persona-текста:

- "Мужчина, 27 лет, Москва, люблю фантастику, киберпанк и документальные фильмы";
- "Девушка, 22, интересуюсь спортом, здоровым питанием и бегом".

### 3.3. Persona из структурированных фичей

Если есть структурированные признаки, можно построить persona embedding через небольшую модель:

1. Собираем фичи пользователя $f_u$ (демография, канал, ответы на анкеты).
2. Обучаем MLP/GBDT для предсказания взаимодействий (вероятность клика/лайка/покупки на item’е), а затем проецируем $f_u$ в векторную форму:

   $$
   e_u = W f_u + b \quad \text{или через последний скрытый слой MLP}.
   $$

Главное — сделать так, чтобы $e_u$ находился **в том же пространстве**, что и эмбеддинги объектов $e_i$ (например, через совместное обучение).

### 3.4. Использование persona embeddings

Если $e_u$ и $e_i$ живут в одном пространстве:

- можем сделать dense retrieval:

  $$
  score(u, i) = \cos(e_u, e_i) \quad \text{или} \quad score(u, i) = e_u^\top e_i.
  $$

- можем использовать $e_u$ как embedding-профиль для двухбашенной модели;
- можем инициализировать user embedding в CF/MF-модели значением $e_u$.

Таким образом, **cold-start user** становится не таким уж cold: у него сразу есть вектор, который можно использовать в тех же retrieval/ранжирующих моделях, что и для warm-пользователей.

### 3.5. Пример: persona + zero-shot items 

```python
import numpy as np

# заглушка для текстового энкодера

def encode(text: str) -> np.ndarray:
    rng = np.random.default_rng(abs(hash(text)) % (2**32))
    v = rng.normal(size=256)
    return v / (np.linalg.norm(v) + 1e-8)

# persona нового пользователя
persona_text = "Мужчина, 28 лет, люблю научную фантастику и киберпанк"
user_vec = encode(persona_text)

# эмбеддинги объектов (например, описания фильмов)
items = {
    1: "Фантастический фильм про космическую станцию",
    2: "Романтическая комедия в Париже",
    3: "Киберпанк-аниме про хакеров",
}

item_vecs = {i: encode(t) for i, t in items.items()}

scores = []
for i, v in item_vecs.items():
    score = float(np.dot(user_vec, v))  # косинус, если нормированы
    scores.append((score, i))

scores.sort(reverse=True)
print("top items:", scores)
```

### 3.6. Ограничения и практические моменты

**Ограничения:**

- persona может быть неполной/неточной (люди не всегда честно отвечают в анкетах);
- есть риски privacy и fairness (нельзя бездумно использовать чувствительные атрибуты);
- нужен аккуратный дизайн форм и текстов, чтобы не перегружать пользователя на старте.

**Практика:**

- часто persona-анкета делается максимально короткой (1–3 вопроса);
- часть persona можно восстанавливать из пре-истории (поисковые запросы, referer и т.п.);
- persona embedding можно потом **обновлять** по реальным взаимодействиям (смешивать с поведенческим профилем).

---

## 4. Federated / cross-domain warm-up

### 4.1. Идея cross-domain профилей

Если пользователь **не новый для экосистемы**, cold-start можно сильно смягчить:

- он раньше пользовался другим продуктом (музыка → видео, маркетплейс → финансы);
- он уже использовал мобильное приложение, а теперь зашёл на веб;
- он был авторизован в одном сервисе, а потом перешёл в другой.

Тогда у нас может быть:

- эмбеддинг пользователя $e_u^{(A)}$ в домене $A$ (старый продукт),
- эмбеддинг пользователя $e_u^{(B)}$ в домене $B$ (новый продукт), который мы хотим инициализировать.

### 4.2. Перенос эмбеддинга между доменами

Простейшая линейная карта между пространствами:

$$
 e_u^{(B)} \approx W e_u^{(A)} + b.
$$

Если есть пользователи, которые пользуются обоими продуктами, можно обучить $W, b$ по задаче регрессии/ранжирования:

- минимизировать $\lVert e_u^{(B)} - (W e_u^{(A)} + b) \rVert^2$ для пользователей, у которых есть оба embedding'а;
- либо обучать $W$ end-to-end вместе с моделью рекомендаций в домене $B$.

Тогда для нового пользователя в домене $B$ с известным $e_u^{(A)}$:

1. Считаем $\tilde e_u^{(B)} = W e_u^{(A)} + b$;
2. Используем $\tilde e_u^{(B)}$ как стартовый embedding в retrieval/ранжировании.

### 4.3. Federated / on-device warm-up

В федеративном сценарии:

- часть истории пользователя и обучение user embedding происходит **на устройстве** (on-device);
- в центральный сервер отправляются не сырые данные, а агрегированные градиенты или уже готовые эмбеддинги.

Для нового устройства/клиента:

- можно загрузить предварительный embedding (из облака) и дообучать его локально;
- при этом сохраняется приватность: сырые клики/история не покидают устройство.

С точки зрения cold-start user это выглядит так:

1. Пользователь ставит новое приложение, но уже имеет on-device историю (например, от старой версии или другого приложения экосистемы).
2. Локальный профиль быстро восстанавливается и синхронизируется с сервером.
3. Даже на первом экране рекомендации уже почти такие же, как у warm user.

### 4.4. Bias и адаптация между доменами

Важно понимать, что разные домены могут иметь разные распределения:

- пользователь, слушающий тяжёлую музыку, не обязательно любит тяжёлое кино;
- покупатель детских товаров не обязательно будет интересоваться детским контентом.

Поэтому cross-domain перенос нужно:

- калибровать (например, домножать на коэффициенты уверенности),
- возможно, ограничивать по времени ("предполагаем похожесть вкусов только по последним X месяцам"),
- адаптировать с учётом бизнес-логики (не все сигналы одинаково важны).

---

## 5. Как слои складываются в реальный пайплайн

Реальный пайплайн для **cold-start users** обычно многоуровневый:

1. **Самый первый экран:**
   - popular/trending fallback (global + segment),
   - возможно, слегка кастомизированный по демографии/каналу.

2. **Если есть контекстные признаки:**
   - запускается контекстная модель $score(u, i) = f(context(u), features(i))$;
   - формируются разные топ-листы для разных сегментов ещё до первого клика.

3. **Если есть persona-информация:**
   - строится persona embedding $e_u$ (из текста/анкеты/фичей);
   - используется dense retrieval по $e_u$ и эмбеддингам объектов.

4. **Если есть cross-domain/federated данные:**
   - подгружается/переносится user embedding из другого домена или устройства;
   - он служит стартовой точкой, пока новая система набирает собственные сигналы.

5. **Переход в warm-зону:**
   - по мере накопления кликов/просмотров пользователь переходит в обычный режим персонализации (two-tower, MF, LightGCN, сложный rerank);
   - cold-start логика постепенно выключается или уменьшает влияние.
