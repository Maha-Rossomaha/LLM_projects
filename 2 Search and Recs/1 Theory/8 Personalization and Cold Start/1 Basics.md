# Персонализация и cold-start

## 0. Обобщение

Персонализация в рекомендательных системах — это переход от общего «топ продаж» к ответу на вопрос:

> *Что именно этому конкретному пользователю имеет смысл показать сейчас, с учётом его вкусов, контекста и бизнес-целей?*

Типичные бизнес-цели:

* увеличить CTR (click-through rate), конверсию в покупку/просмотр,
* увеличить удержание (возвращаемость, время в сервисе),
* сделать рекомендации осмысленными: меньше «мусора», больше релевантного.

При этом сразу возникает проблема **cold-start** и вообще разреженности данных:

* **cold-item**: новый товар/документ никто ещё не смотрел → нет поведенческих сигналов;
* **cold-user**: новый пользователь ещё ничего не сделал → непонятны вкусы;
* **sparse**: даже у старых пользователей/товаров взаимодействий мало, матрица user×item разреженная.

---

## 1. Формальная постановка задачи рекомендаций

Пусть есть множество пользователей $U$, множество объектов (товаров, фильмов, постов) $I$.

Взаимодействия можно представить как матрицу

$$R 
    \in \mathbb{R}^{|U| \times |I|},
$$

где элемент $r_{u,i}$ — какой-то сигнал интереса пользователя $u$ к объекту $i$:

* явный рейтинг (звёзды 1–5),
* бинарный клик (0/1),
* просмотр с dwell-time,
* покупка/добавление в корзину.

**Задача ранжирования**: для каждого пользователя $u$ и текущего контекста $c$ (время, платформа, сессия) выбрать подмножество объектов $I_u \subset I$ и упорядочить их так, чтобы максимизировать заданную метрику качества (обычно CTR, конверсия или суррогатные офлайн-метрики вроде nDCG@K).

Простейшие метрики:

* $\text{CTR} = \frac{\text{кол-во кликов}}{\text{кол-во показов}};$
* $\text{CR} = \frac{\text{кол-во покупок}}{\text{кол-во показов}};$
* retention: доля пользователей, вернувшихся через $D$ дней.

---

## 2. Базовый неперсонализированный baseline: «топ продаж»

Перед персонализацией полезно понимать, с чем мы сравниваемся.

### 2.1. Global popular / топ продаж

Идея: показывать всем один и тот же список — самые популярные объекты по:

* числу показов,
* CTR,
* конверсии в покупку.

#### Пример:

```python
from collections import Counter

# interactions: список (user_id, item_id, clicked)
interactions = [
    (1, 42, 1), (1, 43, 0), (2, 42, 1), (3, 100, 1),
]

shows = Counter()
clicks = Counter()

for _, item_id, clicked in interactions:
    shows[item_id] += 1
    clicks[item_id] += clicked

ctr = {item: clicks[item] / shows[item] for item in shows}

# топ-10 по CTR с минимальным числом показов >= 100
MIN_SHOWS = 100
popular_items = [
    item for item, _ in sorted(
        ctr.items(), key=lambda x: x[1], reverse=True
    ) if shows[item] >= MIN_SHOWS
][:10]
```

**Плюсы:**

* почти не страдает от cold-start user (новому пользователю тоже можно что-то показать);
* очень дешёвая в онлайне, легко кэшируется.

**Минусы:**

* нет персонализации вкусов;
* усиливает популярность (popularity bias), может игнорировать «длинный хвост».

Поэтому персонализация часто строится как *надстройка* над global popular: мы начинаем с сильного baseline и постепенно подмешиваем персональный сигнал.

---

## 3. Классические подходы к персонализации (без нейросетей)

### 3.1. Rule-based и сегментация

Самый простой уровень персонализации — **бизнес-правила**:

* сегментация по демографии (пол, возраст, регион),
* сегментация по источнику трафика (органика/реклама),
* сегментация по устройству (мобильный/десктоп).

Для каждого сегмента можно хранить свои топ-листы:

* топ глобальных рекомендаций для мужчин 25–34 в Москве,
* топ для пользователей, пришедших с конкретной рекламной кампании и т.д.

Это уже частично решает **cold-start user**: даже без его истории мы попадаем в сегмент и выдаём «средний вкус» этого сегмента.

### 3.2. Content-based рекомендации

Идея: описать каждый объект признаками $x_i$ (категория, цена, бренд, embedding текста описания, embedding картинки) и строить профиль пользователя как функцию от просмотренных/купленных объектов.

Простейший вариант: профиль пользователя — среднее эмбеддингов просмотренных им объектов:

$$
\mathbf{p}_u = \frac{1}{|I_u|} \sum_{i \in I_u} \mathbf{e}_i,
$$

где $\mathbf{e}_i$ — вектор признаков/эмбеддинг объекта $i$, а $I_u$ — множество объектов, с которыми взаимодействовал пользователь.

Скор объекта для пользователя:

$$
    \text{score}(u, i) = \langle \mathbf{p}_u, \mathbf{e}_i \rangle \quad \text{или} \quad \cos(\mathbf{p}_u, \mathbf{e}_i).
$$

Плюсы:

* естественно решает **cold-start item**, если у нового объекта есть признаки $x_i$ (описание, категория),
* можно использовать мощные эмбеддеры текста/картинок.

Минусы:

* не использует коллективную информацию о похожести пользователей между собой,
* качество сильно зависит от качества признаков.

### 3.3. Collaborative Filtering (CF)

Collaborative filtering использует саму структуру матрицы взаимодействий $R$.

#### User-based CF

Для пользователя $u$ ищем похожих пользователей $v$, например по косинусной похожести их строк в $R$:

$$
    \text{sim}(u, v) = \cos(R_u, R_v).
$$

Затем рекомендуем объекты, которые популярны у похожих пользователей и которых нет у $u$.

#### Item-based CF

Аналогично, но строим похожесть между объектами по столбцам $R$:

$$
    \text{sim}(i, j) = \cos(R^i, R^j).
$$

Тогда рекомендации пользователю $u$ — это «объекты, похожие на те, с которыми он взаимодействовал».

Плюсы CF:

* хорошо использует коллективные паттерны («пользователи, похожие на тебя, купили X»),
* часто работает лучше простого content-based, если данных много.

Минусы:

* плохо справляется с **cold-start** (для новых пользователей и объектов нет строк/столбцов в $R$),
* чувствителен к разреженности.

### 3.4. Matrix Factorization (MF)

Идея: представить пользователей и объекты через скрытые факторы (эмбеддинги), чтобы матрица рейтингов аппроксимировалась их скалярными произведениями:

$$
    R \approx P Q^\top,
$$

где $P \in \mathbb{R}^{|U| \times d}$ — матрица эмбеддингов пользователей, $Q \in \mathbb{R}^{|I| \times d}$ — матрица эмбеддингов объектов.

Типичная функция потерь (явные рейтинги):

$$
    \min_{P, Q} \sum_{(u,i) \in \mathcal{O}} (r_{u,i} - \langle p_u, q_i \rangle)^2 + \lambda_U |p_u|^2 + \lambda_I |q_i|^2.
$$

Для implicit-feedback (клики, просмотры) используются варианты типа ALS, BPR и т.п.

Плюсы MF:

* хорошо работает при разреженных данных (если их всё-таки достаточно в сумме),
* даёт компактные вектора пользователей и объектов, которые можно использовать дальше (в two-tower, LTR и т.п.).

Минусы:

* всё ещё страдает от cold-start (для новых пользователей/объектов эмбеддинги не обучены),
* плохо использует сложные признаки (категории, текст) без явной интеграции признаков в модель.

---

## 4. Типы cold-start и разреженность матрицы

### 4.1. Cold-start item

Новый товар/документ только что добавлен; нет ни одного взаимодействия:

* в CF-методах столбец в $R$ пустой,
* в MF нет обученного эмбеддинга $q_i$.

### 4.2. Cold-start user

Новый пользователь:

* нет истории, строка $R_u$ пустая,
* нет эмбеддинга $p_u$.

### 4.3. Sparse regime

Даже для старых (u,i) наблюдений мало:

* плотность матрицы низкая (например, 0.1% ненулей и меньше),
* многие пары (u,i) не наблюдались никогда, невозможно оценить интерес напрямую.

Разреженность приводит к:

* нестабильности оценок похожести (user-based, item-based CF),
* переобучению MF и других моделей без правильной регуляризации,
* необходимости агрессивно использовать priors и сглаживание.

---

## 5. Cold-start для объектов (cold-item)

### 5.1. Meta-features и content-based модели

Если у нового объекта есть признаки (категория, бренд, текст описания, изображение), то вместо чисто коллаборативной модели можно построить модель:

$$
\hat{y}_{u,i} = f(\text{features}(u), \text{features}(i)),
$$

где $f$, например, логистическая регрессия или градиентный бустинг.

Пример фичей:

* признаки пользователя: возраст, регион, платёжеспособность,
* признаки товара: категория, цена, бренд, embedding описания.

Новый объект можно сразу встроить в систему — его фичи известны, даже если нет кликов.

### 5.2. Zero-shot эмбеддинги (E5/BGE-M3 и др.)

Если у нас есть мощный текстовый эмбеддер, можно получать эмбеддинг нового объекта напрямую из текста:

$$
\mathbf{e}_i = \text{Encoder}(\text{title}_i, \text{description}_i).
$$

Такие эмбеддинги:

* позволяют находить похожие объекты по смыслу даже без кликов,
* используются как стартовые $q_i$ в MF/two-tower,
* помогают «заполнить» пространство объектов для новых айтемов.

### 5.3. Графовый прогон (graph propagation)

Можно построить граф $G = (V, E)$, где вершины — пользователи и объекты, а рёбра — взаимодействия.

Для новых объектов можно:

* связать их с похожими объектами по контенту (edge item–item),
* использовать алгоритмы типа label/embedding propagation, LightGCN.

Интуиция: если новый объект похож на старый по контенту, то часть «сигнала» от старого объекта можно перекинуть на новый.

### 5.4. Explore–exploit для новых товаров

Даже при хороших priors для нового товара нужно **набрать реальные клики**.

Типичные практики:

* временный **boost** новых товаров в выдаче,
* bandit-алгоритмы ($\varepsilon$-greedy, UCB, Thompson Sampling) для выбора, какие новые товары показывать и кому,
* учёт стоимости exploration: не убить общий CTR.

---

## 6. Cold-start для пользователей (cold-user)

### 6.1. Popular-fallback

Новый пользователь ещё ничего не сделал. Базовый подход:

* показать global popular;
* чуть лучше: популярное внутри его сегмента (по региону, возрасту, источнику трафика).

Формула сглаживания для вероятности показа объекта $i$ сегменту $s$:

$$
P(i \mid s) = \lambda P(i \mid s, \text{data}) + (1 - \lambda) P(i \mid \text{global}),
$$

где $0 \leq \lambda \leq 1$ — степень доверия сегментным данным.

Это решает cold-start user, но не даёт индивидуальности.

### 6.2. Onboarding: опросники и явные сигналы

Можно спросить пользователя при регистрации:

* какие жанры фильмов он любит,
* какие категории товаров интересны,
* какой у него опыт (новичок/профи).

На основе ответов формируется стартовый профиль $p_u$, который потом уточняется по поведению.

### 6.3. Session-based персонализация

Даже если у пользователя нет долгосрочной истории, есть **текущая сессия**:

* что он уже кликнул за последние N минут,
* какие запросы вводил.

Можно строить рекомендации только из краткосрочного контекста (session-based recsys), используя:

* content-based по объектам текущей сессии,
* простые RNN/Transformer-модели по последовательности кликов.

Это позволяет частично персонализировать выдачу **даже без сохранения долгосрочного профиля**.

### 6.4. Federated warm-up и перенос профиля

В реальных продуктах возможен перенос профиля:

* между устройствами (авторизация по аккаунту),
* между сервисами одного холдинга (например, маркетплейс ↔ медиасервис),
* через on-device обучение (federated learning) с передачей обобщённых градиентов, а не сырых данных.

Это всё способы избежать абсолютного cold-start user, если пользователь уже знаком с экосистемой.

---

## 7. Работа с разреженностью (sparse regime)

Даже без жёсткого cold-start взаимодействий может быть мало. Основные техники:

1. **Регуляризация** моделей ($L_2$, ранговые потери с margin) для MF и нейросетей.

2. **Сглаживание popularity**: байесовские priors на CTR/конверсию:

   $$
   \hat{p}_i = \frac{\text{clicks}_i + \alpha \cdot p_\text{global}}{\text{shows}_i + \alpha},
   $$

   где $\alpha$ — «масса» глобального prior.

3. **Кластеризация пользователей/товаров**: вместо индивидуальных параметров для каждого (u,i) учим общие факторы для кластеров.

4. **Shared представления**: two-tower и LightGCN, где эмбеддинги обучаются общей моделью и разделяют статистику между многими (u,i).

---

## 8. Two-tower модели

Two-tower (двухбашенные) модели — стандарт для персонализированного **retrieval**.

### 8.1. Архитектура

Есть две сети:

* user-tower: получает признаки пользователя (id, демография, история), выдаёт вектор $\mathbf{u}_u$,
* item-tower: получает признаки объекта (id, категория, текст и т.п.) и выдаёт вектор $\mathbf{v}_i$.

Скор:

$$
\text{score}(u, i) = \langle \mathbf{u}_u, \mathbf{v}_i \rangle \quad \text{или} \quad \cos(\mathbf{u}_u, \mathbf{v}_i).
$$

В онлайне:

1. Эмбеддинги $\mathbf{v}_i$ всех объектов предрассчитаны и лежат в ANN-индексе.
2. Для запроса вычисляется $\mathbf{u}_u$.
3. Ищем ближайшие $\mathbf{v}_i$ по косинусу — получаем кандидатов.

### 8.2. Обучение

Часто используется задача бинарной классификации «было взаимодействие / не было»:

* положительные пары $(u, i^+)$: реальные взаимодействия,
* отрицательные $(u, i^-)$: сэмплируются (random negatives, hard negatives).

Функция потерь (BCE):

$$
\mathcal{L} = -\sum_{(u,i,y)} \bigl[y \log \sigma(\langle \mathbf{u}_u, \mathbf{v}_i \rangle) + (1-y) \log (1-\sigma(\langle \mathbf{u}_u, \mathbf{v}_i \rangle))\bigr].
$$

Two-tower хорошо масштабируются и:

* дают персонализацию по user-эмбеддингу,
* используют признаки объектов, что помогает с **cold-item**,
* позволяют быстро искать кандидатов через ANN.

### 8.3. Простой пример two-tower на PyTorch

```python
import torch
import torch.nn as nn

class TwoTower(nn.Module):
    def __init__(self, num_users: int, num_items: int, dim: int = 64):
        super().__init__()
        self.user_emb = nn.Embedding(num_users, dim)
        self.item_emb = nn.Embedding(num_items, dim)

    def encode_user(self, user_ids: torch.LongTensor) -> torch.Tensor:
        return self.user_emb(user_ids)

    def encode_item(self, item_ids: torch.LongTensor) -> torch.Tensor:
        return self.item_emb(item_ids)

    def forward(self, user_ids: torch.LongTensor, item_ids: torch.LongTensor) -> torch.Tensor:
        u = self.encode_user(user_ids)
        v = self.encode_item(item_ids)
        # скалярное произведение как логит
        logits = (u * v).sum(dim=-1)
        return logits

# Пример одного шага обучения
model = TwoTower(num_users=10_000, num_items=100_000, dim=64)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.BCEWithLogitsLoss()

batch_user_ids = torch.randint(0, 10_000, (1024,))
batch_pos_item_ids = torch.randint(0, 100_000, (1024,))
# для простоты считаем, что всё это положительные примеры
labels = torch.ones(1024)

logits = model(batch_user_ids, batch_pos_item_ids)
loss = criterion(logits, labels)
loss.backward()
optimizer.step()
```

Для реального обучения нужны отрицательные примеры и продуманный sampling.

---

## 9. LightGCN и графовая симметрия user×item

LightGCN — лёгкая графовая модель для коллаборативной фильтрации.

Идея:

1. Есть двудольный граф user–item.

2. Начальные эмбеддинги пользователей и объектов — $p_u^{(0)}, q_i^{(0)}$.

3. На каждом слое $k$ эмбеддинги обновляются как среднее эмбеддингов соседей (без сложных MLP):

   $$
   p_u^{(k+1)} = \sum_{i \in N(u)} \frac{1}{\sqrt{|N(u)| |N(i)|}} q_i^{(k)},
   $$

   $$
   q_i^{(k+1)} = \sum_{u \in N(i)} \frac{1}{\sqrt{|N(u)| |N(i)|}} p_u^{(k)}.
   $$

4. Итоговый эмбеддинг — взвешенная сумма слоёв:

   $$
   p_u = \sum_{k=0}^K \alpha_k p_u^{(k)}, \quad q_i = \sum_{k=0}^K \alpha_k q_i^{(k)}.
   $$

LightGCN хорошо работает в симметричных user×item задачах:

* учитывает структуру графа,
* распределяет сигнал от активных пользователей к соседним объектам и наоборот,
* частично помогает при разреженности и для слабо наблюдаемых объектов.

Однако чистый LightGCN по id-шникам сам по себе тоже страдает от жёсткого cold-start (для совсем новых узлов нет рёбер). Обычно его комбинируют с признаками и content-based подходами.

---

## 10. Типовые пайплайны персонализации

### 10.1. E-commerce (маркетплейс)

1. **Кандидатный слой (retrieval):**

   * two-tower на user/item id + простые признаки;
   * boosting для новых товаров (explore).
2. **Фильтрация:**

   * правила наличия на складе, регионы доставки, цена.
3. **Реранжирование:**

   * градиентный бустинг с большим набором фичей: CTR пользователя, конверсия товара, свежесть, дисконт, позиция, device.
4. **Онлайн-обновление:**

   * обновление user-профиля при каждом клике/покупке,
   * агрегаты по товарам пересчитываются batch’ами.

**Cold-item:**

* используем категорию, цену, текст описания,
* zero-shot эмбеддинги для первого размещения в пространстве,
* временный boost и bandits для сбора статистики.

**Cold-user:**

* popular-fallback по региону и маркетинговому сегменту,
* onboarding опросник («какие категории интересуют?»),
* быстрый учёт первых кликов (session-based персонализация).

### 10.2. Новости / контентная лента

1. **Retrieval:**

   * two-tower на эмбеддингах пользователя (история прочитанного) и статьи (текстовый embedding),
   * ограничение по свежести.
2. **Реранжирование:**

   * модель, учитывающая позицию в ленте, время публикации, device, source.
3. **Онлайн:**

   * сильный фокус на коротком контексте (последние N прочитанных материалов).

Cold-start здесь особенно болезненен:

* новые статьи должны сразу попадать пользователям,
* новые пользователи приходят с рекламных кампаний, и по ним часто есть хотя бы сегментная информация.

---

## 11. Простые примеры

### 11.1. Popular-fallback для нового пользователя

```python
from collections import defaultdict, Counter

# interactions: (user_id, item_id, clicked)
interactions = [
    (1, 10, 1), (1, 11, 1), (2, 10, 1), (3, 12, 1), (3, 13, 0),
]

user_segment = {
    1: "moscow",
    2: "moscow",
    3: "spb",
}

shows_global = Counter()
clicks_global = Counter()

shows_segment = defaultdict(Counter)
clicks_segment = defaultdict(Counter)

for u, i, c in interactions:
    seg = user_segment.get(u, "unknown")
    shows_global[i] += 1
    clicks_global[i] += c
    shows_segment[seg][i] += 1
    clicks_segment[seg][i] += c

alpha = 20  # сила глобального prior

def smoothed_ctr(clicks, shows, item):
    p_global = clicks_global[item] / shows_global[item] if shows_global[item] > 0 else 0.0
    return (clicks[item] + alpha * p_global) / (shows[item] + alpha)


def recommend_for_new_user(segment: str, k: int = 10):
    shows_seg = shows_segment.get(segment, Counter())
    clicks_seg = clicks_segment.get(segment, Counter())

    items = set(shows_global.keys())
    scored = []
    for i in items:
        ctr_i = smoothed_ctr(clicks_seg, shows_seg, i)
        scored.append((ctr_i, i))

    scored.sort(reverse=True)
    return [i for _, i in scored[:k]]

print(recommend_for_new_user("moscow"))
```

Здесь:

* для нового пользователя из сегмента `moscow` будет построен список рекомендаций,
* при этом используется сглаживание глобальным prior, чтобы не переобучаться на малом количестве показов в сегменте.

### 11.2. Обновление user-профиля по контенту

Предположим, что у нас есть предрасчитанные эмбеддинги объектов (`item_embeddings`), и мы хотим делать простую content-based персонализацию:

```python
import numpy as np

# item_id -> np.array(dim)
item_embeddings = {
    10: np.random.randn(32),
    11: np.random.randn(32),
    12: np.random.randn(32),
}

user_history = {
    1: [10, 11],
    2: [10],
    3: [12],
}


def build_user_profile(user_id: int) -> np.ndarray:
    items = user_history.get(user_id, [])
    if not items:
        return np.zeros(32)
    emb = np.stack([item_embeddings[i] for i in items])
    profile = emb.mean(axis=0)
    # нормируем вектор для косинусной похожести
    norm = np.linalg.norm(profile) + 1e-8
    return profile / norm


def recommend_from_profile(user_id: int, candidate_items: list[int], k: int = 10):
    profile = build_user_profile(user_id)
    scores = []
    for i in candidate_items:
        v = item_embeddings[i]
        v = v / (np.linalg.norm(v) + 1e-8)
        score = float(np.dot(profile, v))
        scores.append((score, i))
    scores.sort(reverse=True)
    return [i for _, i in scores[:k]]

print(recommend_from_profile(1, [10, 11, 12]))
```

Такой профиль пользователя можно комбинировать с two-tower, MF, графовыми моделями и использовать как часть признаков.
