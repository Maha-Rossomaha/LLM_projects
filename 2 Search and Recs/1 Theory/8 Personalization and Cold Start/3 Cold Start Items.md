# Cold-start items 

## 0. Описание задачи: новые объекты

**Cold-start item** — это ситуация, когда в систему добавлен новый объект (товар, фильм, видео, статья), но **по нему ещё нет поведенческих сигналов**:

- нет просмотров,
- нет кликов,
- нет лайков/дизлайков,
- нет покупок.

Формально:

- есть множество пользователей $U$ и множество объектов $I$;
- есть матрица взаимодействий $R \in \mathbb R^{|U| \times |I|}$, где $r_{u,i}$ — сигнал интереса пользователя $u$ к объекту $i$;
- для нового объекта $i_{\text{new}}$ все значения $r_{u, i_{\text{new}}}$ неизвестны.

Бизнес-проблема:

1. Новый объект нужно **как-то показать**, иначе он навсегда останется «в подвале». Но любые показы без сигнала → риск убить CTR/конверсию.
2. Хочется делать это **умно**: показывать его тем пользователям, которым он потенциально интересен, а не просто «всем подряд».

Дальше пойдём по трём уровням:

1. **Meta-features и контентка как первый слой** — что делать, когда про объект известен только его контент и мета-информация.
2. **Zero-shot текстовые эмбеддинги** — как использовать мощные универсальные эмбеддеры (E5, BGE-M3, GTE и др.), чтобы сразу помещать новый объект в общее векторное пространство.
3. **Graph propagation на user×item графе** — как по мере появления первых взаимодействий постепенно «втягивать» новый объект в коллективный граф и улучшать его представление.

---

## 1. Meta-features и контентка как первый слой

### 1.1. Представление нового объекта через мета-признаки

Почти всегда у нового объекта есть **метаданные**, даже если нет ни одного просмотра:

- категория (например, "смартфоны", "кроссовки", "боевики"),
- теги ("для новичков", "скидка", "зима"),
- цена, скидка, бренд,
- текстовое описание (название, аннотация),
- изображение (обложка товара, постера фильма),
- технические параметры (характеристики товара).

Мы строим для объекта вектор признаков:

$$
features(i) = x_i \in \mathbb R^d.
$$

Это может быть конкатенация всего подряд:

- one-hot / multi-hot категорий и тегов,
- нормализованная цена,
- embedding текста (TF-IDF, BM25, либо уже плотный эмбеддинг),
- embedding картинки.

### 1.2. Профиль пользователя и контентный скор

У пользователя есть история взаимодействий $I_u = \{i_1, \dots, i_k\}$.

Строим **профиль пользователя** как функцию от просмотренных/купленных объектов. Простейший вариант:

$$
profile(u) = p_u = \text{avg}(v_{i_1}, \dots, v_{i_k}),
$$

где $v_{i_j}$ — вектор контентных признаков или эмбеддинг объекта $i_j$.

Можно сделать взвешенное среднее:

$$
p_u = \frac{\sum_{j=1}^k w_{a_j} v_{i_j}}{\sum_{j=1}^k w_{a_j}},
$$

где $a_j$ — тип взаимодействия (просмотр, лайк, покупка), а $w_{a_j}$ — вес этого типа (например, покупка важнее просмотра).

**Контентный скор** для пары $(u, i)$:

$$
score(u, i) = f\bigl(profile(u), features(i)\bigr).
$$

В простейшем виде / baseline:

- $f$ — косинусная похожесть между $p_u$ и $v_i$:

  $$
  score(u, i) = \cos(p_u, v_i) = \frac{\langle p_u, v_i \rangle}{\lVert p_u \rVert \, \lVert v_i \rVert},
  $$

- или линейная/логистическая модель: $score = w^\top z_{u,i}$, где $z_{u,i}$ — конкатенация признаков пользователя и объекта,
- или GBDT-модель (CatBoost/XGBoost/LightGBM) поверх богатых признаков.

### 1.3. Пример: модель CTR с meta-features

Пусть цель — предсказать вероятность клика на карточку товара. На уровне логистической регрессии:

- признаки пользователя $f_u$ (регион, возрастная группа, сегмент);
- признаки товара $f_i$ (категория, цена, бренд, текстовые фичи);
- признаки контекста $f_c$ (позиция в выдаче, платформа, время суток).

Тогда для пары $(u, i, c)$ строим общий вектор признаков:

$$
z_{u,i,c} = [f_u, f_i, f_c],
$$

и обучаем модель:

$$
P(\text{click}=1 \mid u, i, c) = \sigma(w^\top z_{u,i,c}),
$$

где $\sigma$ — сигмоида.

**Важно:** новый объект $i_{\text{new}}$ сразу получает вектор $f_{i_{\text{new}}}$ из мета-признаков → его можно подать в ту же модель, даже если по нему нет истории.

### 1.4. Реальные примеры

- **Маркетплейс**: новый товар с категорией "зимние ботинки", брендом, ценой и описанием. Уже можно таргетировать его на пользователей, интересовавшихся похожими категориями, или на сегмент "зимняя обувь".
- **Видео-сервис**: новый ролик с жанром, актёрами, временем и описанием. Для пользователей, любящих похожий жанр, можно поднять его в выдаче.
- **Новостной сайт**: новая статья с темой и рубрикой. Можно сразу показать её тем, кто много читает эту рубрику.

### 1.5. Плюсы и минусы meta-feature слоя

**Плюсы:**

- не страдает от cold-start items: как только есть метаданные, есть и признаки;
- легко объясним бизнесу (категории, бренд, цена — понятные факторы);
- гибко: можно добавлять новые признаки без переизобретения модели.

**Минусы:**

- качество ограничено тем, насколько метаданные отражают «реальный» вкус;
- не учитывает коллективные паттерны (совместные покупки, сложные связи между товарами);
- плохо ловит эффекты популярности, сезонности и экосистемные зависимости без дополнительных признаков.

На практике meta-feature контентка — это **первый слой**, который обеспечивает:

- базовую адекватность для всех новых объектов,
- неплохой recall в зоне cold-start,
- материал для более сложных моделей (эмбеддинги, графы, hybrid CF).

---

## 2. Zero-shot текстовые эмбеддинги (E5/BGE-M3 и подобные)

Meta-features хороши, но часто **текстовое описание** объекта содержит намного больше смысла, чем ручные признаки.

Современные текстовые эмбеддеры (E5, BGE-M3, GTE и др.) позволяют:

- кодировать тексты в вектора фиксированной размерности $e \in \mathbb R^d$;
- так, что **семантически похожие** тексты оказываются близко в этом пространстве.

### 2.1. Эмбеддинг объекта

Для нового объекта $i$ берём весь доступный текст:

- заголовок,
- описание,
- список характеристик.

Объединяем в строку и подаём в Encoder:

$$
e_i = \text{Encoder}(\text{text}_i).
$$

Даже если по этому объекту **нет ни одного клика**, он уже располагается в том же пространстве, что и все остальные объекты.

### 2.2. Эмбеддинг пользователя

Вариант 1: агрегат по истории взаимодействий (classic).

Пусть пользователь $u$ взаимодействовал с объектами $i_1, \dots, i_k$ с эмбеддингами $e_{i_j}$. Тогда:

$$
e_u = \text{agg}(e_{i_1}, \dots, e_{i_k}),
$$

где $\text{agg}$ — среднее, взвешенное среднее, attention-пулинг и т.п.

Простой вариант:

$$
e_u = \frac{1}{k} \sum_{j=1}^k e_{i_j}.
$$

Вариант 2: text-persona.

Если пользователь заполнил профиль/биографию, есть его запросы, интересы в текстовом виде, можно прямо закодировать это:

$$
e_u = \text{Encoder}(\text{persona}_u).
$$

Пример: "Мужчина, 28 лет, живу в Москве, люблю фантастику, ужасы и документальные фильмы про космос".

### 2.3. Рекомендации через dense retrieval

Идея: использовать **поисковую парадигму** (dense retrieval) для рекомендаций.

1. Для всех объектов считаем эмбеддинги $e_i$ и складываем их в ANN-индекс (FAISS, HNSW, ScaNN и т.д.).
2. Для пользователя считаем вектор $e_u$.
3. Ищем ближайшие по косинусной похожести или dot-product объекты:

   $$
   score(u, i) = \cos(e_u, e_i) \quad \text{или} \quad score(u, i) = e_u^\top e_i.
   $$

4. Берём топ-$K$ nearest neighbours.

Ключевой момент: **новый объект** с эмбеддингом $e_{i_{\text{new}}}$ сразу попадает в этот индекс → его можно рекомендовать zero-shot.

### 2.4. Почему это работает для cold-start

1. Эмбеддер (E5/BGE-M3) обучен на огромных корпусах и задаче похожести текстов.
2. Даже если в нашем домене объект новый, его описание обычно содержит слова, фразы и паттерны, которые встречались в обучающем корпусе.
3. Поэтому его эмбеддинг:
   - близок к эмбеддингам похожих объектов (по смыслу),
   - автоматически наследует их "окружение" в векторном пространстве.

Для пользователя, любящего объекты с похожими описаниями, новый item окажется **геометрически близок** без единого клика.

### 2.5. Практический пример (скелет кода)

```python
import numpy as np

# Псевдо-энкодер, реально тут будет вызов модели (E5/BGE и т.п.)
def encode(text: str) -> np.ndarray:
    # заглушка
    rng = np.random.default_rng(abs(hash(text)) % (2**32))
    v = rng.normal(size=768)
    return v / (np.linalg.norm(v) + 1e-8)

# Новый товар
desc_new = "Зимние мужские ботинки из натуральной кожи, утеплённые, подошва нескользящая"
e_new = encode(desc_new)

# История пользователя (описания просмотренных товаров)
user_history_texts = [
    "Мужские зимние кроссовки с утеплителем",
    "Высокие кожаные ботинки для города",
]

embs = np.stack([encode(t) for t in user_history_texts])
e_user = embs.mean(axis=0)

score = float(np.dot(e_user, e_new))  # косинус, т.к. мы нормировали
print("similarity score =", score)
```

В реальной системе вместо случайного `encode` — вызов модели, а поиск делается по ANN-индексу, а не по линейному перебору.

### 2.6. Плюсы и ограничения zero-shot эмбеддингов

**Плюсы:**

- новый объект сразу имеет репрезентацию в том же пространстве, что и старые;
- можно объединить поиск и рекомендации в одну архитектуру dense retrieval;
- легко работает для текстовых доменов (новости, статьи, описания товаров).

**Минусы:**

- если описание плохое/скупое, эмбеддинг будет шумным;
- если домен очень специфический (узкая техническая область), без дообучения модель может не ловить тонкие различия;
- эмбеддинг не знает о **поведенческих** особенностях (популярность, сезонность) без дополнительного fine-tuning или гибридного подхода.

Обычно zero-shot эмбеддинги — это **второй слой** над meta-features:

1. Сначала есть просто признаки категорий/цен/брендов.
2. Добавляем мощный embedding-текст → гораздо более качественное пространство похожести.
3. Позже можно дообучить этот эмбеддер под свои сигналы (fine-tuning).

---

## 3. Graph propagation на user×item графе

### 3.1. Двудольный граф взаимодействий

Рассмотрим граф $G = (V, E)$, где:

- $V = U \cup I$ — множество вершин (пользователи и объекты),
- $E$ — рёбра «пользователь ↔ объект» с весами, отражающими силу взаимодействия (просмотр, лайк, покупка).

Это **двудольный граф**: рёбра соединяют только пользователей с объектами, но не пользователей друг с другом и не объекты между собой (эти связи могут быть добавлены отдельно, но базовый user×item граф — двудольный).

### 3.2. Идея пропагации (message passing)

Интуиция:

- пользователи похожи, если взаимодействуют с похожими объектами;
- объекты похожи, если их выбирают похожие пользователи.

Мы хотим **распространять (propagate) сигнал** по графу:

- от активных пользователей к объектам,
- от популярных/характерных объектов обратно к пользователям.

LightGCN-подобная идея (без ухода в детали архитектуры):

1. У каждого пользователя $u$ и объекта $i$ есть начальные векторы (эмбеддинги):

   $$
   h_u^{(0)}, \quad h_i^{(0)}.
   $$

   Для объектов это могут быть как раз **контентные эмбеддинги** (из meta-features и текста).

2. На каждом шаге $k$ мы обновляем представления как **среднее по соседям**:

   $$
   h_u^{(k+1)} = \sum_{i \in N(u)} w_{u,i} \; h_i^{(k)},
   $$

   $$
   h_i^{(k+1)} = \sum_{u \in N(i)} w_{u,i} \; h_u^{(k)},
   $$

   где $N(u)$ — соседи пользователя (объекты, с которыми он взаимодействовал), $N(i)$ — соседи объекта (пользователи), а $w_{u,i}$ — веса (обычно нормировки по степеням).

3. Итоговый embedding — комбинация слоёв:

   $$
   h_u = \sum_{k=0}^K \alpha_k h_u^{(k)}, \quad h_i = \sum_{k=0}^K \alpha_k h_i^{(k)},
   $$

   где $\alpha_k$ — веса слоёв.

4. Скор, как и раньше:

   $$
   score(u, i) = \langle h_u, h_i \rangle \quad \text{или} \quad \cos(h_u, h_i).
   $$

В чистом LightGCN все эти шаги обучаются end-to-end с BPR-лоссом; здесь важно **понимание идеи**, а не подробности.

### 3.3. Как это помогает cold-item

Рассмотрим новый объект $i_{\text{new}}$.

1. **На старте**:
   - у него есть только контентный embedding $h_{i_{\text{new}}}^{(0)}$, построенный из meta-features и текста;
   - в графе нет рёбер (никто не взаимодействовал).

   В этот момент мы можем использовать его как в контентной модели (см. раздел 1–2).

2. **После первых взаимодействий**:
   - появляются рёбра $(u_1, i_{\text{new}}), (u_2, i_{\text{new}}), \dots$;
   - теперь на шагах пропагации $h_{i_{\text{new}}}^{(k+1)}$ будет получать информацию от эмбеддингов пользователей $h_{u_j}^{(k)}$;
   - а пользователи, в свою очередь, через этот объект получат сигнал о его связях с другими объектами.

3. **Через несколько шагов**:
   - $h_{i_{\text{new}}}$ уже не только "контентный", но и **коллаборативно обогащённый** — в нём отражены связи с похожими пользователями и объектами;
   - объект плавно перемещается из зоны "чисто контентный" в зону "гибридный контент+CF".

### 3.4. Простая схема без полного LightGCN

Можно не делать полноценную GNN, а использовать более простые приёмы пропагации.

Пример high-level пайплайна:

1. Сначала считаем контентные эмбеддинги объектов $c_i$ (из meta-features и текста).
2. Строим user×item граф по взаимодействиям.
3. Для каждого пользователя считаем:

   $$
   g_u = \frac{1}{|N(u)|} \sum_{i \in N(u)} c_i.
   $$

   Это уже "коллаборативный" embedding пользователя, полученный через контент объектов.

4. Для объекта можно сделать обратное усреднение по пользователям:

   $$
   g_i = \frac{1}{|N(i)|} \sum_{u \in N(i)} g_u,
   $$

   если $N(i)$ не пусто (для новых объектов сначала пусто).

5. Итоговый embedding:

   $$
   h_i = \lambda c_i + (1 - \lambda) g_i,
   $$

   где $0 \leq \lambda \leq 1$.

Для **cold-item** на старте $g_i$ нет (нет соседей), поэтому $h_i \approx c_i$. Как только появляются взаимодействия, $g_i$ начинает вносить вклад, и объект становится более "коллаборативным".

### 3.5. Простейший игрушечный пример (Python)

```python
import numpy as np

# контентные эмбеддинги товаров (например, из текста)
item_content = {
    "A": np.array([1.0, 0.0]),   # условно: "зимняя обувь"
    "B": np.array([0.8, 0.1]),
    "C": np.array([0.0, 1.0]),   # условно: "летняя обувь"
    "NEW": np.array([0.9, 0.0]), # новый "зимний" товар
}

# взаимодействия: user -> список товаров
user_items = {
    "u1": ["A", "B"],
    "u2": ["A"],
    "u3": ["C"],
}

# шаг 1: считаем g_u как среднее контентных эмбеддингов просмотренных товаров
user_emb = {}
for u, items in user_items.items():
    embs = np.stack([item_content[i] for i in items])
    user_emb[u] = embs.mean(axis=0)

# допустим, у нового товара пока нет взаимодействий
def item_graph_emb(item_id: str):
    # найдём всех пользователей, у которых в истории есть этот товар
    users = [u for u, items in user_items.items() if item_id in items]
    if not users:
        return None
    embs = np.stack([user_emb[u] for u in users])
    return embs.mean(axis=0)

print("content NEW:", item_content["NEW"])
print("graph  NEW:", item_graph_emb("NEW"))  # None — нет взаимодействий

# Если у NEW появился пользователь u1
user_items["u1"].append("NEW")

user_emb = {}
for u, items in user_items.items():
    embs = np.stack([item_content[i] for i in items])
    user_emb[u] = embs.mean(axis=0)

print("graph  NEW after first user:", item_graph_emb("NEW"))
```

В реальности вместо двухмерных векторов — 64/128/256-мерные эмбеддинги, и можно делать несколько итераций такого "усреднения" (message passing).

### 3.6. Связь с LightGCN

LightGCN делает примерно то же самое, но:

- чётко формализует веса $w_{u,i}$ как $1 / \sqrt{|N(u)| \cdot |N(i)|}$,
- делает несколько слоёв пропагации с суммированием слоёв,
- обучает начальные эмбеддинги и веса end-to-end с BPR-лоссом.

В контексте **cold-start items** важно понимать идею:

1. Начальный embedding нового объекта — **контентный** (meta-features + текст).
2. Как только появляются первые взаимодействия, объект входит в user×item граф.
3. Через пропагацию (GraphCF/LightGCN-подобные подходы) embedding нового объекта обогащается информацией о похожих пользователях и объектах.

Подробный разбор LightGCN можно вынести в отдельный конспект; здесь достаточно понимать, что это более систематичный и обучаемый вариант той же идеи пропагации.

---

## 4. Как эти слои живут вместе в проде

В реальных системах cold-start items обычно обрабатываются **многоуровнево**:

1. **На момент создания объекта**:
   - доступны только meta-features и текст;
   - используем контентную модель CTR/score и/или zero-shot эмбеддинг → объект может участвовать в поиске и рекомендациях.

2. **После первых показов и редких кликов**:
   - появляются первые поведенческие признаки (пара кликов, небольшой CTR);
   - можно:
     - слегка поднимать новый объект в выдаче (exploration),
     - учитывать сырые CTR с байесовским сглаживанием,
     - использовать графовую пропагацию для обогащения embedding'а.

3. **Когда объект становится "тёплым" (warm)**:
   - есть достаточно взаимодействий для CF/MF/LightGCN;
   - объект входит в зону, где его уже можно уверенно использовать в коллаборативных моделях;
   - контентный embedding остаётся как часть гибридной модели (features + CF embedding).

4. **Управление рисками**:
   - нельзя слишком агрессивно пушить все новые объекты — можно убить CTR;
   - используют bandit-алгоритмы ($\varepsilon$-greedy, UCB, Thompson Sampling), чтобы аккуратно балансировать exploration новых объектов и exploitation старых успешных.