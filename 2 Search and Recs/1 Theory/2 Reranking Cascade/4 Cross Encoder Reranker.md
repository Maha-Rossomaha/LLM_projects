# Cross-encoder как финальный reranker

Cross-encoder — это наиболее точная архитектура для оценки релевантности пары (запрос, документ). В отличие от bi-encoder и late interaction, он не кодирует query и document независимо, а подаёт их **вместе** в модель, которая учитывает **все токеновые взаимодействия**.

---

## 1. Принцип работы

Вместо раздельного кодирования:

$$
\text{score}(q, d) = f([q; d])
$$

где $[q; d]$ — последовательность токенов запроса и документа, поданная в модель (например, BERT). Выход может быть:

- специальный токен [CLS] → MLP → score,
- pooling (mean / max / attention) → score,
- логит от бинарного классификатора (релевантен / не релевантен).

Такой подход даёт возможность «взвесить» каждый токен query с каждым токеном doc внутри слоя внимания.

---

## 2. Почему так точно?

- Cross-encoder **видит полную картину**, включая редкие зависимости, фразовые совпадения, синонимы.
- Модель может распознавать даже **парадоксальные и контекстно-зависимые** соответствия (например, "не работает" и "сломано").
- Это почти всегда **SOTA** на любых leaderboard'ах, если нет ограничений на latency.

---

## 3. Проблемы производительности

Если корпус содержит $N$ документов, и каждый запрос нужно сравнить с каждым:

- Bi-encoder: $f(q)$ и $f(d)$ считаются отдельно → быстрый ANN.
- Cross-encoder: $f([q; d])$ нужно вызвать **$N$ раз на каждый запрос**.

Сложность:

- $O(N · L^2)$ — где $L$ — длина $[q; d]$ (обычно 128–512 токенов).
- Даже с FP16 и batching, это **дорого**:
  - 1 запрос × 1000 документов = 1000 forward pass.
  - Даже 5 мс на один → 5 секунд на один запрос.

Поэтому cross-encoder используется **только на малом top‑M** после первичного отбора (например, 1000 → 50 → 10).

---

## 4. Типовой pipeline использования

```
Corpus → Retrieval (bi-encoder / ColBERT / BM25) → Candidates (top‑K)
           ↓
        Cross-encoder reranking (top‑M)
           ↓
        Top-N результатов
```

---

## 5. Edge cases и риски

### Длинные документы

- При длине > 512 токенов нужно **обрезать** или **разбивать на чанки**.
- Можно использовать "best chunk" стратегию: взять максимум из всех чанков документа.

### Параллельные запросы

- Нужно батчить на GPU.
- Один batched inference → [Q1 + D1], [Q1 + D2], ..., [Q2 + D1], ...
- Поддержка от моделей HuggingFace, ONNX, Triton, vLLM и др.

### Ограничение по памяти

- В 16-битном режиме можно уместить \~10–20 пар в 24GB GPU.
- Нужно следить за **максимальной длиной токенов** и **паддингом**.

---

## 6. Distillation (в сторону bi‑encoder)

- Cross-encoder можно использовать как **teacher-модель** для обучения bi-encoder.
- Bi-encoder копирует скоринг, не видя взаимодействий напрямую.
- Это улучшает качество retriever'а без повышения latency.

---

## 7. Пример использования 

```python
from sentence_transformers import CrossEncoder

model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

query = "What is the capital of France?"
documents = [
    "Paris is the capital of France.",
    "Berlin is the capital of Germany.",
    "The Eiffel Tower is in Paris."
]

pairs = [(query, doc) for doc in documents]
scores = model.predict(pairs)

for (q, d), score in zip(pairs, scores):
    print(f"Score: {score:.3f} | {d}")
```

---

## 8. Когда стоит применять

- Когда важно **максимальное качество**.
- В финальном rerank после top-K retrieval.
- При ограниченном количестве кандидатов (например, top-50).
- В задачах sensitive QA, медицинских/правовых документах.

---

## 9. Выводы

- Cross-encoder — это «золотой стандарт» по качеству.
- Он обеспечивает полное взаимодействие между токенами запроса и документа.
- Но дорог в вычислении, требует агрессивной фильтрации кандидатов.
- Используется как финальный reranker и как источник знаний для distillation.

