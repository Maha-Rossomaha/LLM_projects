# Оптимизация latency и ресурсоёмкости в reranking

Современные каскады поиска (retrieval → rerank) должны одновременно обеспечивать высокое качество и укладываться в строгие ограничения по времени отклика (SLA). Ниже собраны основные методы управления latency и нагрузкой на ресурсы.

---

## 1. Деление latency budget по слоям

Типовой latency budget: **500–800 мс** на весь pipeline. Он распределяется между слоями:

| Слой      | Примерное время | Пример процента |
| --------- | --------------- | --------------- |
| Retrieval | ≤100 мс         | 50%             |
| Rerank #1 | ≤200 мс         | 30%             |
| Rerank #2 | ≤100 мс         | 20%             |

> Под Rerank #1 часто понимается ColBERT или LightReranker, а под Rerank #2 — Cross-Encoder. Эти цифры можно адаптировать под устройство и SLA.

---

## 2. Adaptive $K$ / $M$

При повышенной нагрузке важно **динамически сокращать объёмы** кандидатов на каждом этапе:

- **$K$** — количество кандидатов после retrieval (би-энкодер / BM25)
- **$M$** — после первого reranker (например, ColBERT)
- **$N$** — после cross-encoder (выход в UI / LLM)

**Пример стратегии:**

- Если GPU idle: $K=1000 \to M=100 \to N=20$
- Если latency перегружена: $K=500 \to M=50 \to N=10$

Можно подключать SLA-aware контроллер:

```python
if current_latency > threshold:
    K, M, N = 500, 50, 10
else:
    K, M, N = 1000, 100, 20
```

---

## 3. Tail-latency mitigation

**Tail latency** — редкие, но долгие запросы, которые нарушают SLA.

### Hedged queries

- Отправить один и тот же запрос на **несколько реплик**.
- Использовать **первый вернувшийся** ответ.
- Особенно полезно для retrieval.

### Early exit

- Прерывание обработки, если достигается определённый порог уверенности (confidence).
- Например: если top-1 score ≫ остальных, не запускать cross-encoder.

### Timeout

- Жёсткий лимит по времени: если cross-encoder не успевает за 100 мс, возвращать результат предыдущего слоя.

---

## 4. Кэширование результатов rerank

Чтобы избежать повторной нагрузки:

- **Запросный кэш**: (query → top-N docs)
- **Embedding-кэш**: (query → vector) и (doc → vector)
- **Cross-encoder кэш**: ((q, d) → score)

Полезные техники:

- LRU / LFU политики
- Кэшировать только top-10 частых запросов
- Версионировать кэш по модели (query + model_version → score)

---

## 5. Практика балансировки SLA ↔ качество

### Что делать

- Собирать p50 / p95 / p99 latency по каждому слою
- Автоматически подстраивать $K/M/N$ по SLA
- Пропускать тяжелый rerank при высокой нагрузке
- Использовать FP16 / INT8 inference
- Кэшировать популярные запросы и документы

### Что избегать

- Фиксированных глубин при высокой волатильности нагрузки
- Выхода за p99 SLA даже при высоком качестве
- Одновременного запуска дорогих reranker’ов на все запросы