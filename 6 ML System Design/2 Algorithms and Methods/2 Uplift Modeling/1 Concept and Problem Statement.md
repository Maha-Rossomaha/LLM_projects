# Концепция и постановка задачи

> **Терминология:**  
Далее вместо *treatment* используем слово **воздействие** (коммуникация/оффер/кампания/назначение терапии).  
**RCT** - Randomized Control Trial, рандомизированный контролируемый эксперимент.

---

## 1. Постановка задачи и базовые понятия

### 1.1. Объекты, признаки и исходы

- **X** — вектор признаков объекта (клиента/пациента/юзера).
- **W ∈ {0,1}** — индикатор воздействия (1 — воздействие назначено/получено; 0 — нет).
- **Y** — целевой исход (обычно бинарный: конверсия/покупка/ответ; может быть вещественным — выручка, чек).

### 1.2. Потенциальные исходы и каузальные эффекты

- **Потенциальные исходы**: $Y(1)$ — что было бы с объектом при воздействии; $Y(0)$ — без воздействия.
- **Наблюдаемый исход**: $Y = Y(W)$. Мы никогда не видим одновременно $Y(1)$ и $Y(0)$ для одного и того же объекта (проблема фундаментальной невозможности наблюдать контрфактуал).

### 1.3. Определения эффектов

- **ATE (Average Treatment Effect)**: $\operatorname{ATE} = \mathbb{E}[Y(1) - Y(0)]$.
- **CATE/HTE (Conditional/Heterogeneous Treatment Effect)**: $\tau(x) = \mathbb{E}[Y(1) - Y(0) \mid X=x]$ — условный (гетерогенный) эффект.
- **ITE (Individual Treatment Effect)**: $\tau_i = Y_i(1) - Y_i(0)$ — индивидуальный эффект (не наблюдаем напрямую).
- **Uplift** (для бинарного Y):
  $$
  u(x) = \Pr(Y=1\mid X=x, W=1) - \Pr(Y=1\mid X=x, W=0),
  $$
  что совпадает с $\tau(x)$ при стандартных допущениях (SUTVA, ignorability). Для регрессионных Y — аналогично через математические ожидания.

> **Интуиция:** uplift — это *приростная вероятность целевого события* за счёт воздействия для объектов с признаками $x$.

---

## 2. Зачем нужен uplift и почему обычные модели отклика не подходят

### 2.1. Что оптимизирует бизнес

Мы хотим **максимизировать прирост результата** (прибыль/конверсии/здоровьесберегающий исход), а не просто вероятность отклика.

- **Оптимальное правило назначения воздействия\***: применяй воздействие, если
  $$
  u(x)\cdot V - C > 0,
  $$
  где $V$ — ценность целевого события (маржа/полезность), $C$ — стоимость воздействия.
  Иначе — не применяй.

### 2.2. Почему обычные response-модели дают ложную оптимизацию

- **Модель отклика** $\Pr(Y=1\mid X=x)$ или $\Pr(Y=1\mid X=x, W=1)$
  выбирает тех, кто *и так купит* (**always-buyers**), и может навредить тем, у кого воздействие снижает вероятность отклика (**sleeping dogs/do-not-disturbers**).
- Исторически **нерандомное назначение** (маркетинг таргетировал «вероятных покупателей») → **selection bias**: модель путает «склонность к покупке» с «эффектом воздействия».

### 2.3. Матрица поведенческих типов

| Тип                                   | Без воздействия | С воздействием | Инкремент |
| ------------------------------------- | --------------- | -------------- | --------- |
| **Always-buyers**                     | купит           | купит          | ≈ 0       |
| **Persuadables**                      | не купит        | купит          | +         |
| **Sleeping dogs (Do-not-disturbers)** | купит           | не купит       | −         |
| **Never-buyers**                      | не купит        | не купит       | 0         |

**Uplift-модели** — единственные, кто пытается ранжировать по **инкременту**, выводя наверх persuadables и убирая sleeping dogs.

### 2.4. Числовой пример

**Пусть**  
**Cегмент A:** $\Pr(Y=1\mid W=1)=30\%$, $\Pr(Y=1\mid W=0)=20\%$ → $u(A)=+10pp$.  
**Сегмент B:** $\Pr(Y=1\mid W=1)=45\%$, $\Pr(Y=1\mid W=0)=44\%$ → $u(B)=+1pp$.

Обычная модель отклика выберет **B** (потому что 45% > 30%). Но **инкремент** больше в **A** (+10pp). Если воздействие стоит денег, A приносит больше прибыли.

---

## 3. Базовая идея uplift-моделей

### 3.1. Что предсказываем

Не сам отклик, а **разницу откликов с/без воздействия**:

$$
\Delta p(x) = \Pr(Y=1\mid X=x, W=1) - \Pr(Y=1\mid X=x, W=0).
$$

### 3.2. Как используем прогноз

- Ранжируем объекты по $\widehat{\Delta p}(x)$ (от большего к меньшему).
- При ограниченном бюджете или стоимости воздействия выбираем топ-$K$ объектов, где ожидаемая **прибыль** положительна:
  $$
  \widehat{\Delta p}(x)\cdot V - C > 0.
  $$
- Если $V$ и $C$ зависят от $x$, используем объектно-зависимое правило.

### 3.3. Минимальные допущения (интуитивно)

- **SUTVA**: воздействие одного не влияет на исход другого; само воздействие однозначно определено.
- **Переносимость (ignorability)**: условно на X назначение воздействия «как бы случайно» (в RCT — истинно, в наблюдательных данных — достигается пропенсити/матчингом/весами).

> Детально допущения и способы их проверки — в отдельном конспекте про каузальные основы и дизайн эксперимента.

---

## 4. Примеры применения

### 4.1. Маркетинг / CRM

- **Вопрос**: кому отправлять email/push/баннер, чтобы увеличить конверсию/ARPU?
- **Воздействие**: коммуникация (сообщение, скидка, купон).
- **Цель**: максимизировать инкрементальные покупки при бюджете и ограничениях на контактную политику.

### 4.2. Банковские офферы / персональные ставки

- **Вопрос**: кому показывать предложение КСП/кредит с персональной ставкой?
- **Воздействие**: сам факт предложения (показ оффера) — бинарное; (вариант: уровень ставки — непрерывное воздействие, это уже обобщение uplift).
- **Цель**: максимизировать инкрементально одобренные и взятые кредиты при ограничениях по риску и стоимости коммуникации.

### 4.3. Медицина

- **Вопрос**: какой подгруппе пациентов терапия даёт положительный эффект?
- **Воздействие**: назначение терапии.
- **Цель**: максимизировать благоприятный исход, минимизируя побочные эффекты и стоимость.

### 4.4. Рекомендательные системы

- **Вопрос**: что показать на первом экране — промо A или без промо?
- **Воздействие**: показ промо-блока.
- **Цель**: максимизировать инкрементальную вероятность клика/покупки, а не общую кликабельность.

---

## 5. Методы оценки uplift-моделей

Оценка должна измерять **инкрементальную пользу политики назначения воздействия**. Стандарт — **uplift-кривые**, **Qini** и **AUUC**.

### 5.1. Подготовка holdout-выборки

- Разбиваем данные на train/valid/test так, чтобы в каждом сплите сохранялась доля воздействия (treatment rate) и был **контроль**.
- Для корректного сравнения моделей используем **один и тот же** тестовый сплит.

### 5.2. Построение uplift-кривой (Radcliffe & Surry)

1. Для каждого объекта теста получаем $\widehat{u}(x)$ и сортируем по убыванию.
2. Разбиваем на одинаковые по размеру **квантили/децили** (или строим кумулятивно по процентилям).
3. Для каждого префикса (топ-q%) вычисляем **накопленный инкремент**:
   $$
   G(q) = \sum_{b\in \text{топ-}q\%} \Big(R_{t,b} - R_{c,b} \cdot \tfrac{N_{t,b}}{N_{c,b}}\Big),
   $$
   где $R_{t,b}$, $R_{c,b}$ — число ответов (положительных таргетов) в группе воздействия и контроля в бине $b$, $N_{t,b}$, $N_{c,b}$ — их размеры. Масштабирующий множитель $\tfrac{N_{t,b}}{N_{c,b}}$ компенсирует неодинаковые размеры.
4. Рисуем кривую $G(q)$ vs $q$. Случайная политика даёт базовую (почти) прямую линию.

> **Интерпретация:** чем выше кривая и больше расстояние до базовой линии, тем больше **инкрементальной** пользы приносит модель при таргетинге топ-q% объектов.

### 5.3. Qini-кривая и Qini-коэффициент

- **Qini-кривая** — это uplift-кривая, но часто нормированная на общий размер выборки/контрольной доли.
- **Qini-коэффициент** — площадь между кривой модели и кривой случайного таргетинга:
  $$
  Q = \int_0^1 \big(G(q) - G_{\text{random}}(q)\big)\,dq.
  $$
- Вычисляется численно (трапеции) по дискретным точкам (децилям/квантилям).

### 5.4. AUUC (Area Under the Uplift Curve)

- Площадь под uplift-кривой (часто без вычитания random). Иногда используют **нормированный AUUC**.
- При равной доле воздействия в тесте AUUC и Qini монотонно связаны; сравнимость важнее абсолютного масштаба.

### 5.5. Небольшой числовой пример (децильный)

Предположим, у нас 10 равных децилей по рангу $\widehat{u}(x)$. В каждом дециле посчитаны $(N_{t}, R_{t})$ и $(N_{c}, R_{c})$. Инкремент в дециле $b$:

$$
\Delta_b = R_{t,b} - R_{c,b}\cdot \tfrac{N_{t,b}}{N_{c,b}}.
$$

Кумулятивная кривая: $G_k = \sum_{b=1}^k \Delta_b$. Далее считаем **Qini** как сумму площадей трапеций под $G_k$ минус базовая линия (random).

> **Практика:** многие библиотеки реализуют Qini/AUUC; важно лишь правильно передать факты воздействия/контроля и ранги $\widehat{u}(x)$.

### 5.6. Политико-ориентированные метрики (profit/value)

Если известны **стоимость контакта** $C$ и **ценность исхода** $V$, можно строить **Value curve**:

$$
\text{Value}(q) = \sum_{i\in \text{топ-}q\%} \big(\mathbb{1}\{W_i=1\}\cdot Y_i - \mathbb{1}\{W_i=0\}\cdot Y_i \cdot r\big)\cdot V - C,
$$

где $r$ — поправка на разницу размеров групп (как выше). Это даёт более «бизнесовую» картину, чем чистый Qini.

---

## 6. Мини-кейсы (интуиция)

### 6.1. Email-кампания

- Бюджет позволяет отправить письмо 20% базы.
- Обычный lookalike даёт много always-buyers → ROI низкий.
- Uplift-ранжирование приводит в топ persuadables → рост инкрементальных заказов при той же рассылке.

### 6.2. Персональное кредитное предложение (КСП)

- Бинарная постановка: показывать оффер (W=1) или нет (W=0).
- Uplift моделирует $\Delta p(x)$ «возьмёт кредит при показе vs без показа»; политика — показывать тем, где $\Delta p(x)\cdot V - C > 0$.
- Обобщение (вне рамок этого конспекта): непрерывная ставка → **continuous treatment** (см. позже).

### 6.3. Медицинская терапия

- РКИ показывает ATE≈0, но uplift/HTE выявляет подгруппу с положительным CATE (например, по биомаркерам). Политика: назначать терапию только ей.

---

## 7. Частые ошибки и анти‑паттерны

- **Оценивать по отклику, а не по инкременту.**
- **Смешивать train/test** без сохранения доли W=1 и контроля.
- **Игнорировать стоимость воздействия** — тогда оптимизация по Qini может быть не равна оптимуму прибыли.
- **Учить «две отдельные модели отклика»** и без контроля качества разности — может давать высокую вариативность (об этом — в конспекте по архитектурам).
- **Использовать исторические, нериндомизированные назначения** без корректировки (propensity/IPTW/matching) — получим систематическую ошибку.

---

### Нотация (для справки)

- $X$ — признаки; $W$ — воздействие; $Y$ — исход.
- $Y(1), Y(0)$ — потенциальные исходы.
- $\operatorname{ATE}$, $\tau(x)=\operatorname{CATE}$, $\tau_i=\operatorname{ITE}$.
- $u(x)$ — uplift (приростная вероятность).
