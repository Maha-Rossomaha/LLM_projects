# 8. Современные методы и связь с каузальными моделями

**Цель:** увязать uplift‑моделирование с более широкой экосистемой causal ML и понять, где на этой шкале находятся S/T/X/R/DR‑learner’ы, леса и нейросети.

---

## 1. Картина в целом: от прогнозирования к причинности

Можно условно нарисовать шкалу:

- **Чистый prediction**:
  - обычные модели $\mathbb E[Y \mid X]$, AUC/logloss;
  - нас не волнует вопрос «что было бы, если…».

- **Uplift / CATE**:
  - оцениваем $\tau(x) = \mathbb E[Y(1) - Y(0) \mid X=x]$;
  - фокус на **гетерогенности эффекта** и выборе политики таргетинга;
  - чаще всего в условиях RCT или квазирандомизации.

- **Общий causal inference**:
  - ATE/ATT/ATC, CATE, dose–response, медиация, инструменты и т.п.;
  - работа с пропенсити, балансировкой, чувствительным анализом;
  - строгие статистические методы, доверительные интервалы.

Современные методы causal ML (Double ML, Causal Forest, GRF, нейросетевые архитектуры) лежат как раз **между uplift и строгим каузальным выводом**:

- используют flexible ML (GBM, RF, NN) для «nuisance»-частей ($m(X), e(X)$);
- строят **ортогональные оценки эффектов**, устойчивые к ошибкам этих моделей;
- подходят и для uplift‑подобных задач (CATE), и для «классического» causal inference.

---

## 2. Double Machine Learning (DML)

### 2.1. Интуиция DML

Классическая проблема:

- хотим оценить параметр $\theta$ (ATE, CATE и т.п.);
- есть сложные зависимости между $X, W, Y$, которые удобно описывать ML‑моделями $m(X), e(X)$;
- обычные ML‑оценки могут вносить сильный bias в оценку $\theta$.

**Double Machine Learning (Chernozhukov et al.)** предлагает рецепт:

1. Выделить **nuisance‑функции** (шумы):
   - $m(X)$: исход без учёта treatment;
   - $e(X)$: propensity;
   - иногда дополнительные функции (например, для инструментов).

2. Построить **ортогональное условие момента**, где влияние ошибок $\hat m, \hat e$ на оценку $\theta$ в первой степени «занулено».

3. Оценивать $m, e$ при помощи flexible ML (GBM, RF, NN) с **cross‑fitting**.

4. На основе полученных резидуалов решать относительно $\theta$ уже простую задачу (регрессия, M‑оценка), получая хорошие статистические свойства.

Главная идея: **разделить сложное прогнозирование и оценку причинного параметра**, защитив вторую от ошибок первой.

---

### 2.2. DML для ATE (упрощённая схема)

Для бинарного $W$, scalar ATE $\theta = \mathbb E[Y(1)-Y(0)]$:

1. Оцениваем nuisance‑функции:

   - $\hat m_1(X) \approx \mathbb E[Y \mid X, W=1]$;
   - $\hat m_0(X) \approx \mathbb E[Y \mid X, W=0]$;
   - $\hat e(X) \approx P(W=1\mid X)$;

   с cross‑fitting: для каждого наблюдения используем модели, обученные **не** на нём.

2. Строим AIPW‑подобный **score**:

   $$
   \psi(Z_i; \theta) = \Big[ \hat m_1(X_i) - \hat m_0(X_i)
   + \frac{W_i}{\hat e(X_i)} (Y_i - \hat m_1(X_i))
   - \frac{1-W_i}{1-\hat e(X_i)} (Y_i - \hat m_0(X_i)) \Big] - \theta,
   $$

   где $Z_i = (X_i, W_i, Y_i)$.

3. DML‑оценка $\hat \theta$ решает условие момента:

   $$
   \frac{1}{n} \sum_{i=1}^n \psi(Z_i; \hat \theta) = 0.
   $$

Отсюда получается формула **DR‑оценки ATE** (см. конспект по DR), только подчёркивается важность cross‑fitting и ортогональности.

---

### 2.3. DML для CATE / uplift

Для CATE / uplift можно:

- рассматривать $\tau(x)$ как функцию, а не скаляр;
- строить похожие ортогональные псевдо‑таргеты и обучать ML‑регрессию на них.

Фактически:

- **R‑learner** = частный случай DML с Robinson‑декомпозицией;
- **DR‑learner** = AIPW‑псевдо‑таргет + регрессия;
- **Orthogonal Random Forest** = DML + лес как «локальный регрессор».

DML здесь работает как общий теоретический «зонт», под который попадают многие конструкции uplift‑моделирования.

---

### 2.4. Мини‑скелет DML‑пайплайна (идея, без библиотек)

```python
# 1. Разбиваем данные на K фолдов для cross-fitting
folds = make_folds(X, K=2)
theta_scores = []

for k in range(K):
    train_idx, test_idx = folds[k]["train"], folds[k]["test"]

    # 2. На train оцениваем m1, m0, e любыми ML-моделями
    m1 = fit_model(X[train_idx], Y[train_idx], W[train_idx] == 1)
    m0 = fit_model(X[train_idx], Y[train_idx], W[train_idx] == 0)
    e  = fit_propensity(X[train_idx], W[train_idx])

    # 3. На test считаем псевдо-таргеты
    m1_hat = m1.predict(X[test_idx])
    m0_hat = m0.predict(X[test_idx])
    e_hat  = e.predict_proba(X[test_idx])

    W_t = W[test_idx]
    Y_t = Y[test_idx]

    score = (m1_hat - m0_hat
             + W_t / e_hat * (Y_t - m1_hat)
             - (1 - W_t) / (1 - e_hat) * (Y_t - m0_hat))

    theta_scores.append(score)

# 4. DML-оценка ATE
all_scores = np.concatenate(theta_scores)
theta_hat = all_scores.mean()
```

В реальных реализациях (econml, DoubleML и др.) всё это обёрнуто в аккуратные классы.

---

## 3. Causal Forest и Generalized Random Forest (GRF)

### 3.1. Causal Tree → Causal Forest

**Causal Tree (Athey & Imbens)**:

- дерево, специально построенное для оценки гетерогенного эффекта $\tau(x)$;
- сплиты выбираются так, чтобы максимизировать различие оценок эффекта в листьях;
- используется "honest" подход: одна часть данных строит структуру, другая оценивает эффекты в листьях → меньше оптимистичного смещения.

**Causal Forest (Wager & Athey)**:

- ансамбль causal‑деревьев (bagging + случайный выбор признаков);
- оценка $\tau(x)$ = взвешенное среднее эффектов по деревьям:

  $$
  \hat \tau(x) = \sum_{i=1}^n \alpha_i(x) Y_i,
  $$

  где веса $\alpha_i(x)$ зависят от того, в какие листья попал $x$ в деревьях и какие там были сплиты.

Лес даёт более устойчивые и гладкие оценки $\tau(x)$, чем одиночное дерево.

---

### 3.2. Generalized Random Forests (GRF)

GRF обобщает идею causal forest на широкий класс задач:

- CATE (как в causal forest);
- инструментальные переменные;
- quantile treatment effects;
- честные регрессии и др.

Ключевая идея:

1. Задать **условие момента** $\mathbb E[\psi(Z; \theta(X)) \mid X] = 0$ для параметра интереса $\theta(X)$ (например, $\tau(X)$).
2. Построить лес, который определяет веса соседства $\alpha_i(x)$ для каждого $x$.
3. В точке $x$ решить локальную задачу:

   $$
   \sum_{i=1}^n \alpha_i(x) \psi(Z_i; \theta(x)) = 0.
   $$

Для CATE это приводит к локальной оценке treatment‑эффекта с ортогонализацией (похоже на R‑learner, но с лесом как локальным регрессором).

**Итого:** GRF = «случайный лес + Double ML в каждой точке».

---

### 3.3. GRF и uplift‑метрики

Causal forest / GRF выдают качественную оценку $\hat \tau(x)$. Дальше всё классическое:

- ранжируем клиентов по $\hat \tau(x)$;
- строим Qini / AUUC;
- оптимизируем policy value.

Главное отличие от простых uplift‑моделей:

- встроенная ортогонализация;
- возможность получать доверительные интервалы на $\tau(x)$;
- лучшее поведение в high‑dimensional и сложных нелинейных случаях.

---

## 4. Meta‑learners как частные случаи DML

### 4.1. S/T/X‑learners

- **S‑learner**: одна модель $m(x,w)$, uplift как разность $m(x,1)-m(x,0)$. Это ещё не DML, а просто outcome‑регрессия.
- **T‑learner**: две модели $m_1, m_0$, uplift как разность. Тоже ещё не DML – нет ортогонализации.
- **X‑learner**: использует псевдо‑таргеты $D^{(1)}, D^{(0)}$ и регрессию по ним – ближе к DR/AIPW, но без явного условия момента.

Их можно рассматривать как **инженерные эвристики**, которые предвосхищают идеи DML, но не всегда реализуют полноценно ортогональные скоры.

---

### 4.2. R‑ и DR‑learners как DML

- **R‑learner** использует Robinson‑форму:

  $$
  Y - m(X) = \tau(X) (W - e(X)) + \varepsilon.
  $$

  Здесь $m,e$ – nuisance, $\tau(X)$ – параметр. Это типичный DML‑сетап с локальной регрессией $\tau(X)$ на основе ортогонального момента.

- **DR‑learner** строит AIPW‑псевдо‑таргеты $Z_i$, у которых:

  $$
  \mathbb E[Z_i \mid X=x] = \tau(x),
  $$

  при корректности хотя бы одной из nuisance‑моделей. Затем обычная регрессия $Z$ на $X$ – тоже вариант DML, где orthogonal score уже свёрнут в $Z$.

**Вывод:** многие meta‑learners = «обёртки над AIPW/Robinson» → частные случаи Double ML.

---

## 5. Uplift vs Causal Inference: различия и пересечения

### 5.1. Общая теоретическая база

И uplift, и causal inference опираются на одну и ту же формализацию:

- потенциальные исходы $Y(0), Y(1)$;
- treatment $W$;
- ковариаты $X$;
- предположения ignorability, overlap, SUTVA.

**Разница чаще в акцентах и целевой аудитории, чем в математике.**

---

### 5.2. Чем uplift обычно отличается

1. **Фокус на ранжировании и политике**

   - uplift‑модели оперируют $\hat \tau(x)$ как **скором для таргетинга**;
   - ключевые метрики – Qini, AUUC, policy value;
   - доверительные интервалы и формальные тесты вторичны.

2. **Чаще RCT / полукаузальные данные**

   - в маркетинге часто есть рандомизация (часть не таргетится → контроль);
   - меньше внимания уделяется скрытым конфоундерам и инструментам.

3. **Бизнес‑ориентированные решения**

   - вопрос «кому слать оффер и какой?» важнее, чем «каков `true ATE` в популяции?».

---

### 5.3. Чем causal inference отличается

1. **Фокус на параметрах и интерпретации**

   - ATE/ATT/ATC, медианные эффекты, медиация, контрафактические сценарии;
   - строгие доверительные интервалы, p‑values, sensitivity analysis.

2. **Сложные источники смещения**

   - скрытые конфоундеры;
   - нарушенная рандомизация;
   - инструментальные переменные, front‑door / back‑door критерии.

3. **Более широкий спектр задач**

   - policy evaluation в экономике, медицине;
   - анализ эффектов законов, реформ, цен.

---

### 5.4. Пересечение

- uplfit‑архитектуры (особенно R/DR‑learners, Causal Forest) = инструменты causal inference для CATE;
- causal inference даёт теоретический фундамент (какие предположения нужны, как проверять устойчивость);
- на практике грамотный uplift‑проект – это почти всегда **каузальный проект** с упором на бизнес‑целевую функцию.

---

## 6. Нейросетевые подходы: TARNet, DragonNet, CEVAE

### 6.1. Общая мотивация

Нейросети удобны, когда:

- сложные и высокоразмерные X (изображения, тексты, эмбеддинги);
- нелинейные зависимости и взаимодействия;
- нужен **shared representation learning** для разных значений W.

Классические модели:

- **TARNet** (Treatment‑Agnostic Representation Network);
- **DragonNet**;
- **CEVAE** и другие вариации VAE для скрытых конфоундеров.

---

### 6.2. TARNet

**Идея:** разделить представление X и моделирование исхода в каждой группе.

Архитектура:

1. Общая «база» (encoder):

   $$
   R = \phi(X; \theta_{enc}),
   $$

   где $\phi$ – несколько слоёв NN → низкоразмерный representation.

2. Два «верхних» блока (heads):

   $$
   \hat m_0(x) = f_0(R; \theta_0), \quad
   \hat m_1(x) = f_1(R; \theta_1).
   $$

Loss:

- суммарный риск предсказания $Y$ в обеих группах;
- регуляризация на smoothness/weight decay.

Оценка uplift:

$$
\hat \tau_{TARNet}(x) = \hat m_1(x) - \hat m_0(x).
$$

Плюсы:

- общая репрезентация X для обеих групп → лучше использует данные;
- подходит для высокоразмерных и сложных признаков.

Минусы:

- сама по себе не решает проблему конфоундинга → нужна связка с propensity / DML.

---

### 6.3. DragonNet

DragonNet расширяет TARNet, добавляя:

1. Отдельный output для **propensity**:

   $$
   \hat e(x) = g(R; \theta_e).
   $$

2. **Targeted regularization**:

   - в лосс добавляется компонент, близкий к AIPW‑скору;
   - сеть обучается так, чтобы комбинация $\hat m_0, \hat m_1, \hat e$ давала «каузально корректные» оценки.

Условно: DragonNet ≈ TARNet + встроенная пропенсити‑модель + DML‑регуляризация в одном end‑to‑end пайплайне.

---

### 6.4. CEVAE (Causal Effect VAE)

Проблема: **скрытые конфоундеры** (latent U), влияющие и на W, и на Y.

CEVAE предлагает генеративную модель:

- скрытая переменная $Z$ ≈ латентный конфоундер;
- граф:

  $$
  Z \to X, \quad Z \to W, \quad (Z,W) \to Y.
  $$

Обучаем VAE‑подобную модель:

1. Encoder: $q_\phi(Z \mid X, W, Y)$;
2. Decoder’ы для $p_\theta(X \mid Z), p_\theta(W \mid Z), p_\theta(Y \mid Z, W)$.

После обучения можно:

- для данного $x$ интегрировать по $Z$, чтобы получить $\mathbb E[Y(w) \mid X=x]$;
- оценивать ATE/CATE даже при частично скрытом конфоундинге (при сильных допущениях).

На практике CEVAE – тяжёлый метод, но важен как концептуальный шаг к учёту латентных факторов.

---

## 7. Перспективы: off‑policy learning, causal bandits, personalization

### 7.1. Off‑policy evaluation и uplift

Сценарий:

- у нас есть лог данных, собранный под какой‑то исторической политикой $\pi_0$;
- мы хотим оценить **новую** политику $\pi$ (например, другой таргетинг или размер скидки), не разыгрывая полноценный A/B.

Инструменты:

- **Inverse Propensity Scoring (IPS)**, DR‑оценки policy value;
- causal‑модели $\hat \tau(x)$, построенные по $\pi_0$, чтобы симулировать эффект $\pi$;
- комбинация uplift‑моделей с off‑policy evaluation → ключевой элемент рекомендательных систем и таргетинговых платформ.

---

### 7.2. Causal bandits и contextual bandits

Контекстуальные многорукие бандиты:

- в каждый момент видим контекст $X$;
- выбираем действие (treatment) $W \in \{0,1,\dots,K\}$;
- наблюдаем исход $Y$ только для выбранного $W$.

Связь с uplift / causal ML:

- uplift‑модель $\hat \tau(x)$ задаёт **prior** для политики;
- bandit‑алгоритм (UCB, Thompson Sampling) добавляет **exploration**, дообучаясь на новых данных;
- появляется цикл *online learning* + *causal estimation*.

Перспективы:

- персонализированный таргетинг, оптимизирующий долгосрочный NPV;
- адаптивные кампании, где стратегия меняется в процессе.

---

### 7.3. Персонализация и RL

В более сложных сценариях:

- есть длинный горизонт (LTV, последовательные взаимодействия);
- действия зависят от истории (не только текущий X);
- нужны методы **reinforcement learning** (off‑policy RL) с каузальной интерпретацией.

Здесь uplift‑модели становятся:

- либо building block’ами для оценки одномоментных эффектов;
- либо источником признаков / priors для RL‑алгоритмов.

---

## 8. Краткие выводы

1. **DML** предоставляет общий теоретический каркас: как использовать мощные ML‑модели для nuisance‑частей, не убивая корректность оценки эффекта.
2. **R‑, DR‑learners и Causal Forest / GRF** – практические инструменты, реализующие идеи DML для CATE/uplift.
3. **Meta‑learners** (S/T/X) можно понимать как эвристические (но полезные) приближения к этим более строгим методам.
4. **Нейросетевые модели** (TARNet, DragonNet, CEVAE) приносят representation learning и generative modeling в causal ML, особенно для сложных X и скрытых конфоундеров.
5. Uplift‑моделирование – это не «отдельная дисциплина», а частный случай causal ML с акцентом на ранжирование и бизнес‑политику.
6. Будущее – за связкой uplift / causal ML с **off‑policy learning, bandits и RL**, где персонализированные решения оптимизируются в онлайне, но при этом опираются на строгую каузальную логику.

