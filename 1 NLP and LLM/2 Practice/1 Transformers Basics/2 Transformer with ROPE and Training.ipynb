{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFd4gLTldVdQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import einops\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from jaxtyping import Float, Int\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\") as fin:\n",
    "    text = fin.read()\n",
    "    \n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path='openai-community/gpt2')\n",
    "assert tokenizer.tokenize(\"Hello there sometrashtoken\") == ['Hello', 'Ġthere', 'Ġsomet', 'r', 'ash', 'token']\n",
    "assert tokenizer.eos_token == \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer doesn't have special token for PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer: AutoTokenizer, text: str):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts: List[List[int]] = []\n",
    "        random.seed(1)\n",
    "        tokenized_tokens = self.tokenizer.encode(text)\n",
    "        i = 0\n",
    "        while i < len(tokenized_tokens):\n",
    "            seq_len = random.randint(200, 301)\n",
    "            self.texts.append(tokenized_tokens[i:i+seq_len])\n",
    "            i += seq_len\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index) -> List[int]:\n",
    "        return self.texts[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.texts.__len__()\n",
    "    \n",
    "\n",
    "dataset = MyDataset(tokenizer, text)\n",
    "sample_0 = dataset.tokenizer.decode(dataset[0])\n",
    "\n",
    "assert sample_0.startswith(text[:100])\n",
    "\n",
    "print(sample_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate FN \n",
    "Принимает `List[List[int]]` батч объектов и возвращает 2 тензора:\n",
    "\n",
    "* input_ids - `[batch, seq_len]` - батч токенов, в котором добавлены паддинги до максимальной длины в батче.\n",
    "* mask - `[batch, seq_len]` - батч масок. На позиции `[i, j]` стоит 0, если токен является паддингом, иначе 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[List[int]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    max_seq_len = max(len(elem) for elem in batch)\n",
    "    input_ids = torch.LongTensor([seq + [tokenizer.pad_token_id] * (max_seq_len - len(seq)) for seq in batch])\n",
    "    mask = torch.LongTensor([[1 if elem != tokenizer.pad_token_id else 0 for elem in seq] for seq in input_ids])\n",
    "    return (input_ids, mask)\n",
    "\n",
    "\n",
    "batch = [\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2],\n",
    "    [1, 2, 3, 4, 5, 6, 7],\n",
    "]\n",
    "input_ids_ref = torch.LongTensor([\n",
    "    [1, 2, 3, 4, 50256, 50256, 50256],\n",
    "    [1, 2, 50256, 50256, 50256, 50256, 50256],\n",
    "    [1, 2, 3, 4, 5, 6, 7],\n",
    "])\n",
    "\n",
    "\n",
    "mask_ref = torch.LongTensor([\n",
    "    [1, 1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "input_ids, mask = collate_fn(batch)\n",
    "\n",
    "assert (input_ids == input_ids_ref).all()\n",
    "assert (mask == mask_ref).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "sampler = RandomSampler(dataset)\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "for input_ids, mask in train_loader:\n",
    "    break\n",
    "print(mask)\n",
    "\n",
    "assert (mask.sum(dim=1) < mask.size(1)).sum() < mask.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768 # он же hidden_dim - внутрення размерность модели\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5 \n",
    "    d_vocab: int = 50257 # он же vocab_size, размер словаря модели\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024 # число позиционных эмбеддингов\n",
    "    d_head: int = 64 # размерность головы аттеншена\n",
    "    d_mlp: int = 3072 # внутренняя размерность FFN-слоя\n",
    "    n_heads: int = 12 # число голов аттеншена\n",
    "    n_layers: int = 12 # число слоев трансформера\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GELU(x: torch.Tensor):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        return nn.functional.embedding(input=input_ids, weight=self.W_E)\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        positions = torch.arange(input_ids.shape[1], device=input_ids.device)\\\n",
    "            .unsqueeze(0)\\\n",
    "                .expand(input_ids.shape[0], input_ids.shape[1])\n",
    "        return nn.functional.embedding(positions, self.W_pos)\n",
    "    \n",
    "    \n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_vocab\"]:\n",
    "        return torch.einsum('abc, cd -> abd', x, self.W_U) + self.b_U\n",
    "    \n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        res_in = torch.matmul(x, self.W_in) + self.b_in\n",
    "        return torch.matmul(GELU(res_in), self.W_out) + self.b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "https://arxiv.org/pdf/1910.07467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model)) # gamma\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Float[torch.Tensor, \"batch seq_len d_model\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        return x / torch.sqrt(torch.sum(torch.square(x), dim=-1, keepdim=True) / x.size(-1)) * self.w\n",
    "    \n",
    "    \n",
    "cfg_rmsnorm = Config(d_model=5)\n",
    "x = torch.Tensor([[[0.1, 0.2, 0.3, 0.4, 0.5]]])\n",
    "layer = RMSNorm(cfg_rmsnorm)\n",
    "y = torch.Tensor([[[0.3015, 0.6030, 0.9045, 1.2060, 1.5076]]])\n",
    "assert torch.allclose(y, layer(x), atol=1e-4, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Masking \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[torch.Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(float(\"-inf\"), dtype=torch.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[torch.Tensor, \"batch seq_len d_model\"], mask: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        # Берем размерности\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_heads = self.cfg.n_heads\n",
    "        d_head = self.cfg.d_head\n",
    "        \n",
    "        # 1. Трансформируем матрицы проекций в формат [d_model, d_model]\n",
    "        W_Q = self.W_Q.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_K = self.W_K.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_V = self.W_V.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        \n",
    "        b_Q = self.b_Q.view(-1)\n",
    "        b_K = self.b_K.view(-1)\n",
    "        b_V = self.b_V.view(-1)\n",
    "        \n",
    "        # 1. получаем проекции  Q, K, V\n",
    "        Q = torch.matmul(x, W_Q) + b_Q\n",
    "        K = torch.matmul(x, W_K) + b_K\n",
    "        V = torch.matmul(x, W_V) + b_V\n",
    "\n",
    "        Q = einops.rearrange(Q, 'b s (n d) -> b n s d', n=num_heads)\n",
    "        K = einops.rearrange(K, 'b s (n d) -> b n s d', n=num_heads) \n",
    "        V = einops.rearrange(V, 'b s (n d) -> b n s d', n=num_heads) \n",
    "\n",
    "        # 2. Q x K^T\n",
    "        scores = torch.einsum('b n i d, b n j d -> b n i j', Q, K)\n",
    "        \n",
    "        # 3. Нормализация\n",
    "        scores /= math.sqrt(d_head)\n",
    "        \n",
    "        # 4. Маскирование\n",
    "        scores = self.apply_causal_mask(scores, mask)\n",
    "        \n",
    "        # 5. softmax\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # 6. Финальная проекция\n",
    "        attention = torch.einsum('b h i j, b h j d -> b h i d', scores, V)\n",
    "        attention = torch.einsum('b h i d, h d m -> b i m', attention, self.W_O) + self.b_O\n",
    "\n",
    "        return attention\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, \n",
    "        attn_scores: Float[torch.Tensor, \"batch n_heads seq_len seq_len\"], \n",
    "        mask: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch n_heads seq_len seq_len\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        Используем треугольную маску, чтобы не смотреть в будущее.\n",
    "        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n",
    "        '''\n",
    "        seq_len = attn_scores.size(-1)\n",
    "        old_device = attn_scores.device\n",
    "        attn_scores = attn_scores.to(device)\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask, value=self.IGNORE)\n",
    "\n",
    "        mask = mask.to(device)\n",
    "        padding_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        attn_scores = attn_scores.masked_fill(padding_mask == 0, value=self.IGNORE)\n",
    "\n",
    "        return attn_scores.to(old_device)\n",
    "\n",
    "\n",
    "mask_padding = torch.LongTensor([\n",
    "    [1, 1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "lengths = mask_padding.sum(dim=1).tolist()\n",
    "\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 7\n",
    "d_head = 8\n",
    "n_heads = 4\n",
    "\n",
    "x = torch.rand(batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "attn = Attention(cfg)\n",
    "softmax_res = torch.softmax(attn.apply_causal_mask(x, mask_padding), dim=-1)\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "    for head_idx in range(n_heads):\n",
    "        sm = softmax_res[batch_idx, head_idx]\n",
    "        l = lengths[batch_idx]\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                # i < j - Causal mask, проверяем, что не смотрим в будущее\n",
    "                # j >= l - проверяем, что не смотрим на паддинги\n",
    "                if i < j or j >= l:\n",
    "                    assert sm[i, j] == 0, (batch_idx, head_idx, i, j, sm[i, j])\n",
    "                \n",
    "_ = attn(torch.rand(batch_size, seq_len, 768), mask_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Embeddings  \n",
    "https://arxiv.org/pdf/2104.09864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg: Config, theta: int = 10_000):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.max_seq_len = cfg.n_ctx\n",
    "        self.theta = theta\n",
    "        self.d = cfg.d_head\n",
    "        \n",
    "        # Углы theta_i. \n",
    "        freqs = theta ** (-2 * torch.arange(self.d // 2) / self.d).to(device)\n",
    "        position_id = torch.arange(0, self.max_seq_len).float().to(device)\n",
    "        \n",
    "        # нужно получить матрицу m theta_i размера [max_seq_len, self.d] вида m theta_i\n",
    "        # где m берется из position_id, а theta из freqs\n",
    "        \n",
    "        idx_theta = torch.einsum('i, j -> ij', position_id, freqs)\n",
    "        \n",
    "        # max_seq_len, d_head\n",
    "        cos = idx_theta.cos()\n",
    "        sin = idx_theta.sin()\n",
    "        \n",
    "        # нужно продублировать размерности для формулы 34. theta_i встерчается два раза подряд в синусах и косинуса\n",
    "        # тут нам поможет torch.repeat_interleave\n",
    "        cos = cos.repeat_interleave(2, dim=-1)\n",
    "        sin = sin.repeat_interleave(2, dim=-1)\n",
    "        \n",
    "        # 1, max_seq_len, 1, d_head\n",
    "        self.register_buffer(\"sin\", sin.unsqueeze(1).unsqueeze(0).to(device))\n",
    "        self.register_buffer(\"cos\", cos.unsqueeze(1).unsqueeze(0).to(device))\n",
    "    \n",
    "    @staticmethod\n",
    "    def rotate_neg_vector(\n",
    "        x: Float[torch.Tensor, \"batch seq_len num_heads d_head\"]\n",
    "    ):\n",
    "        # На входе x = [x1, x2, x3, x4, ... x_{n-1}, x_n]\n",
    "        # На выходе x' = [-x2, x1, -x4, x3, ..., -x_n, x_{n-1}]\n",
    "        x_new = torch.empty_like(x)\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x_new = torch.stack([-x2, x1], dim=-1).reshape_as(x)\n",
    "        return x_new.to(device)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Float[torch.Tensor, \"batch seq_len num_heads d_head\"]\n",
    "    ):\n",
    "        old_device = x.device\n",
    "        x = x.to(device)\n",
    "        seq_len = x.size(1)\n",
    "        x_rotated = self.rotate_neg_vector(x)\n",
    "        res = x * self.cos[:, :seq_len, :, :] + x_rotated * self.sin[:, :seq_len, :, :]\n",
    "        return res.to(old_device)\n",
    "    \n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "num_heads = 2\n",
    "d_head = 16\n",
    "\n",
    "torch.manual_seed(1)\n",
    "x = torch.rand(batch_size, seq_len, num_heads, d_head)\n",
    "\n",
    "rope_config = Config(\n",
    "    n_heads=2,\n",
    "    d_head=16,\n",
    ")\n",
    "\n",
    "rope_layer = RotaryPositionalEmbeddings(rope_config)\n",
    "y = rope_layer(x)\n",
    "\n",
    "\n",
    "thetas = [10_000 ** (-2 * (i - 1) / rope_config.d_head) for i in range(1, rope_config.d_head // 2 + 1)]\n",
    "all_good = True\n",
    "for batch_idx in range(batch_size):\n",
    "    for m in range(seq_len):\n",
    "        if not all_good:\n",
    "            break\n",
    "        for head_idx in range(num_heads):\n",
    "            if not all_good:\n",
    "                break\n",
    "            for d_idx in range(d_head):\n",
    "                # 0, 2, 4\n",
    "                if d_idx % 2 == 0:\n",
    "                    val = x[batch_idx, m, head_idx, d_idx] * math.cos(m * thetas[d_idx // 2]) - x[batch_idx, m, head_idx, d_idx + 1] * math.sin(m * thetas[d_idx // 2])\n",
    "                else:\n",
    "                    val = x[batch_idx, m, head_idx, d_idx] * math.cos(m * thetas[d_idx // 2]) + x[batch_idx, m, head_idx, d_idx - 1] * math.sin(m * thetas[d_idx // 2])\n",
    "                if abs(y[batch_idx, m, head_idx, d_idx] - val) > 1e-3:\n",
    "                    print(f\"Ошибка на позиции {m} и размерности {d_idx} в голове {head_idx}\")\n",
    "                    print(f\"Полученное значение {y[batch_idx, m, head_idx, d_idx]}, референс {val}\")\n",
    "                    all_good = False\n",
    "                    break\n",
    "\n",
    "if all_good:\n",
    "    print(\"Тесты прошли успешно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rope X Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithRope(nn.Module):\n",
    "    IGNORE: Float[torch.Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(float(\"-inf\"), dtype=torch.float32, device=device))\n",
    "\n",
    "        self.rope = RotaryPositionalEmbeddings(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Float[torch.Tensor, \"batch seq_len d_model\"], \n",
    "        mask: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        # Берем размерности\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_heads = self.cfg.n_heads\n",
    "        d_head = self.cfg.d_head\n",
    "\n",
    "        # 1. Трансформируем матрицы проекций в формат [d_model, d_model]\n",
    "        W_Q = self.W_Q.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_K = self.W_K.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        W_V = self.W_V.permute(1, 0, 2).reshape(self.cfg.d_model, self.cfg.d_model)\n",
    "        \n",
    "        b_Q = self.b_Q.view(-1)\n",
    "        b_K = self.b_K.view(-1)\n",
    "        b_V = self.b_V.view(-1)\n",
    "        \n",
    "        # 1. получаем проекции  Q, K, V\n",
    "        Q = torch.matmul(x, W_Q) + b_Q\n",
    "        K = torch.matmul(x, W_K) + b_K\n",
    "        V = torch.matmul(x, W_V) + b_V\n",
    "\n",
    "        Q = einops.rearrange(Q, 'b s (n d) -> b n s d', n=num_heads)\n",
    "        K = einops.rearrange(K, 'b s (n d) -> b n s d', n=num_heads) \n",
    "        V = einops.rearrange(V, 'b s (n d) -> b n s d', n=num_heads) \n",
    "\n",
    "        # 2. применяю RoPE для вращения Q и K\n",
    "        Q = self.rope(Q)\n",
    "        K = self.rope(K)\n",
    "\n",
    "        # 3. Q x K^T\n",
    "        scores = torch.einsum('b n i d, b n j d -> b n i j', Q, K)\n",
    "        \n",
    "        # 4. Нормализация\n",
    "        scores /= math.sqrt(d_head)\n",
    "        \n",
    "        # 5. Маскирование\n",
    "        scores = self.apply_causal_mask(scores, mask)\n",
    "        \n",
    "        # 6. softmax\n",
    "        scores = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # 7. Финальная проекция\n",
    "        attention = torch.einsum('b h i j, b h j d -> b h i d', scores, V)\n",
    "        attention = torch.einsum('b h i d, h d m -> b i m', attention, self.W_O) + self.b_O\n",
    "\n",
    "        return attention\n",
    "    \n",
    "    def apply_causal_mask(\n",
    "        self, \n",
    "        attn_scores: Float[torch.Tensor, \"batch n_heads seq_len seq_len\"], \n",
    "        mask: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch n_heads seq_len seq_len\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        Используем треугольную маску, чтобы не смотреть в будущее.\n",
    "        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n",
    "        '''\n",
    "        seq_len = attn_scores.size(-1)\n",
    "        old_device = attn_scores.device\n",
    "        attn_scores = attn_scores.to(device)\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask, value=self.IGNORE)\n",
    "\n",
    "        mask = mask.to(device)\n",
    "        padding_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        attn_scores = attn_scores.masked_fill(padding_mask == 0, value=self.IGNORE)\n",
    "\n",
    "        return attn_scores.to(old_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = RMSNorm(cfg)\n",
    "        self.attn = AttentionWithRope(cfg)\n",
    "        self.ln2 = RMSNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Float[torch.Tensor, \"batch seq_len d_model\"],\n",
    "        mask: Float[torch.Tensor, \"batch seq_len\"] \n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_model\"]:\n",
    "        x = x + self.attn(self.ln1(x), mask)\n",
    "        return x + self.mlp(self.ln2(x))\n",
    "\n",
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = RMSNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: Int[torch.Tensor, \"batch seq_len\"],\n",
    "        mask: Int[torch.Tensor, \"batch seq_len\"]\n",
    "    ) -> Float[torch.Tensor, \"batch seq_len d_vocab\"]:\n",
    "        x = self.embed(input_ids)\n",
    "        # x = self.embed(input_ids) + self.pos_embed(input_ids)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = Config(\n",
    "    d_model=128,\n",
    "    n_ctx=512,\n",
    "    n_heads=8,\n",
    "    d_head=16,\n",
    "    d_mlp=512,\n",
    "    n_layers=12\n",
    ")\n",
    "model = DemoTransformer(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_ids, mask in train_loader:\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    p = model(input_ids, mask)\n",
    "\n",
    "assert list(p.shape) == [input_ids.size(0), input_ids.size(1), train_config.d_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Берем input_ids, mask, прогоняем через модель, получаем тензор p `[batch_size, seq_len, vocab_size]`\n",
    "2. В качестве меток мы берем **те же input_ids**. Только их нужно сдвинуть на 1 вправо, т.к. i-й токен предсказывает (i + 1)-й\n",
    "3. В качестве предиктов берем **input_ids**. Только начало нужно тоже обрезать, т.к. у нас нет токенов, которые занимались бы предсказанием 0-го токена в последоватсельности\n",
    "4. Паддингам ставим метки -100, это значение ignore_loss, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) его игнорирует при подсчете лососв\n",
    "5. Превращаем p в тензор `[batch_size * (seq_len - 1), vocab_size]`, вектор правильных меток labels (из input_ids) превращаем в `[batch_size * (seq_len - 1)]`, считаем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "pad_id = 50256\n",
    "\n",
    "def calculate_loss(critertion, logits, input_ids, pad_id=pad_id):\n",
    "    logits = logits.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "\n",
    "    sequence = input_ids[:, 1:]\n",
    "    last_vector = torch.full((input_ids.size(0), 1), -100).to(device)\n",
    "    shifted_ids = torch.cat((sequence, last_vector), dim=1)\n",
    "    shifted_ids[shifted_ids == pad_id] = -100\n",
    "\n",
    "    return critertion(logits.view(-1, logits.size(-1)), shifted_ids.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "num_classes = 7\n",
    "\n",
    "# batch_size seq_len\n",
    "input_ids = torch.LongTensor(\n",
    "    [\n",
    "        [0, 1,  pad_id, pad_id],\n",
    "        [0, 1, 2, 3]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# batch_size, seq_len, num_classes\n",
    "logits = torch.Tensor(\n",
    "    [[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971],\n",
    "         [0.7544, 0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051],\n",
    "         [0.4635, 0.4550, 0.5725, 0.4980, 0.9371, 0.6556, 0.3138],\n",
    "         [0.1980, 0.4162, 0.2843, 0.3398, 0.5239, 0.7981, 0.7718]],\n",
    "\n",
    "        [[0.0112, 0.8100, 0.6397, 0.9743, 0.8300, 0.0444, 0.0246],\n",
    "         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676, 0.9906, 0.2885],\n",
    "         [0.8750, 0.5059, 0.2366, 0.7570, 0.2346, 0.6471, 0.3556],\n",
    "         [0.4452, 0.0193, 0.2616, 0.7713, 0.3785, 0.9980, 0.9008]]]\n",
    ")\n",
    "\n",
    "loss = calculate_loss(criterion, logits, input_ids)\n",
    "\n",
    "assert (loss.item() - 1.9343) < 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    for input_ids, mask in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids.to(device), mask.to(device))\n",
    "        loss = calculate_loss(criterion, logits, input_ids)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "1. Подаем input_ids, mask\n",
    "2. По последнему токену жадно предсказываем следующий\n",
    "3. Конактенируем этот токен к input_ids, расширяем mask\n",
    "4. Повторяем num_tokens_to_generate раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = text[:5]\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "orig_size = input_ids.size(1)\n",
    "\n",
    "num_tokens_to_generate = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_tokens_to_generate):\n",
    "        output = model(input_ids, mask)\n",
    "        logits = output[:, -1, :]\n",
    "        output = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        input_ids = torch.cat([input_ids, output], dim=-1)\n",
    "        output_mask = torch.ones((mask.size(0), 1), dtype=torch.long).to(device)\n",
    "        mask = torch.cat([mask, output_mask], dim=-1)\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Input text + Generated\", tokenizer.decode(input_ids[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "287f177bea3a4a1ebf5d4fd0af5fc8ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "313dff4a97b64f60a4e2b4a12af40cc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3db66262f47f4e9098999b64246cd5da",
       "IPY_MODEL_545fbc08d4404b5f9e5251758ac15ee0",
       "IPY_MODEL_4bf13e51b72d4204a6e6bd723208495a"
      ],
      "layout": "IPY_MODEL_287f177bea3a4a1ebf5d4fd0af5fc8ce"
     }
    },
    "3db66262f47f4e9098999b64246cd5da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40bf9f987ee94484b44407999e1b29ac",
      "placeholder": "​",
      "style": "IPY_MODEL_fbe8106a981b4f6a847fc581a397ee79",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "40bf9f987ee94484b44407999e1b29ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bf13e51b72d4204a6e6bd723208495a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6184db87bde246fabe674a271cf5ef60",
      "placeholder": "​",
      "style": "IPY_MODEL_ca480e3ade314b44b333d83891d45bef",
      "value": " 4/4 [00:02&lt;00:00,  1.91it/s]"
     }
    },
    "545fbc08d4404b5f9e5251758ac15ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71225365696d44afa104150576c60b9c",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eed5663c31194e72800e241234cd804c",
      "value": 4
     }
    },
    "6184db87bde246fabe674a271cf5ef60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71225365696d44afa104150576c60b9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca480e3ade314b44b333d83891d45bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eed5663c31194e72800e241234cd804c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fbe8106a981b4f6a847fc581a397ee79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
