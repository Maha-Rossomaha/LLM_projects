{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f5a017",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Case\n",
    "> Требуется улучшить этап отбора кандидатов в поисковой веб-системе. На текущий момент в качестве кандгена (кандидатогенерации) используется BM25 и обратный индекс. BM25 уже тюнили, дальше качество нарастить не выходит. В качестве бизнес-метрики можем взять производные поведенческого отклика, например, CTR@K или timespent на выдаче и документах.\n",
    "\n",
    "Очевидным направлением развития является построение нейросетевого кандгена. Обычно в описанных случаях действуют следующим образом:\n",
    "0. Выбирают ML-метрику, которую хотелось бы оптимизировать. Для кандгена катастрофически важно выдать как можно больше релеватных документов в пределах фиксированной длины выдачи, поэтому подходящая метрика -- Recall@K. Мы будем использовать ее модификацию, но об этом позже.\n",
    "1. Сэмплируют запросы из потока / формируют специфичные корзины запросов в зависимости от дополнительных бизнес-требований. Давайте считать, что они отсутствуют. Тут обязателен контроль их качества, можно исходить из символьных эвристик или применять LLM для классификации, как вы это делали в предыдущей домашке.\n",
    "2. Обкачивают поисковый движок, формируя глубокие выдачи. Эпитет \"глубокие\" относится к глубине погружения пользователя в выдачу, то есть предельные позиции взаимодействия с документами. Так вот для обучения требуется брать документов в избытке, в том числе те, с которыми пользователь никогда бы не повзаимодействовал. В целом, длина выдачи 1000 -- отличный выбор. Предварительно есть смысл сгладить все условия отбора по BM25.\n",
    "3. Разметка пар запрос-документ на задачу релевантности. LLM -- вновь отличный выбор. Разметка порядковая, но может быть как бинарной, так и n-арной. Важно сформировать определение \"релевантного\" документа, то есть определить порог, по которому мы будем считать документ подходящим под запрос.\n",
    "4. Релевантные пары запрос-документ берем в качестве позитивов, выбираем базовый эмбеддер и учим его контрастивно как bi-энкодер на эту выборку, негативы можем формировать в режиме in-batch.\n",
    "5. Если все сделано верно (данных достаточно, гиперпараметры подобраны, код не багованный), естественным следствием будет рост качества.\n",
    "\n",
    "Датасет, на котором мы будем строить кандген -- MS Marco Dev. \n",
    "\n",
    "Библиотека для работы с датасетами - `ir_datasets` ([API](https://ir-datasets.com/python.html)). \"IR\" от Information Retrieval - библиотека содержит инструменты работы с датасетами поиска.  \n",
    "В коде будет использоваться `polars` ([API](https://docs.pola.rs/api/python/stable/reference/index.html)), аналог `pandas`, только на порядки быстрее.\n",
    "\n",
    "[Описание датасета](https://ir-datasets.com/msmarco-passage.html#msmarco-passage/dev/judged)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c991d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5159a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ir-datasets -q\n",
    "!pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import faiss\n",
    "import random\n",
    "import numpy as np\n",
    "import ir_datasets\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1709701",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4889e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Columns:\n",
    "    query_id: str = \"query_id\"\n",
    "    doc_id: str = \"doc_id\"\n",
    "    index_id: str = \"index_id\"\n",
    "    text: str = \"text\"\n",
    "    qrels_relevance: str = \"relevance\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    sampled_index_size: int = 150_000\n",
    "    relevance_threshold: int = 1\n",
    "    test_size: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(\"msmarco-passage/dev/judged\")\n",
    "\n",
    "columns = Columns()\n",
    "dataset_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea51812",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pl.DataFrame(dataset.queries_iter()).select(\n",
    "    pl.col(columns.query_id).cast(pl.Int32),\n",
    "    pl.col(columns.text)\n",
    ")\n",
    "\n",
    "qrels = pl.DataFrame(dataset.qrels_iter()).drop(\"iteration\").select(\n",
    "    pl.col(columns.query_id).cast(pl.Int32),\n",
    "    pl.col(columns.doc_id).cast(pl.Int32),\n",
    "    pl.col(columns.qrels_relevance).cast(pl.Int32)\n",
    ")\n",
    "\n",
    "documents = pl.DataFrame(dataset.docs_iter()).select(\n",
    "    pl.col(columns.doc_id).cast(pl.Int32),\n",
    "    pl.col(columns.text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_document_ids = qrels[columns.doc_id].unique().to_list()\n",
    "sampled_document_ids = np.random.default_rng().integers(dataset.docs_count(), size=dataset_config.sampled_index_size).tolist()\n",
    "\n",
    "sampled_documents = documents.filter(pl.col(columns.doc_id).is_in(sampled_document_ids + target_document_ids)).with_row_index(columns.index_id)\n",
    "len(target_document_ids), len(sampled_document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids = qrels[columns.query_id].to_numpy()\n",
    "doc_ids = qrels[columns.doc_id].to_numpy()\n",
    "\n",
    "# Разбиваем индексы\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(qrels)),\n",
    "    test_size=dataset_config.test_size,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_qrels = qrels[train_idx]\n",
    "test_qrels = qrels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = queries.filter(pl.col(columns.query_id).is_in(train_qrels[columns.query_id].to_list()))\n",
    "test_queries = queries.filter(pl.col(columns.query_id).is_in(test_qrels[columns.query_id].to_list()))\n",
    "\n",
    "train_documents = sampled_documents.filter(pl.col(columns.doc_id).is_in(train_qrels[columns.doc_id].to_list()))\n",
    "test_documents = sampled_documents.filter(pl.col(columns.doc_id).is_in(test_qrels[columns.doc_id].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec8e2b",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainTestConfig:\n",
    "    device: str = torch.device([\"cpu\", \"cuda\"][torch.cuda.is_available()])\n",
    "    batch_size: int = 32\n",
    "    max_query_len: int = 32\n",
    "    max_doc_len: int = 128\n",
    "    sampled_index_size: int = 150_000\n",
    "    recalls_k: list = (1, 5, 10, 50)\n",
    "    acc_steps: int = 4\n",
    "    epochs: int = 3\n",
    "    relevance_threshold: int = 1\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str = \"microsoft/deberta-v3-small\"\n",
    "    agg: str = \"mean\"\n",
    "    freeze_base: bool = False\n",
    "    out_dim: int = 256\n",
    "\n",
    "@dataclass\n",
    "class LossConfig:\n",
    "    thrsh: float = 0.1\n",
    "    temperature: float = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = Columns()\n",
    "dataset_config = DatasetConfig()\n",
    "train_test_config = TrainTestConfig()\n",
    "\n",
    "loss_config = LossConfig()\n",
    "model_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8419b",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "`DenseRetrievalDataset` - датасет, который внутри формирует множество релевантных пар и выдает на каждый индекс произвольную пару оттуда вместе с `query_id` и `doc_id`.\n",
    "\n",
    "`train_collate_fn` - функция, которая внутри токенизирует батчем текст запроса и документа и отдает кортеж из тензоров, в которые включаются id запросов и документов, токены запросов и документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRetrievalDataset(Dataset):\n",
    "    def __init__(self, queries, documents, qrels, columns, config):\n",
    "        self.columns = columns\n",
    "        self.config = config\n",
    "\n",
    "        self.queries = queries\n",
    "        self.documents = documents\n",
    "        self.qrels = qrels.filter(\n",
    "            pl.col(columns.qrels_relevance) >= config.relevance_threshold\n",
    "        )\n",
    "        self.query_texts = dict(zip(\n",
    "            queries[columns.query_id].to_list(),\n",
    "            queries[columns.text].to_list()\n",
    "        ))\n",
    "\n",
    "        self.doc_texts = dict(zip(\n",
    "            documents[columns.doc_id].to_list(),\n",
    "            documents[columns.text].to_list()\n",
    "        ))\n",
    "\n",
    "        self.relevant = {}\n",
    "        for row in self.qrels.iter_rows(named=True):\n",
    "            query_id = row[columns.query_id]\n",
    "            doc_id = row[columns.doc_id]\n",
    "            if query_id in self.query_texts and doc_id in self.doc_texts:\n",
    "                self.relevant.setdefault(query_id, []).append(doc_id)\n",
    "\n",
    "        self.query_ids = list(self.relevant.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_id = self.query_ids[idx]\n",
    "\n",
    "        doc_id = random.choice(self.relevant[query_id])\n",
    "        return {\n",
    "            \"query_id\": query_id,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"query_text\": self.query_texts[query_id],\n",
    "            \"doc_text\": self.doc_texts[doc_id]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(data, tokenizer, config):\n",
    "    query_texts = [item[\"query_text\"] for item in data]\n",
    "    doc_texts = [item[\"doc_text\"] for item in data]\n",
    "    query_ids = [item[\"query_id\"] for item in data]\n",
    "    doc_ids = [item[\"doc_id\"] for item in data]\n",
    "\n",
    "    query_tokens = tokenizer(\n",
    "        query_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=config.max_query_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    doc_tokens = tokenizer(\n",
    "        doc_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=config.max_doc_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        torch.tensor(query_ids),\n",
    "        torch.tensor(doc_ids),\n",
    "        query_tokens,\n",
    "        doc_tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea63ba1",
   "metadata": {},
   "source": [
    "### Loss Function (InfoNCE + In-batch negatives mining)\n",
    "\n",
    "`ContrastiveLoss`, который реализует расчет следующей функции ошибки:\n",
    "$$\\mathcal{L}=\\mathbb{E}_T\\text{CrossEntropy}\\left(q_iD^T-B_i, M_i\\right)$$\n",
    "$$T=\\{Q, D\\},\\quad Q=\\{q_i\\big|q_i\\in\\mathbb{R}^n,\\|q_i\\|_2=1\\}_{i=1}^N,\\quad D=\\{d_i\\big|d_i\\in\\mathbb{R}^n,\\|d_i\\|_2=1\\}_{i=1}^N$$\n",
    "$$(q_i, d_i) \\,-\\,\\text{позитивная пара}$$\n",
    "$$M_i\\in[0,1]^N,\\quad \\forall{j}\\in\\overline{1,N}:\\;M[j]=\\frac{[q_i = q_j]}{\\sum\\limits_k{[q_i=q_k]}}$$\n",
    "$$B_i\\in[0,1]^N,\\quad \\forall{j}\\in\\overline{1,N}:\\;M[j]=b*[q_i = q_j]$$\n",
    "$$b\\,-\\,\\text{вещественный гиперпараметр}$$\n",
    "\n",
    "Смысл $b$ - [статья LaBSE](https://arxiv.org/pdf/2007.01852), _Additive Margin Softmax_. (штрафуем позитивы, вынуждая модель еще больше поднимать косинус, чтобы компенсировать штраф)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ead770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.thrsh = cfg.thrsh # то самое b\n",
    "        self.temperature = cfg.temperature  # τ\n",
    "\n",
    "    def forward(self, queries, documents, labels):\n",
    "        q = F.normalize(queries,  dim=1, p=2)\n",
    "        d = F.normalize(documents, dim=1, p=2)\n",
    "\n",
    "        logits = (q @ d.T) / self.temperature\n",
    "\n",
    "        logits = logits - self.thrsh * (labels > 0).float()\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(labels * log_probs).sum(dim=1).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4646d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87a966",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f054ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.agg = config.agg.lower()\n",
    "        cfg = AutoConfig.from_pretrained(config.model_name, output_hidden_states=False)\n",
    "        self.backbone = AutoModel.from_pretrained(config.model_name, config=cfg)\n",
    "\n",
    "        if config.freeze_base:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.head = nn.Linear(self.backbone.config.hidden_size, config.out_dim, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.backbone(**input).last_hidden_state\n",
    "        if self.agg == 'cls':\n",
    "            x = x[:, 0]\n",
    "        elif self.agg == 'mean':\n",
    "            mask = input[\"attention_mask\"].unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "        x = self.head(x)\n",
    "        return F.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5a976",
   "metadata": {},
   "source": [
    "### Test metric and inference\n",
    "\n",
    "Пайплайн для оценки работы retriever с использованием модифицированной метрики Recall@K на тестовой выборке.\n",
    "\n",
    "##### 1. Функция `inference`\n",
    "\n",
    "Выполняет прогон эмбеддера по списку текстов.\n",
    "\n",
    "* Для запросов (`is_query=True`) и документов (`is_query=False`) используется разная максимальная длина токенизации.\n",
    "* Тексты разбиваются на батчи и токенизируются.\n",
    "* Эмбеддер переводится в режим `eval` и на заданное устройство (`config.device`).\n",
    "* Вычисленные векторы собираются и возвращаются в виде одного тензора.\n",
    "\n",
    "##### 2. Функция `calc_recall`\n",
    "\n",
    "Вычисляет усреднённый модифицированный `Recall@K` для набора запросов.\n",
    "\n",
    "* Из FAISS-индекса извлекаются top-K ближайших документов для каждого запроса.\n",
    "* ID документов из индекса сопоставляются с исходными документами.\n",
    "* Для каждого запроса считается доля релевантных документов в выдаче на уровне каждого `K` из списка `config.recalls_k`.\n",
    "* Итоговое значение — среднее по всем валидным запросам.\n",
    "\n",
    "Формула:\n",
    "\n",
    "$$\n",
    "Recall@K = \\frac{\\text{\\# релевантных документов в top-K}}{\\min(\\text{\\# всех релевантных}, \\text{\\# документов в выдаче})}\n",
    "$$\n",
    "\n",
    "##### 3. Вспомогательная функция `_ensure_index_id`\n",
    "\n",
    "Гарантирует наличие уникального поля с индексом документа (`index_id`) в DataFrame.\n",
    "\n",
    "##### 4. Функция `test_retriever`\n",
    "\n",
    "Запускает полный процесс оценки:\n",
    "\n",
    "1. При необходимости уменьшает набор документов для ускорения теста.\n",
    "2. Генерирует эмбеддинги документов и нормализует их (`faiss.normalize_L2`).\n",
    "3. Строит FAISS-индекс по эмбеддингам документов.\n",
    "4. Генерирует эмбеддинги запросов и нормализует их.\n",
    "5. Передаёт индекс, векторы запросов и данные релевантности (`qrels`) в `calc_recall`.\n",
    "6. Возвращает словарь с `Recall@K` для всех заданных значений `K`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference(embedder, texts, is_query, config, tokenizer):\n",
    "    max_len = config.max_query_len if is_query else config.max_doc_len\n",
    "\n",
    "    dl = DataLoader(texts, batch_size=config.batch_size, shuffle=False)\n",
    "    all_vecs = []\n",
    "\n",
    "    embedder.eval()\n",
    "    embedder.to(config.device)\n",
    "\n",
    "    for batch in tqdm(dl, desc=\"Inference\", total=len(dl)):\n",
    "        tokenized_batch = tokenizer(\n",
    "            batch,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_batch = {k: v.to(config.device) for k, v in tokenized_batch.items()}\n",
    "        vec = embedder(tokenized_batch)\n",
    "        all_vecs.append(vec.cpu())\n",
    "\n",
    "    return torch.cat(all_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(index, query_embeddings, query_ids, qrels, documents, columns, config):\n",
    "    Ks = sorted(config.recalls_k)\n",
    "    max_k = Ks[-1]\n",
    "\n",
    "    query_embs_np = query_embeddings.astype(\"float32\", copy=False)\n",
    "\n",
    "    _, idx_mat = index.search(query_embs_np, max_k)\n",
    "\n",
    "    index_ids = documents[columns.index_id].to_numpy()\n",
    "    doc_ids = documents[columns.doc_id].to_numpy()\n",
    "    id_map = dict(zip(index_ids, doc_ids))\n",
    "\n",
    "    doc_id_mat = np.vectorize(id_map.get)(idx_mat, -1)\n",
    "\n",
    "    rel_dict = defaultdict(set)\n",
    "    for row in qrels.iter_rows(named=True):\n",
    "        if row[columns.qrels_relevance] >= config.relevance_threshold:\n",
    "            rel_dict[row[columns.query_id]].add(row[columns.doc_id])\n",
    "\n",
    "    recall_sum = {k: 0.0 for k in Ks}\n",
    "    valid_queries = 0\n",
    "\n",
    "    for row_idx, query_id in enumerate(query_ids):\n",
    "        relevant = rel_dict.get(query_id)\n",
    "        if not relevant:\n",
    "            continue\n",
    "\n",
    "        retrieved_ids = doc_id_mat[row_idx]\n",
    "        valid_queries += 1\n",
    "\n",
    "        for k in Ks:\n",
    "            topk = set(retrieved_ids[:k])\n",
    "            topk.discard(-1)\n",
    "\n",
    "            intersection = topk & relevant\n",
    "            denom = min(len(relevant), len(topk))\n",
    "            recall_sum[k] += len(intersection) / denom if denom else 0.0\n",
    "\n",
    "    recall_mean = {k: (recall_sum[k] / valid_queries if valid_queries else 0.0) for k in Ks}\n",
    "    return recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_index_id(df: pl.DataFrame, name):\n",
    "    if name in df.columns:\n",
    "        if df[name].n_unique() != df.height:\n",
    "            df = df.with_columns(pl.arange(0, df.height).alias(name))\n",
    "        return df\n",
    "    else:\n",
    "        return df.with_row_index(name, offset=0)\n",
    "\n",
    "\n",
    "def test_retriever(embedder, test_queries, test_qrels, documents, columns,\n",
    "                   config, tokenizer, use_small_eval_set=False, subset_size=500):\n",
    "    if use_small_eval_set:\n",
    "        docs_eval = documents[:subset_size]\n",
    "    else:\n",
    "        docs_eval = documents\n",
    "    docs_df = _ensure_index_id(docs_eval, columns.index_id)\n",
    "\n",
    "    doc_vecs = inference(\n",
    "        embedder,\n",
    "        docs_df[columns.text].to_list(),\n",
    "        is_query=False,\n",
    "        config=config,\n",
    "        tokenizer=tokenizer\n",
    "    ).cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    faiss.normalize_L2(doc_vecs)\n",
    "\n",
    "    dim = doc_vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    index.add(doc_vecs)\n",
    "\n",
    "    q_vecs = inference(\n",
    "        embedder,\n",
    "        test_queries[columns.text].to_list(),\n",
    "        is_query=True,\n",
    "        config=config,\n",
    "        tokenizer=tokenizer\n",
    "    ).cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    faiss.normalize_L2(q_vecs)\n",
    "\n",
    "    q_ids = test_queries[columns.query_id].to_numpy()\n",
    "\n",
    "    recall = calc_recall(\n",
    "        index=index,\n",
    "        query_embeddings=q_vecs,\n",
    "        query_ids=q_ids,\n",
    "        qrels=test_qrels,\n",
    "        documents=docs_df,\n",
    "        columns=columns,\n",
    "        config=config\n",
    "    )\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a608a",
   "metadata": {},
   "source": [
    "### Training function + Gradient accumulation\n",
    "\n",
    "Готовим лоадеры, обучаем модель в контрастимном режиме и считаем метрику раз в эпоху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_retriever(\n",
    "    embedder, train_queries, train_documents,\n",
    "    train_qrels, test_queries, test_qrels,\n",
    "    documents, columns, dataset_config,\n",
    "    train_test_config, loss_config, tokenizer\n",
    "):\n",
    "    train_ds = DenseRetrievalDataset(\n",
    "        train_queries, train_documents, train_qrels,\n",
    "        columns, dataset_config\n",
    "    )\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=train_test_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=lambda b: train_collate_fn(b, tokenizer, train_test_config),\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    optim  = AdamW(\n",
    "        filter(lambda p: p.requires_grad, embedder.parameters()),\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = ContrastiveLoss(loss_config)\n",
    "\n",
    "    embedder.to(train_test_config.device).train()\n",
    "\n",
    "    for ep in range(1, train_test_config.epochs + 1):\n",
    "        pbar = tqdm(train_dl, desc=f\"E{ep}\")\n",
    "        running = 0.0\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step, batch in enumerate(pbar, start=1):\n",
    "            q_ids, d_ids, q_tok, d_tok = batch\n",
    "            q_tok = {k: v.to(train_test_config.device) for k, v in q_tok.items()}\n",
    "            d_tok = {k: v.to(train_test_config.device) for k, v in d_tok.items()}\n",
    "\n",
    "            with torch.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                q_vec = embedder(q_tok)\n",
    "                d_vec = embedder(d_tok)\n",
    "\n",
    "                labels = torch.eye(q_vec.size(0), device=train_test_config.device)\n",
    "\n",
    "                loss_qd = loss_fn(q_vec, d_vec, labels)\n",
    "                loss_dq = loss_fn(d_vec, q_vec, labels)\n",
    "\n",
    "                loss = 0.5 * (loss_qd + loss_dq) / train_test_config.acc_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running += loss.item() * train_test_config.acc_steps\n",
    "\n",
    "            if step % train_test_config.acc_steps == 0:\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(embedder.parameters(), 1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{running/step:.4f}\")\n",
    "\n",
    "        embedder.eval()\n",
    "        with torch.no_grad():\n",
    "            scores = test_retriever(\n",
    "                embedder, test_queries, test_qrels,\n",
    "                documents, columns, train_test_config, tokenizer\n",
    "            )\n",
    "        print(f\"Epoch {ep}  Recall: {scores}\")\n",
    "        embedder.train()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ec098",
   "metadata": {},
   "source": [
    "### Final\n",
    "\n",
    "Обучаем эмбеддер и сравниваем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(model_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)\n",
    "embedder.to(train_test_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd825c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.eval()\n",
    "baseline_recall = test_retriever(\n",
    "    embedder=embedder,\n",
    "    test_queries=test_queries,\n",
    "    test_qrels=test_qrels,\n",
    "    documents=sampled_documents,\n",
    "    columns=columns,\n",
    "    config=train_test_config,\n",
    "    tokenizer=tokenizer,\n",
    "    use_small_eval_set=False,\n",
    "    subset_size=500\n",
    ")\n",
    "print(\"Baseline Recall:\", baseline_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0611c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recall = train_retriever(\n",
    "    embedder=embedder,\n",
    "    train_queries=train_queries,\n",
    "    train_documents=train_documents,\n",
    "    train_qrels=train_qrels,\n",
    "    test_queries=test_queries,\n",
    "    test_qrels=test_qrels,\n",
    "    documents=sampled_documents,\n",
    "    columns=columns,\n",
    "    dataset_config=dataset_config,\n",
    "    train_test_config=train_test_config,\n",
    "    loss_config=loss_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Recall после обучения:\", final_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.eval()\n",
    "\n",
    "after_recall = test_retriever(\n",
    "    embedder=embedder,\n",
    "    test_queries=test_queries,\n",
    "    test_qrels=test_qrels,\n",
    "    documents=sampled_documents,\n",
    "    columns=columns,\n",
    "    config=train_test_config,\n",
    "    tokenizer=tokenizer,\n",
    "    use_small_eval_set=False,\n",
    "    subset_size=500\n",
    ")\n",
    "\n",
    "print(\"Recall до обучения:\", baseline_recall)\n",
    "print(\"Recall после обучения:\", after_recall)\n",
    "\n",
    "for k in train_test_config.recalls_k:\n",
    "    delta = after_recall[k] - baseline_recall[k]\n",
    "    print(f\"Delta Recall@{k:<2}: {delta:+.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
