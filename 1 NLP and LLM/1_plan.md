# ĞŸĞ»Ğ°Ğ½ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM + NLP (Senior, Search/RecSys)

## I. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚
**Ğ¦ĞµĞ»ÑŒ:** ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ LLM, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.

- ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Transformer: attention, encoder/decoder ÑÑ…ĞµĞ¼Ñ‹  
  ğŸ”— [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/)  
  ğŸ”— [Transformers code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)  
  ğŸ”— [Attention is all you need](https://arxiv.org/pdf/1706.03762)  
  ğŸ”— [Mixture of Experts Explained](https://huggingface.co/blog/moe)  
  ğŸ”— [A Visual Guide to MoE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)  
  ğŸ”— [Understanding Mixture of Experts: Building a MoE Model with PyTorch](https://medium.com/@prateeksikdar/understanding-mixture-of-experts-building-a-moe-model-with-pytorch-dd373d9db81c)

- ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹: BERT, GPT, T5, LLaMA, Mistral, Claude, Gemini Ğ¸ Ñ‚.Ğ¿.  
  ğŸ”— [BERT](https://huggingface.co/blog/bert-101)  
  ğŸ”— [GPT-3](https://dugas.ch/artificial_curiosity/GPT_architecture.html)  
  ğŸ”— [T5](https://medium.com/40gagangupta_82781understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)  
  ğŸ”— [LLama3-1](https://ai.meta.com/blog/meta-llama-3-1/)  
  ğŸ”— [LLama4-1](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)  
  ğŸ”— [Mistral 7b](https://medium.com/dair-ai/papers-explained-mistral-7b-b9632dedf580)  
  ğŸ”— [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts)  
  ğŸ”— [Gemini 2.5](https://arxiv.org/pdf/2507.06261)  

- ĞÑ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ: causal vs masked LM, decoder-only vs encoder-decoder  
  ğŸ”— https://huggingface.co/course/chapter1  
  ğŸ”— https://medium.com/%40tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa  
  ğŸ”— https://huggingface.co/docs/transformers/en/tasks/language_modeling

- Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (BPE, SentencePiece)  
  ğŸ”— https://huggingface.co/course/chapter6  
  ğŸ”— https://huggingface.co/docs/tokenizers/index

- Ğ¡Ğ»Ğ¾Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Embeddings, LayerNorm, FeedForward, Positional Encoding, Attention  
  ğŸ”— https://lilianweng.github.io/lil-log/

- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ `transformers`, `config`, `forward`, `past_key_values`  
  ğŸ”— https://huggingface.co/docs/transformers/index

---

## II. Ğ¤Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
**Ğ¦ĞµĞ»ÑŒ:** Ğ£Ğ¼ĞµÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±ÑƒÑ LLM Ğ¿Ğ¾Ğ´ ÑĞ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.

- ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ fine-tuning (FP32, full finetune)  
  ğŸ”— https://huggingface.co/course/chapter3

- Parameter-efficient tuning:  
  ğŸ”— https://github.com/huggingface/peft  
  ğŸ”— https://arxiv.org/abs/2305.14314 (QLoRA)  
  ğŸ”— https://sebastianraschka.com/blog/2023/peft-qlora.html

- Low-bit inference: quantization (int8, int4), `bitsandbytes`, `AutoGPTQ`  
  ğŸ”— https://github.com/TimDettmers/bitsandbytes  
  ğŸ”— https://github.com/PanQiWei/AutoGPTQ

- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ `PEFT`, `Trainer`, `accelerate`, `deepspeed`  
  ğŸ”— https://github.com/huggingface/accelerate  
  ğŸ”— https://huggingface.co/docs/transformers/perf_train_gpu_one

- ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (NTK scaling, FlashAttention, LongContext)  
  ğŸ”— https://github.com/Dao-AILab/flash-attention  
  ğŸ”— https://huggingface.co/LongChat  
  ğŸ”— https://blog.llamaindex.ai/long-context-llms/

---

## III. Prompt Engineering Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
**Ğ¦ĞµĞ»ÑŒ:** ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ»ÑĞ±Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.

- Few-shot, zero-shot, CoT, ReAct, Self-Ask  
  ğŸ”— https://github.com/dair-ai/Prompt-Engineering-Guide  
  ğŸ”— https://github.com/openai/openai-cookbook

- Temperature, top_p, repetition_penalty  
  ğŸ”— https://platform.openai.com/docs/guides/text-generation

- Prompt compression, reranking, robustness  
  ğŸ”— https://arxiv.org/abs/2309.02772 (Prompt Compression for LLMs)

- Chain-of-prompt, tree-of-thought, multi-hop reasoning  
  ğŸ”— https://arxiv.org/abs/2305.10601 (Tree of Thought)  
  ğŸ”— https://github.com/kyegomez/tree-of-thoughts

- Prompt evaluation  
  ğŸ”— https://github.com/promptfoo/promptfoo  
  ğŸ”— https://github.com/open-eval/open-eval

---

## IV. Embeddings Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ
**Ğ¦ĞµĞ»ÑŒ:** Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ dense-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.

- Sentence embeddings, contextual embeddings, dense vs sparse  
  ğŸ”— https://www.sbert.net/

- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: GTE, BGE, E5, MiniLM, Cohere Embed, Ada  
  ğŸ”— https://huggingface.co/intfloat/e5-large-v2  
  ğŸ”— https://huggingface.co/BAAI/bge-base-en  
  ğŸ”— https://cohere.com/docs/embed

- Triplet loss, contrastive learning  
  ğŸ”— https://www.pinecone.io/learn/series/fine-tune-llm/contrastive-learning/

- ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ  
  ğŸ”— https://scikit-learn.org/stable/modules/clustering.html  
  ğŸ”— https://umap-learn.readthedocs.io/en/latest/

---

## V. Retrieval Ğ¸ RAG
**Ğ¦ĞµĞ»ÑŒ:** Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ retrieval-augmented generation Ğ¸ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ LLM Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸.

- ĞÑĞ½Ğ¾Ğ²Ñ‹ RAG  
  ğŸ”— https://arxiv.org/abs/2005.11401  
  ğŸ”— https://www.pinecone.io/learn/retrieval-augmented-generation/  
  ğŸ”— https://www.llamaindex.ai/guides/retrievers/rag-intro

- FAISS, pgvector, Chroma, Pinecone  
  ğŸ”— https://github.com/facebookresearch/faiss  
  ğŸ”— https://github.com/pgvector/pgvector  
  ğŸ”— https://github.com/chroma-core/chroma  
  ğŸ”— https://www.pinecone.io/docs/

- Hybrid search, reranking:  
  ğŸ”— https://zilliz.com/blog/hybrid-search  
  ğŸ”— https://github.com/stanford-futuredata/ColBERT  
  ğŸ”— https://github.com/naver/splade

- RAG evaluation:  
  ğŸ”— https://github.com/explodinggradients/ragas

---

## VI. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ inference Ğ¸ latency
**Ğ¦ĞµĞ»ÑŒ:** Ğ Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.

- vLLM  
  ğŸ”— https://docs.vllm.ai/

- TGI (Text Generation Inference)  
  ğŸ”— https://huggingface.co/docs/text-generation-inference

- Flash Attention, speculative decoding  
  ğŸ”— https://github.com/Dao-AILab/flash-attention  
  ğŸ”— https://arxiv.org/abs/2302.01318 (Speculative Decoding)

- Quantization-aware training, AutoGPTQ  
  ğŸ”— https://github.com/PanQiWei/AutoGPTQ  
  ğŸ”— https://github.com/huggingface/optimum

---

## VII. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
**Ğ¦ĞµĞ»ÑŒ:** ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ‚Ğ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.

- LM Evaluation  
  ğŸ”— https://github.com/EleutherAI/lm-evaluation-harness  
  ğŸ”— https://github.com/open-eval/open-eval

- RAG evaluation  
  ğŸ”— https://github.com/explodinggradients/ragas  
  ğŸ”— https://github.com/facebookresearch/RA-Eval

- Attention tracing, token-level logit analysis  
  ğŸ”— https://github.com/cdpierse/transformers-interpret  
  ğŸ”— https://github.com/jessevig/bertviz

- Adversarial prompting, hallucination reduction  
  ğŸ”— https://github.com/thunlp/OpenPrompt  
  ğŸ”— https://arxiv.org/abs/2305.11738 (Faithfulness Benchmarks)

---

## VIII. LLM Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹
**Ğ¦ĞµĞ»ÑŒ:** ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²ÑÑ‘ Ğ²Ñ‹ÑˆĞµĞ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğµ Ğ² end-to-end Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°Ñ….

- Semantic Search (DenseRetriever, reranker)  
  ğŸ”— https://www.pinecone.io/learn/semantic-search/  
  ğŸ”— https://sebastianraschka.com/blog/2023/retrieval-reranking.html

- Hybrid Search  
  ğŸ”— https://www.trychroma.com/docs/hybrid-search

- LangChain search example  
  ğŸ”— https://github.com/langchain-ai/langchain/blob/master/cookbook/search_rag.md

- MTEB leaderboard  
  ğŸ”— https://huggingface.co/spaces/mteb/leaderboard

---

## IX. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹  
  ğŸ”— https://paperswithcode.com/llm-leaderboard  
  ğŸ”— https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

- ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ  
  ğŸ”— https://arxiv.org/abs/2309.02772  
  ğŸ”— https://arxiv.org/abs/2307.03172 (Selectively forgetting with token pruning)

- Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ  
  ğŸ”— https://arxiv.org/abs/2306.15595 (PII Risk in LLMs)

- Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ  
  ğŸ”— https://huggingface.co/docs/datasets/index  
  ğŸ”— https://github.com/facebookresearch/dynabench