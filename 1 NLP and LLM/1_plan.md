# ĞŸĞ»Ğ°Ğ½ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM + NLP (Senior, Search/RecSys)

## I. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚
**Ğ¦ĞµĞ»ÑŒ:** ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ LLM, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ.

- **Transformers**  
  ğŸ”— [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/)  
  ğŸ”— [Transformers code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)  
  ğŸ”— [Attention is all you need](https://arxiv.org/pdf/1706.03762)  
  ğŸ”— [Mixture of Experts Explained](https://huggingface.co/blog/moe)  
  ğŸ”— [A Visual Guide to MoE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)  
  ğŸ”— [Understanding Mixture of Experts: Building a MoE Model with PyTorch](https://medium.com/@prateeksikdar/understanding-mixture-of-experts-building-a-moe-model-with-pytorch-dd373d9db81c)

- **Architectures**  
  ğŸ”— [HF LM](https://huggingface.co/course/chapter1)  
  ğŸ”— [Understanding Causal LLMâ€™s, Masked LLMâ€™s, and Seq2Seq](https://medium.com/%40tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)  
  ğŸ”— [HF Encoder-Decoder](https://huggingface.co/learn/llm-course/en/chapter1/6)
  
- **Tokenizers**  
  ğŸ”— [HF Tokenizers](https://huggingface.co/course/chapter6)  

- **Embeddings**  
  ğŸ”— [Embedding layer tutorial: A comprehensive guide to neural network representations](https://www.byteplus.com/en/topic/400368)  
  ğŸ”— [Word2Vec](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  
  ğŸ”— [GloVe](https://aclanthology.org/D14-1162/)  
  ğŸ”— [FastText](https://arxiv.org/abs/1607.04606)

- **Layers and Activations**   
  ğŸ”— [Layer Normalization](https://medium.com/@aisagescribe/ace-ai-interview-series-8-what-is-the-common-normalization-method-in-llm-training-18e559f46e08)  
  ğŸ”— [Feed-Forward](https://sebastianraschka.com/blog/2023/transformer-feedforward.html)  
  ğŸ”— [Positional Encoding](https://codelabsacademy.com/ru/news/roformer-enhanced-transformer-with-rotary-position-embedding-2024-5-30/)  
  ğŸ”— [Dropout](https://habr.com/ru/companies/wunderfund/articles/330814/)  

- **Models**  
  ğŸ”— [BERT](https://huggingface.co/blog/bert-101)  
  ğŸ”— [GPT-3](https://dugas.ch/artificial_curiosity/GPT_architecture.html)  
  ğŸ”— [T5](https://medium.com/40gagangupta_82781understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)  
  ğŸ”— [LLama3-1](https://ai.meta.com/blog/meta-llama-3-1/)  
  ğŸ”— [LLama4-1](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)  
  ğŸ”— [Mistral 7b](https://medium.com/dair-ai/papers-explained-mistral-7b-b9632dedf580)  
  ğŸ”— [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts)  
  ğŸ”— [Gemini 2.5](https://arxiv.org/pdf/2507.06261)  

---

## II. Ğ¤Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
**Ğ¦ĞµĞ»ÑŒ:** Ğ£Ğ¼ĞµÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±ÑƒÑ LLM Ğ¿Ğ¾Ğ´ ÑĞ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.

- **Classic fine-tuning (FP32, full finetune)**  
  ğŸ”— [Fine Tuning](https://huggingface.co/course/chapter3)  

- **Parameter-efficient tuning**  
  ğŸ”— [PEFT](https://habr.com/ru/articles/791966/)  
  ğŸ”— [LoRA](https://arxiv.org/abs/2106.09685)  
  ğŸ”— [QLoRA 1](https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf)  
  ğŸ”— [QLoRA 2](https://www.unite.ai/lora-qlora-and-qa-lora-efficient-adaptability-in-large-language-models-through-low-rank-matrix-factorization/)  
  ğŸ”— [QLoRA 3](https://sebastianraschka.com/blog/2023/peft-qlora.html)  
  ğŸ”— [Prefix Tuning](https://arxiv.org/abs/2101.00190)  
  ğŸ”— [Prompt Tuning](https://arxiv.org/abs/2104.08691)  
  ğŸ”— [P-tuning v2](https://arxiv.org/abs/2110.07602)  
  ğŸ”— [Adapters](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters#:~:text=The%20idea%20of%20parameter%2Defficient,the%20pretrained%20LLM%20remain%20frozen.)  
  ğŸ”— [Domain Adaptation and Comparison]()

- **Alignment**  
  ğŸ”— [InstuctGPT](https://arxiv.org/abs/2203.02155)  
  ğŸ”— [RLHF](https://huggingface.co/blog/rlhf)  
  ğŸ”— [DPO](https://huggingface.co/blog/pref-tuning)  

- **Low-bit inference: quantization**  
  ğŸ”— [A Survey on Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2003.13630)  

- **Effective Training**:  
  ğŸ”— [Gradient Accumulation and Checkpointing](https://aman.ai/primers/ai/grad-accum-checkpoint/)  
  ğŸ”— [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)  
  ğŸ”— [Pipeline Parallelism](https://docs.pytorch.org/docs/stable/distributed.pipelining.html)  
  ğŸ”— [Tensor Parallelism](https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html)  
  ğŸ”— [ZeRO stages](https://huggingface.co/docs/accelerate/v0.10.0/en/deepspeed)  

- **Instruments**  
  ğŸ”— [PEFT](https://huggingface.co/docs/peft/index)  
  ğŸ”— [Accelerate](https://github.com/huggingface/accelerate)  
  ğŸ”— [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)

- **Long context**  
  ğŸ”— [NTK Scaling](https://en.wikipedia.org/wiki/Neural_tangent_kernel)  
  ğŸ”— [Flash Attention](https://github.com/Dao-AILab/flash-attention)  

---

## III. Prompt Engineering Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
**Ğ¦ĞµĞ»ÑŒ:** ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ Ğ»ÑĞ±Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.

- **In-context learning**   
  ğŸ”— [Zero, One and Few-Shot](https://arxiv.org/abs/2301.00234)  
  ğŸ”— [Chain of Thought](https://arxiv.org/abs/2201.11903)  
  ğŸ”— [SelfAsk](https://arxiv.org/abs/2210.03350)  
  ğŸ”— [ReAct](https://arxiv.org/abs/2210.03629)  

- **Decoding**  
  ğŸ”— [Decoding Parameters](https://platform.openai.com/docs/guides/text-generation)  
  ğŸ”— [Decoding Algorithms](http://arxiv.org/html/2402.06925v3)  

- **Reasoning**  
  ğŸ”— [Tree of Thought](https://arxiv.org/abs/2305.10601)  
  ğŸ”— [Multi-hop Reasoning](https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning)

- **Prompt evaluation**  
  ğŸ”— [Promptfoo](https://github.com/promptfoo/promptfoo)  
  ğŸ”— [OpenAI Evals](https://github.com/open-eval/open-eval)

---

## IV. Embeddings Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ
**Ğ¦ĞµĞ»ÑŒ:** Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ dense-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.

- **Text Vector Representations**   
  ğŸ”— [Dense and Sparse Embeddings](https://mlokhandwalas.medium.com/dense-and-sparse-embeddings-a-comprehensive-overview-c5f6473ee9d0)  
  ğŸ”— [Contextual Embeddings](https://arxiv.org/abs/2003.07278)  
  ğŸ”— [Sentence Embeddings](https://cohere.com/llmu/sentence-word-embeddings)  

- **Adaptation** Triplet loss, contrastive learning  
  ğŸ”— [Contrastive Learning](https://medium.com/@sulbha.jindal/new-llm-learning-method-contrastive-learning-19425fda59a6)  
  ğŸ”— [Triplet Loss](https://www.v7labs.com/blog/triplet-loss)  
  ğŸ”— [Info-NCE](https://arxiv.org/pdf/2402.05369)  
  ğŸ”— [Supervised Contrastive Loss](https://arxiv.org/abs/2004.11362)  
  ğŸ”— [Negatives Mining](https://arxiv.org/pdf/2407.15831)  
  ğŸ”— [Hard and Soft Negatives]()  
  ğŸ”— [ACNE: Asymmetric Contrastive Negative Example Mining]()  
  ğŸ”— [MoCo and Memory Bank](https://arxiv.org/html/2501.16360v1)  
  ğŸ”— [Inductive Bias]()  



- **Clustering**  
  ğŸ”— [Clustering](https://scikit-learn.org/stable/modules/clustering.html)  
  ğŸ”— [Text Clustering Algorithms and Metrics](https://arxiv.org/html/2403.15112v5)  
  ğŸ”— [UMAP](https://umap-learn.readthedocs.io/en/latest/)  
  ğŸ”— [DAPT and TAPT](https://ceur-ws.org/Vol-2723/short33.pdf)  
  ğŸ”— [Inductive Bias](https://arxiv.org/html/2402.18426v1)  

- **Embedding Models**  
  ğŸ”— [GTE](https://arxiv.org/abs/2308.03281)  
  ğŸ”— [BGE](https://arxiv.org/abs/2402.03216)  
  ğŸ”— [E5](https://arxiv.org/abs/2212.03533)  
  ğŸ”— [MiniLM](https://arxiv.org/abs/2002.10957)  
  ğŸ”— [Cohere Embed]()  
  ğŸ”— [Ada](https://arxiv.org/abs/2401.12421)  
  ğŸ”— [SBERT](https://arxiv.org/abs/1908.10084)  

---

## V. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
**Ğ¦ĞµĞ»ÑŒ:** ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ‚Ğ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.

- **Model Interpretation**  
  ğŸ”— [Attention tracing and BertViz](https://medium.com/@GaryFr0sty/visualize-attention-scores-of-llms-with-bertviz-3deb94b455b3)    
  ğŸ”— [Token-Level Logit Analysis](https://arxiv.org/abs/1706.04599)  
  ğŸ”— [Layer-Wise Relevance Propagation](https://arxiv.org/abs/1509.06321)  
  ğŸ”— [Integrated Gradients](https://arxiv.org/abs/1703.01365)  
  ğŸ”— [SHAP GitHub](https://github.com/shap/shap)  
  ğŸ”— [Captum (PyTorch Explainability)](https://captum.ai/)

- **Diagnosis of Errors and Hallucinations**  
  ğŸ”— [Hallucination Sources](https://medium.com/@tam.tamanna18/understanding-llm-hallucinations-causes-detection-prevention-and-ethical-concerns-914bc89128d0)  
  ğŸ”— [Faithfulness-tests](https://arxiv.org/abs/2305.18029)  
  ğŸ”— [Toxicity Bias Tests](https://medium.com/@rajneeshjha9s/tools-to-identify-and-mitigate-bias-toxicity-in-llms-b34e95732241)

- **Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ (robustness)**  
  ğŸ”— [Adversarial Prompting](https://www.promptingguide.ai/risks/adversarial)  
  ğŸ”— [Prompt Mutations](https://elsworth.phd/Formalisms/A-Survey-of-Prompt-Mutations)  
  ğŸ”— [Stress Tests Long Inputs](https://arxiv.org/abs/2307.03172)

---

## VI. LLM Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹
**Ğ¦ĞµĞ»ÑŒ:** ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ²ÑÑ‘ Ğ²Ñ‹ÑˆĞµĞ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğµ Ğ² end-to-end Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°Ñ….

- **Semantic Search**   
  ğŸ”— [Semantic Search](https://www.pinecone.io/learn/semantic-search/)  
  ğŸ”— [Retrieval Reranking](https://sebastianraschka.com/blog/2023/retrieval-reranking.html)  
  ğŸ”— [Hybrid Search](https://www.trychroma.com/docs/hybrid-search)

- **Answer generation**  
  ğŸ”— [LangChain Generation](https://github.com/langchain-ai/langchain/blob/master/cookbook/search_rag.md)