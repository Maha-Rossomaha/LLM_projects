# –ü–ª–∞–Ω –∏–∑—É—á–µ–Ω–∏—è LLM + NLP (Senior, Search/RecSys)

## I. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç
**–¶–µ–ª—å:** –ü–æ–Ω–∏–º–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ LLM, —Ä–∞–∑–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∑–∞–¥–∞—á—É.

- **Transformers**  
  üîó [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/)  
  üîó [Transformers code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)  
  üîó [Attention is all you need](https://arxiv.org/pdf/1706.03762)  
  üîó [Mixture of Experts Explained](https://huggingface.co/blog/moe)  
  üîó [A Visual Guide to MoE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)  
  üîó [Understanding Mixture of Experts: Building a MoE Model with PyTorch](https://medium.com/@prateeksikdar/understanding-mixture-of-experts-building-a-moe-model-with-pytorch-dd373d9db81c)

- **Architectures**  
  üîó [HF LM](https://huggingface.co/course/chapter1)  
  üîó [Understanding Causal LLM‚Äôs, Masked LLM‚Äôs, and Seq2Seq](https://medium.com/%40tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)  
  üîó [HF Encoder-Decoder](https://huggingface.co/learn/llm-course/en/chapter1/6)
  
- **Tokenizers**  
  üîó [HF Tokenizers](https://huggingface.co/course/chapter6)  

- **Embeddings**  
  üîó [Embedding layer tutorial: A comprehensive guide to neural network representations](https://www.byteplus.com/en/topic/400368)  
  üîó [Word2Vec](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  
  üîó [GloVe](https://aclanthology.org/D14-1162/)  
  üîó [FastText](https://arxiv.org/abs/1607.04606)

- **Layers and Activations**   
  üîó [Layer Normalization](https://medium.com/@aisagescribe/ace-ai-interview-series-8-what-is-the-common-normalization-method-in-llm-training-18e559f46e08)  
  üîó [Feed-Forward](https://sebastianraschka.com/blog/2023/transformer-feedforward.html)  
  üîó [Positional Encoding](https://codelabsacademy.com/ru/news/roformer-enhanced-transformer-with-rotary-position-embedding-2024-5-30/)  
  üîó [Dropout](https://habr.com/ru/companies/wunderfund/articles/330814/)  

- **Models**  
  üîó [BERT](https://huggingface.co/blog/bert-101)  
  üîó [GPT-3](https://dugas.ch/artificial_curiosity/GPT_architecture.html)  
  üîó [T5](https://medium.com/40gagangupta_82781understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)  
  üîó [LLama3-1](https://ai.meta.com/blog/meta-llama-3-1/)  
  üîó [LLama4-1](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)  
  üîó [Mistral 7b](https://medium.com/dair-ai/papers-explained-mistral-7b-b9632dedf580)  
  üîó [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts)  
  üîó [Gemini 2.5](https://arxiv.org/pdf/2507.06261)  

---

## II. –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
**–¶–µ–ª—å:** –£–º–µ—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é LLM –ø–æ–¥ —Å–≤–æ—é –∑–∞–¥–∞—á—É —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.

- **Classic fine-tuning (FP32, full finetune)**  
  üîó [Fine Tuning](https://huggingface.co/course/chapter3)  

- **Parameter-efficient tuning**  
  üîó [PEFT](https://habr.com/ru/articles/791966/)  
  üîó [LoRA](https://arxiv.org/abs/2106.09685)  
  üîó [QLoRA 1](https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf)  
  üîó [QLoRA 2](https://www.unite.ai/lora-qlora-and-qa-lora-efficient-adaptability-in-large-language-models-through-low-rank-matrix-factorization/)  
  üîó [QLoRA 3](https://sebastianraschka.com/blog/2023/peft-qlora.html)  
  üîó [Prefix Tuning](https://arxiv.org/abs/2101.00190)  
  üîó [Prompt Tuning](https://arxiv.org/abs/2104.08691)  
  üîó [P-tuning v2](https://arxiv.org/abs/2110.07602)  
  üîó [Adapters](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters#:~:text=The%20idea%20of%20parameter%2Defficient,the%20pretrained%20LLM%20remain%20frozen.)  
  üîó [Domain Adaptation and Comparison]()

- **Alignment**  
  üîó [InstuctGPT](https://arxiv.org/abs/2203.02155)  
  üîó [RLHF](https://huggingface.co/blog/rlhf)  
  üîó [DPO](https://huggingface.co/blog/pref-tuning)  

- **Low-bit inference: quantization**  
  üîó [A Survey on Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2003.13630)  

- **Effective Training**:  
  üîó [Gradient Accumulation and Checkpointing](https://aman.ai/primers/ai/grad-accum-checkpoint/)  
  üîó [Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)  
  üîó [Pipeline Parallelism](https://docs.pytorch.org/docs/stable/distributed.pipelining.html)  
  üîó [Tensor Parallelism](https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html)  
  üîó [ZeRO stages](https://huggingface.co/docs/accelerate/v0.10.0/en/deepspeed)  

- **Instruments**  
  üîó [PEFT](https://huggingface.co/docs/peft/index)  
  üîó [Accelerate](https://github.com/huggingface/accelerate)  
  üîó [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)

- **Long context**  
  üîó [NTK Scaling](https://en.wikipedia.org/wiki/Neural_tangent_kernel)  
  üîó [Flash Attention](https://github.com/Dao-AILab/flash-attention)  

---

## III. Prompt Engineering –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
**–¶–µ–ª—å:** –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã –ø–æ–¥ –ª—é–±—ã–µ –∑–∞–¥–∞—á–∏, —Å–Ω–∏–∂–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

- **In-context learning**   
  üîó [Zero, One and Few-Shot](https://arxiv.org/abs/2301.00234)  
  üîó [Chain of Thought](https://arxiv.org/abs/2201.11903)  
  üîó [SelfAsk](https://arxiv.org/abs/2210.03350)  
  üîó [ReAct](https://arxiv.org/abs/2210.03629)  

- **Decoding**  
  üîó [Decoding Parameters](https://platform.openai.com/docs/guides/text-generation)  
  üîó [Decoding Algorithms](http://arxiv.org/html/2402.06925v3)  

- **Reasoning**  
  üîó [Tree of Thought](https://arxiv.org/abs/2305.10601)  
  üîó [Multi-hop Reasoning](https://www.moveworks.com/us/en/resources/ai-terms-glossary/multi-hop-reasoning)

- **Prompt evaluation**  
  üîó [Promptfoo](https://github.com/promptfoo/promptfoo)  
  üîó [OpenAI Evals](https://github.com/open-eval/open-eval)

---

## IV. Embeddings –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
**–¶–µ–ª—å:** –°—Ç—Ä–æ–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dense-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏.

- **Text Vector Representations**   
  üîó [Dense and Sparse Embeddings](https://mlokhandwalas.medium.com/dense-and-sparse-embeddings-a-comprehensive-overview-c5f6473ee9d0)  
  üîó [Contextual Embeddings](https://arxiv.org/abs/2003.07278)  
  üîó [Sentence Embeddings](https://cohere.com/llmu/sentence-word-embeddings)  

- **Adaptation** Triplet loss, contrastive learning  
  üîó [Contrastive Learning](https://medium.com/@sulbha.jindal/new-llm-learning-method-contrastive-learning-19425fda59a6)  
  üîó [Triplet Loss](https://www.v7labs.com/blog/triplet-loss)  
  üîó [Info-NCE](https://arxiv.org/pdf/2402.05369)  
  üîó [Supervised Contrastive Loss](https://arxiv.org/abs/2004.11362)  
  üîó [Negatives Mining](https://arxiv.org/pdf/2407.15831)  
  üîó [Hard and Soft Negatives]()  
  üîó [ACNE: Asymmetric Contrastive Negative Example Mining]()  
  üîó [MoCo and Memory Bank](https://arxiv.org/html/2501.16360v1)  
  üîó [Inductive Bias]()  



- **Clustering**  
  üîó [Clustering](https://scikit-learn.org/stable/modules/clustering.html)  
  üîó [Text Clustering Algorithms and Metrics](https://arxiv.org/html/2403.15112v5)  
  üîó [UMAP](https://umap-learn.readthedocs.io/en/latest/)  
  üîó [DAPT and TAPT](https://ceur-ws.org/Vol-2723/short33.pdf)  
  üîó [Inductive Bias](https://arxiv.org/html/2402.18426v1)  

- **Embedding Models**  
  üîó [GTE](https://arxiv.org/abs/2308.03281)  
  üîó [BGE](https://arxiv.org/abs/2402.03216)  
  üîó [E5](https://arxiv.org/abs/2212.03533)  
  üîó [MiniLM](https://arxiv.org/abs/2002.10957)  
  üîó [Cohere Embed]()  
  üîó [Ada](https://arxiv.org/abs/2401.12421)  
  üîó [SBERT](https://arxiv.org/abs/1908.10084)  

---

## V. –ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç–ª–∞–¥–∫–∞ –º–æ–¥–µ–ª–µ–π
**–¶–µ–ª—å:** –ü–æ–Ω–∏–º–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ—Ç–ª–∞–≤–ª–∏–≤–∞—Ç—å –æ—à–∏–±–∫–∏, —Å–Ω–∏–∂–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏.

- **Model Interpretation**  
  üîó [Attention tracing and BertViz](https://medium.com/@GaryFr0sty/visualize-attention-scores-of-llms-with-bertviz-3deb94b455b3)    
  üîó [Token-Level Logit Analysis](https://arxiv.org/abs/1706.04599)  
  üîó [Layer-Wise Relevance Propagation](https://arxiv.org/abs/1509.06321)  
  üîó [Integrated Gradients](https://arxiv.org/abs/1703.01365)  
  üîó [SHAP GitHub](https://github.com/shap/shap)  
  üîó [Captum (PyTorch Explainability)](https://captum.ai/)

- **Diagnosis of Errors and Hallucinations**  
  üîó [Hallucination Sources](https://medium.com/@tam.tamanna18/understanding-llm-hallucinations-causes-detection-prevention-and-ethical-concerns-914bc89128d0)  
  üîó [Faithfulness-tests](https://arxiv.org/abs/2305.18029)  
  üîó [Toxicity Bias Tests](https://medium.com/@rajneeshjha9s/tools-to-identify-and-mitigate-bias-toxicity-in-llms-b34e95732241)

- **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ (robustness)**  
  üîó [Adversarial Prompting](https://www.promptingguide.ai/risks/adversarial)  
  üîó [Prompt Mutations](https://elsworth.phd/Formalisms/A-Survey-of-Prompt-Mutations)  
  üîó [Stress Tests Long Inputs](https://arxiv.org/abs/2307.03172)

---

## VI. LLM –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
**–¶–µ–ª—å:** –ü—Ä–∏–º–µ–Ω—è—Ç—å –≤—Å—ë –≤—ã—à–µ–æ–ø–∏—Å–∞–Ω–Ω–æ–µ –≤ end-to-end –ø–∞–π–ø–ª–∞–π–Ω–∞—Ö.

- **Semantic Search**   
  üîó [Semantic Search](https://www.pinecone.io/learn/semantic-search/)  
  üîó [Retrieval Reranking](https://sebastianraschka.com/blog/2023/retrieval-reranking.html)  
  üîó [Hybrid Search](https://www.trychroma.com/docs/hybrid-search)

- **Answer generation**  
  üîó [LangChain Generation](https://github.com/langchain-ai/langchain/blob/master/cookbook/search_rag.md)


---

## VII. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π  
  üîó https://paperswithcode.com/llm-leaderboard  
  üîó https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è  
  üîó https://arxiv.org/abs/2309.02772  
  üîó https://arxiv.org/abs/2307.03172 (Selectively forgetting with token pruning)

- –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å  
  üîó https://arxiv.org/abs/2306.15595 (PII Risk in LLMs)

- –î–∞—Ç–∞—Å–µ—Ç—ã –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è  
  üîó https://huggingface.co/docs/datasets/index  
  üîó https://github.com/facebookresearch/dynabench

---

  ## VIII. –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π (12 –Ω–µ–¥–µ–ª—å)

| –ù–µ–¥–µ–ª–∏ | –ö–ª—é—á–µ–≤–∞—è —Ü–µ–ª—å | –†–µ–∑—É–ª—å—Ç–∞—Ç / –º–µ—Ç—Ä–∏–∫–∞ |
|--------|---------------|---------------------|
| **1 ‚Äì 2** | –†–∞–∑–æ–±—Ä–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏–∫—É self-attention –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å mini-Transformer ¬´—Å –Ω—É–ª—è¬ª (PyTorch) | –ù–æ—É—Ç–±—É–∫ —Å –∫–æ–¥–æ–º + –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É —Å–ª–æ–∂–Ω–æ—Å—Ç—å $O(n^2)$ –∏ –∫–∞–∫ –µ—ë —Å–Ω–∏–∂–∞–µ—Ç FlashAttention |
| **3 ‚Äì 4** | Fine-tune *BERT-base* –Ω–∞ SQuAD v1 (sub-set) | F1 ‚â• 80 % –∏ –æ—Ç—á—ë—Ç –æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö |
| **5 ‚Äì 6** | LoRA / QLoRA-–∞–¥–∞–ø—Ç–∞—Ü–∏—è *Llama-3-8B* –ø–æ–¥ —Å–≤–æ–π –¥–æ–º–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å | Œî perplexity ‚â• -15 % –ø—Ä–∏ GPU memory ‚â§ 12 GB |
| **7 ‚Äì 8** | –û—Ç–ª–∞–¥–∏—Ç—å prompt-–ø–∞—Ç—Ç–µ—Ä–Ω—ã (CoT, ReAct) –∏ —Å–Ω–∏–∑–∏—Ç—å hallucination-rate | ‚âà 50 —Ä—É—á–Ω—ã—Ö –∫–µ–π—Å–æ–≤ ‚Üí –¥–æ–ª—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π ‚â§ 10 % |
| **9 ‚Äì 10** | –°–æ–±—Ä–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (*BM25 + E5-small*) –∏ –∏–∑–º–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ | nDCG@10 ‚â• 0.40 –Ω–∞ —Ç—Ä–µ–∫–µ MTEB-QA |
| **11 ‚Äì 12** | –ó–∞–ø—É—Å—Ç–∏—Ç—å RAG-–ø—Ä–æ—Ç–æ—Ç–∏–ø (retriever ‚Üí reranker ‚Üí generator) + –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π evaluation | groundedness ‚â• 0.60, regression guardrails –≤ CI |

> **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ:** –≤–µ—Å—Ç–∏ –∂—É—Ä–Ω–∞–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (lm-eval-harness, ragas) –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–ø–æ —Å —á–∏—Å—Ç—ã–º–∏ README.
