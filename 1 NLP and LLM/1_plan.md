# –ü–ª–∞–Ω –∏–∑—É—á–µ–Ω–∏—è LLM + NLP (Senior, Search/RecSys)

## I. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç
**–¶–µ–ª—å:** –ü–æ–Ω–∏–º–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ LLM, —Ä–∞–∑–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∑–∞–¥–∞—á—É.

- **Transformers**:  
  üîó [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/)  
  üîó [Transformers code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)  
  üîó [Attention is all you need](https://arxiv.org/pdf/1706.03762)  
  üîó [Mixture of Experts Explained](https://huggingface.co/blog/moe)  
  üîó [A Visual Guide to MoE](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)  
  üîó [Understanding Mixture of Experts: Building a MoE Model with PyTorch](https://medium.com/@prateeksikdar/understanding-mixture-of-experts-building-a-moe-model-with-pytorch-dd373d9db81c)

- **Architectures**:  
  üîó [HF LM](https://huggingface.co/course/chapter1)  
  üîó [Understanding Causal LLM‚Äôs, Masked LLM‚Äôs, and Seq2Seq](https://medium.com/%40tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)  
  üîó [HF Encoder-Decoder](https://huggingface.co/learn/llm-course/en/chapter1/6)
  
- **Tokenizers**  
  üîó [HF Tokenizers](https://huggingface.co/course/chapter6)  

- **Embeddings**  
  üîó [Embedding layer tutorial: A comprehensive guide to neural network representations](https://www.byteplus.com/en/topic/400368)  
  üîó [Word2Vec](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  
  üîó [GloVe](https://aclanthology.org/D14-1162/)

- **Layers and Activations**:   
  üîó Layer Normalization  
  üîó [Feed-Forward](https://sebastianraschka.com/blog/2023/transformer-feedforward.html)  
  üîó [Positional Encoding](https://codelabsacademy.com/ru/news/roformer-enhanced-transformer-with-rotary-position-embedding-2024-5-30/)  
  üîó [Dropout](https://habr.com/ru/companies/wunderfund/articles/330814/)  

- **Models**:  
  üîó [BERT](https://huggingface.co/blog/bert-101)  
  üîó [GPT-3](https://dugas.ch/artificial_curiosity/GPT_architecture.html)  
  üîó [T5](https://medium.com/40gagangupta_82781understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)  
  üîó [LLama3-1](https://ai.meta.com/blog/meta-llama-3-1/)  
  üîó [LLama4-1](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)  
  üîó [Mistral 7b](https://medium.com/dair-ai/papers-explained-mistral-7b-b9632dedf580)  
  üîó [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts)  
  üîó [Gemini 2.5](https://arxiv.org/pdf/2507.06261)  

---

## II. –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
**–¶–µ–ª—å:** –£–º–µ—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é LLM –ø–æ–¥ —Å–≤–æ—é –∑–∞–¥–∞—á—É —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.

- **–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π fine-tuning (FP32, full finetune)**  
  üîó https://huggingface.co/course/chapter3

- **Parameter-efficient tuning**  
  üîó [PEFT](https://habr.com/ru/articles/791966/)  
  üîó [LoRA](https://arxiv.org/abs/2106.09685)  
  üîó [QLoRA 1](https://medium.com/@gitlostmurali/understanding-lora-and-qlora-the-powerhouses-of-efficient-finetuning-in-large-language-models-7ac1adf6c0cf)  
  üîó [QLoRA 2](https://www.unite.ai/lora-qlora-and-qa-lora-efficient-adaptability-in-large-language-models-through-low-rank-matrix-factorization/)  
  üîó [QLoRA 3](https://sebastianraschka.com/blog/2023/peft-qlora.html)  
  üîó [Prefix Tuning](https://arxiv.org/abs/2101.00190)  
  üîó [Prompt Tuning](https://arxiv.org/abs/2104.08691)  
  üîó [Adapters](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters#:~:text=The%20idea%20of%20parameter%2Defficient,the%20pretrained%20LLM%20remain%20frozen.)  

- Low-bit inference: quantization (int8, int4), `bitsandbytes`, `AutoGPTQ`  
  üîó https://github.com/TimDettmers/bitsandbytes  
  üîó https://github.com/PanQiWei/AutoGPTQ

- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã `PEFT`, `Trainer`, `accelerate`, `deepspeed`  
  üîó https://github.com/huggingface/accelerate  
  üîó https://huggingface.co/docs/transformers/perf_train_gpu_one

- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (NTK scaling, FlashAttention, LongContext)  
  üîó https://github.com/Dao-AILab/flash-attention  
  üîó https://huggingface.co/LongChat  
  üîó https://blog.llamaindex.ai/long-context-llms/

---

## III. Prompt Engineering –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
**–¶–µ–ª—å:** –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã –ø–æ–¥ –ª—é–±—ã–µ –∑–∞–¥–∞—á–∏, —Å–Ω–∏–∂–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

- Few-shot, zero-shot, CoT, ReAct, Self-Ask  
  üîó https://github.com/dair-ai/Prompt-Engineering-Guide  
  üîó https://github.com/openai/openai-cookbook

- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã decoding: temperature, top_p, repetition_penalty  
  üîó https://platform.openai.com/docs/guides/text-generation

- Prompt compression, reranking, robustness  
  üîó https://arxiv.org/abs/2309.02772 (Prompt Compression for LLMs)

- Reasoning —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: Chain-of-prompt, tree-of-thought, multi-hop reasoning  
  üîó https://arxiv.org/abs/2305.10601 (Tree of Thought)  
  üîó https://github.com/kyegomez/tree-of-thoughts

- Prompt evaluation  
  üîó https://github.com/promptfoo/promptfoo  
  üîó https://github.com/open-eval/open-eval

---

## IV. Embeddings –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
**–¶–µ–ª—å:** –°—Ç—Ä–æ–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å dense-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏.

- Sentence embeddings, contextual embeddings, dense vs sparse  
  üîó https://www.sbert.net/

- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: GTE, BGE, E5, MiniLM, Cohere Embed, Ada  
  üîó https://huggingface.co/intfloat/e5-large-v2  
  üîó https://huggingface.co/BAAI/bge-base-en  
  üîó https://cohere.com/docs/embed

- Triplet loss, contrastive learning  
  üîó https://www.pinecone.io/learn/series/fine-tune-llm/contrastive-learning/

- –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è  
  üîó https://scikit-learn.org/stable/modules/clustering.html  
  üîó https://umap-learn.readthedocs.io/en/latest/

---

## V. –ê–Ω–∞–ª–∏–∑ –∏ –æ—Ç–ª–∞–¥–∫–∞ –º–æ–¥–µ–ª–µ–π
**–¶–µ–ª—å:** –ü–æ–Ω–∏–º–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ—Ç–ª–∞–≤–ª–∏–≤–∞—Ç—å –æ—à–∏–±–∫–∏, —Å–Ω–∏–∂–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏.

- LM Evaluation  
  üîó https://github.com/EleutherAI/lm-evaluation-harness  
  üîó https://github.com/open-eval/open-eval

- RAG evaluation  
  üîó https://github.com/explodinggradients/ragas  
  üîó https://github.com/facebookresearch/RA-Eval

- Attention tracing, token-level logit analysis  
  üîó https://github.com/cdpierse/transformers-interpret  
  üîó https://github.com/jessevig/bertviz

- Adversarial prompting, hallucination reduction  
  üîó https://github.com/thunlp/OpenPrompt  
  üîó https://arxiv.org/abs/2305.11738 (Faithfulness Benchmarks)

---

## VI. LLM –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –ø–æ–∏—Å–∫–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
**–¶–µ–ª—å:** –ü—Ä–∏–º–µ–Ω—è—Ç—å –≤—Å—ë –≤—ã—à–µ–æ–ø–∏—Å–∞–Ω–Ω–æ–µ –≤ end-to-end –ø–∞–π–ø–ª–∞–π–Ω–∞—Ö.

- Semantic Search (DenseRetriever, reranker)  
  üîó https://www.pinecone.io/learn/semantic-search/  
  üîó https://sebastianraschka.com/blog/2023/retrieval-reranking.html

- Hybrid Search  
  üîó https://www.trychroma.com/docs/hybrid-search

- LangChain search example  
  üîó https://github.com/langchain-ai/langchain/blob/master/cookbook/search_rag.md

- MTEB leaderboard  
  üîó https://huggingface.co/spaces/mteb/leaderboard

---

## VII. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π  
  üîó https://paperswithcode.com/llm-leaderboard  
  üîó https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è  
  üîó https://arxiv.org/abs/2309.02772  
  üîó https://arxiv.org/abs/2307.03172 (Selectively forgetting with token pruning)

- –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å  
  üîó https://arxiv.org/abs/2306.15595 (PII Risk in LLMs)

- –î–∞—Ç–∞—Å–µ—Ç—ã –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è  
  üîó https://huggingface.co/docs/datasets/index  
  üîó https://github.com/facebookresearch/dynabench

---

  ## VIII. –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π (12 –Ω–µ–¥–µ–ª—å)

| –ù–µ–¥–µ–ª–∏ | –ö–ª—é—á–µ–≤–∞—è —Ü–µ–ª—å | –†–µ–∑—É–ª—å—Ç–∞—Ç / –º–µ—Ç—Ä–∏–∫–∞ |
|--------|---------------|---------------------|
| **1 ‚Äì 2** | –†–∞–∑–æ–±—Ä–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏–∫—É self-attention –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å mini-Transformer ¬´—Å –Ω—É–ª—è¬ª (PyTorch) | –ù–æ—É—Ç–±—É–∫ —Å –∫–æ–¥–æ–º + –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É —Å–ª–æ–∂–Ω–æ—Å—Ç—å $O(n^2)$ –∏ –∫–∞–∫ –µ—ë —Å–Ω–∏–∂–∞–µ—Ç FlashAttention |
| **3 ‚Äì 4** | Fine-tune *BERT-base* –Ω–∞ SQuAD v1 (sub-set) | F1 ‚â• 80 % –∏ –æ—Ç—á—ë—Ç –æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö |
| **5 ‚Äì 6** | LoRA / QLoRA-–∞–¥–∞–ø—Ç–∞—Ü–∏—è *Llama-3-8B* –ø–æ–¥ —Å–≤–æ–π –¥–æ–º–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å | Œî perplexity ‚â• -15 % –ø—Ä–∏ GPU memory ‚â§ 12 GB |
| **7 ‚Äì 8** | –û—Ç–ª–∞–¥–∏—Ç—å prompt-–ø–∞—Ç—Ç–µ—Ä–Ω—ã (CoT, ReAct) –∏ —Å–Ω–∏–∑–∏—Ç—å hallucination-rate | ‚âà 50 —Ä—É—á–Ω—ã—Ö –∫–µ–π—Å–æ–≤ ‚Üí –¥–æ–ª—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π ‚â§ 10 % |
| **9 ‚Äì 10** | –°–æ–±—Ä–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (*BM25 + E5-small*) –∏ –∏–∑–º–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ | nDCG@10 ‚â• 0.40 –Ω–∞ —Ç—Ä–µ–∫–µ MTEB-QA |
| **11 ‚Äì 12** | –ó–∞–ø—É—Å—Ç–∏—Ç—å RAG-–ø—Ä–æ—Ç–æ—Ç–∏–ø (retriever ‚Üí reranker ‚Üí generator) + –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π evaluation | groundedness ‚â• 0.60, regression guardrails –≤ CI |

> **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ:** –≤–µ—Å—Ç–∏ –∂—É—Ä–Ω–∞–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (lm-eval-harness, ragas) –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–ø–æ —Å —á–∏—Å—Ç—ã–º–∏ README.
