# Эволюция моделей Claude (Anthropic) — 2022‑2025

> Этот конспект последовательно прослеживает развитие линеек **Claude Instant → Claude 2/2.1 → Claude 3 (Haiku / Sonnet / Opus) → Claude 3.5**, выделяя архитектурные новации, рост контекстного окна и методы обучения (*Constitutional AI + RLHF*). Формулы приведены в упрощённом виде для иллюстрации вычислительных тенденций.

---

## 0. Хронология на одном взгляде

| Год     | Поколение               | Активные параметры\* | Контекст, токены         | Ключевые приёмы                                  |
| ------- | ----------------------- | -------------------- | ------------------------ | ------------------------------------------------ |
| 2022 Q4 | **Claude Instant 1.0**  | \~6 B                | 9 k → 100 k              | RLHF (челов.+AI)                                 |
| 2023 Q3 | **Claude Instant 1.2**  | \~8 B                | 100 k                    | Long RoPE, KV‑кэш                                |
| 2023 Q3 | **Claude 2.0**          | \~12 B               | 100 k                    | Improved RoPE, CAI v1                            |
| 2023 Q4 | **Claude 2.1**          | \~12 B               | 200 k                    | RoPE‑Rescale, Gradient Checkpointing             |
| 2024 Q1 | **Claude 3 (H/S/O)**    | 7 → 70 B†            | 200 k (до 1 M)           | Sparse Attn, Multimodal Vision, CAI v2           |
| 2024 Q4 | **Claude 3.5 (H/S)**    | 7 → 70 B             | 200 k (с двойным output) | Weight Sharing, Vision v2, «Computer use» API    |
| 2025 Q2 | **Claude 3.7 (Sonnet)** | 7 → 70 B             | 200 k+                   | Incremental Retrieval, Multi‑Agent Orchestration |

\* *«Активные параметры» — количество весов, действительно участвующих в вычислении одного шага декодирования.*
† *Anthropic не публикует точных размеров; оценки основаны на утечках и анализе latency/VRAM.*

---

## 1. Claude Instant 1.x (2022–2023)

### 1.1 Детализация архитектуры Claude Instant 1.x
| Компонент                  | Детали реализации                                                       |
| -------------------------- | ----------------------------------------------------------------------- |
| **Тип модели**             | Decoder-only Transformer                                                |
| **Глубина**                | $N=48$ блоков                                                         |
| **Скрытое состояние**      | $d_{model}=4096$                                                     |
| **Attention-головы**       | $H=32;\Rightarrow;d_{head}=d_{model}/H=128$                         |
| **MLP**                    | SwiGLU-блок c расширением $\approx4\times$ до $d_{ff}\approx16384$ |
| **Нормализация**           | RMSNorm перед каждым под-блоком                                         |
| **Positional Encoding**    | Long RoPE («θ-scaling» Rotary)                                          |
| **Общее # параметров** | ≈ 25–35 B (оценка — точное значение не раскрыто)                        |

**Базовый блок (Pre-Norm)**  
$$
\~x=x+\text{Attention}(\text{RMSNorm}(x))\\  
y=\~x+\text{SwiGLU}(\text{RMSNorm}(\~x))​


$$

* **Self-Attention**.
 $Q,K,V\in\mathbb R^{n\times d_{\text{model}}}$;
$$
\text{Attn(Q,K,V)}=\text{softmax}(\frac{QK^\top}{\sqrt{d_{head}}})V
$$
  Реализована через **Flash Attention v1**, уменьшающую как время, так и пик памяти с $O(n^2)$ до $O(n)$ путём блочного вычисления и фьюза операций.  
* **Long RoPE**. Rotary-фазу $\theta_i$ масштабируют по формуле
$$
\theta_i=\theta_0\alpha^{\frac{2i}{d_{model}}},\quad\alpha<1
$$
что «растягивает» спираль Фурье-частот до $L=100.000$ токенов без ввода новых весов.
* **MLP**. $\mathrm{SwiGLU}(a,b)=\mathrm{SiLU}(a)\odot b$ обеспечивает лучшее соотношение «качество/параметры» по сравнению с ReLU/GELU.

### 1.2 Оптимизации памяти/скорости
| Приём                      | Что даёт                                                                                                           |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **Flash Attention v1**     | $-65%$ VRAM и $\sim2\times$ ускорение матрицы внимания                                    |
| **Gradient Checkpointing** | Хранятся не все активации ⇒ экономия до $40–50%$ памяти при $\sim1.2–1.3\times$ доп. FLOPs |
| **Chunk Prefill**          | Длинный промпт разбивается на чанки ($\approx2$-3 k ток.) и заливается «плитками», сокращая p95-latency          |
| **Host-KV Cache**          | Сдвигает $K,V$ для ранних токенов в системную RAM; GPU держит лишь «хвост» контекста                             |


### 1.3 Контекст 9 k → 100 k

* **Теоретически**: $n!\uparrow!11\times\Rightarrow$ $O(n^2)!=!121\times$ больше FLOPs.
* **Практически**: за счёт Flash Attention, host-KV и Grouped Query / Multi-Query схем (одни $K,V$ на несколько $Q$) — ~ 4× рост задержки вместо 121×.
* **Внутренние тесты Anthropic**: 72 k токенов «The Great Gatsby» дифф-поиск за 22 с на A100 80 GB

### 1.4 Подход к обучению

* **Предобучение**. Стан­дартное предсказание следующего токена на корпусе >1 трлн токенов (включая код, веб, книги).
* **RLHF** с человеческими сравнениями + AI-критиком.
* **Constitutional AI v0.9**. 9 принципов формируют дополнительный штраф $R_{\text{const}}(a)$, добавляемый к наградe RLHF:  
$R=R_{human}-\lambda R_{const},\quad \lambda \approx 0.2$

### 1.5Коротко о «фишках» Claude Instant
* **KV-Sharing между потоками**. При мульти-запросах дубли $K,V$ для одинаковых префиксов кешируются.
* **Weight Tying**. Матрицы токен-эмбеддинга и выходного линейного слоя разделены, экономя ≈ 200 M параметров.
* **Mixed-precision (bfloat16)**. Баланс памяти и точности; весовые квантизированы до int8 при inference для быстрых режимов.

---

## 2. Claude 2 / 2.1 (2023–2024)

### 2.1 Новые возможности

1. **Контекст 200 k** — rescale‑RoPE (частоты $\omega_i\gets\omega_i/\beta$) + динамический KV‑кэш.
2. **Токенная память**: во время генерации префикс «сворачивается» в компресс‑резюме длиной $\le 8,k$, экономя $\approx75%$ RAM.
3. **Снижение галлюцинаций** — доработанный Constitutional AI v1 с $m=12$ статей.

### 2.2 Формула пропускной способности

Пусть $W=8192$ — окно активного KV. Тогда асимптотика decode‑шага
$M_{KV}=O(WdH)+O(KdH),$
где $K\ll n$ — количество новых токенов (обычно 1). При $n\gg W$ память остаётся константой.

---

## 3. Claude 3 семейство (Haiku / Sonnet / Opus), 2024 Q1

### 3.1 Общая матрица размеров

| Модель                                      | Параметры (оценка) | d_model | Слои $N$ | Heads $H$ | Время ответа\*     |
| ------------------------------------------- | ------------------ | -------- | ---------- | ----------- | ------------------ |
| **Haiku**                                   | 7 B                | 4096     | 48         | 32          | \~0.8× GPT‑3.5     |
| **Sonnet**                                  | 25 B               | 6144     | 60         | 48          | \~1.2× GPT‑4‑turbo |
| **Opus**                                    | 70 B               | 8192     | 80         | 64          | \~1.4× GPT‑4o      |
| \* *Нормировано на A100 80 GB (batch = 1).* |                    |          |            |             |                    |

### 3.2 Архитектурные акценты

* **Селективная разреженность**: в средних слоях применяется Sparse Attention с маской N²/4, давая 2× экономию FLOPs.
* **Мультимодальная проекция**: совместный линейный слой $W_{vis}\in\mathbb{R}^{d\times d_{vis}}$ для слияния patch‑embedding'ов.
* **Vision‑Adapter** — две резид. «колонки» с kernel size 3 для локальных фичей.

### 3.3 Потенциал >1 M токенов

Вычислительная сложность одной головы со скользящим окном $W=4096$:
$C=O(nWd),\quad C\ll O(n^2d) \text{ при } n\gg W.$
Теоретически $n$ ограничено лишь VRAM → Anthropic продемонстрировал recall при $n=1,024,000$.

---

## 4. Claude 3.5 (2024 Q4 – 2025 Q2)

### 4.1 Улучшения

1. **Weight Sharing 2×** — пары слоёв делят параметры FFN, снижая активные FLOPs без качевого регресса.
2. **Vision v2** — кросс‑модальная attention‑решётка «block‑fusion» повышает точность при $\mathcal{O}(1)$ росте FLOPs.
3. **Computer‑Use** API: встроенный агент запуска desktop‑действия; реализовано через latent‑tool use супервизию.
4. **Режим Research** — распределённая оркестрация multi‑agent (up to 16 под‑моделей) с дифференцируемым планировщиком.

### 4.2 Формула экономии веса

Если доля весов, подлежащих шарингу, $p=0.5$, то
$P_{eff}=P\,(1-p)+P\,p/2=P\,\bigl(1-p/2\bigr)=0.75P.$
Практически это даёт \~1.33× рост «параметров на доллар».

---

## 5. Constitutional AI: сквозной каркас безопасности

1. **Stage I (Supervised)** — модель следует цепочке правил ${\pi_j}_{j=1}^m$; градиент добавляет штраф за отклонение.
2. **Stage II (RLHF)** — AI‑критик оценивает кандидаты, минимизируя комбинированную функцию
   $L=\lambda_1 L_{SL}+\lambda_2 L_{HF}+\lambda_3 L_{viol},$
   где $L_{viol}=\sum_{j} \mathbf{1}[r_j>0] r_j^2$.
3. **Stage III (Self‑Critique)** — модель делает «chain‑of‑thought» и корректирует ответ, если нарушены правила.

---

## 6. Динамика контекстного окна

$$
\text{Context}(t)=
\begin{cases}
9\,000,& t \le \text{May 2023};\\
100\,000,& t \in (\text{May 2023},\ \text{Nov 2023}];\\
200\,000,& t \in (\text{Nov 2023},\ 2025);\\
>1\,000\,000,& t = \text{pilot customers}.\\
\end{cases}
$$

Рост контекстного окна достигнут без квадратичного роста VRAM благодаря **оконному KV‑кэшу** и **RoPE‑rescale**, что сохраняет асимптотику $O(nWd)$ при $n \gg W$.

## 7. Итоги

* Фокус Anthropic — *безопасное масштабирование*: каждое поколение приносит либо √n‑ускорение, либо 2× увеличение контекста при сохранении памяти.
* **Claude 3.5** демонстрирует, что *выборочный шаринг* и *агентная оркестрация* позволяют одновременно улучшать IQ‑score и latency.
* Дальнейшие тренды: гибридные Mixture‑of‑Experts для активных $\le 12,B$ параметров и динамическая выборка long‑range KV.

$$
$$
