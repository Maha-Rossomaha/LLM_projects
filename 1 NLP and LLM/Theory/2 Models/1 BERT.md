# BERT Architecture 
URL: https://huggingface.co/blog/bert-101

---

BERT — двунаправленная энкодер‑модель Transformer, пред‑обученная на 3,3 млрд токенов Wikipedia + BooksCorpus и двух задачах **Masked Language Modeling** и **Next Sentence Prediction**; базовая конфигурация (12 слоёв, 110 M параметров) показала SOTA‑результаты на GLUE, SQuAD и других бенчмарках, а Google интегрировал её в поиск (10 % запросов) ещё в 2019–20 гг. 

---

## 1  Что такое BERT и зачем он нужен
BERT ввёл парадигму «пред‑обучить → дообучить» и заменил ранее задачи‑специфичные модели универсальными контекстными эмбеддингами, что упростило пайплайны NLU‑систем и резко подняло метрики на 11 публичных наборах данных. 

---

## 2  Архитектура и входная последовательность

### 2.1 Tokenization (WordPiece)
BERT пользуется **WordPiece**: итеративно строит словарь под‑слов для уменьшения OOV‑проблем и компактности словаря = 30 522 entry. 

### 2.2 Три компонента эмбеддинга
Для каждого токена $i$ формируется  
$$
E_i = T_i + S_i + P_i,
$$  
где $T_i$ — токен‑эмбеддинг, $S_i$ — эмбеддинг сегмента (A/B), $P_i$ — синусоидальный positional encoding, заимствованный из Transformer. 

### 2.3 Слои Transformer
BERT — **encoder‑only**: Base = 12 слоёв, Hidden = 768, Heads = 12 (110 M параметров); Large = 24 слоя, Hidden = 1024, Heads = 16 (340 M параметров). 

---

## 3  Предобучающие задачи

### 3.1 Masked Language Modeling (MLM)
15 % токенов маскируются, а модель восстанавливает их по контексту:  
$$
\mathcal{L}_{\text{MLM}}
= -\sum_{i\in\mathcal M}\log P_\theta\!\bigl(x_i\mid x_{\setminus\mathcal M}\bigr).
$$  
MLM обеспечивает двунаправленный контекст, в отличие от автогрессивных LM. 

### 3.2 Next Sentence Prediction (NSP)
Модель классифицирует, является ли вторая строка продолжением первой:  
$$
P(\text{IsNext}\mid h_{[CLS]}) = \sigma\!\bigl(h_{[CLS]}^\top W + b\bigr).
$$  
Пары 50 % настоящих / 50 % случайных; цель — научить моделировать меж‑фразовые отношения. 

---

## 4  Данные и процесс обучения
Обучение на 2,5 B слов English Wikipedia + 800 M BooksCorpus (итого 3,3 B).   
Базовая модель тренировалась 4 дня на 4 TPU v3, Large — 4 дня на 16 TPU v3.   

---

## 5  Метрики и бенчмарки

| Benchmark | Метрика | BERT‑base | BERT‑large |
|-----------|---------|-----------|------------|
| GLUE avg | Score | 80.5 | 82.1  |
| SQuAD v1 | F1 | 93.2 | 94.9  |
| SQuAD v2 | F1 | 83.1 | 86.9  |

---

## 6  Дообучение (Fine‑tuning)
Часто достаточно 2–3 эпох с learning‑rate $2\!\times\!10^{-5}$ и batch 32 (AdamW + linear decay).  
На GLUE большинство задач дообучаются ≤ 25 мин на одной GPU. 

---

## 7  Малые и ускоренные версии
**DistilBERT** сокращает параметры на 40 % и ускоряет инференс на 60 %, сохраняя 97 % качества.    
Другие варианты — TinyBERT, MobileBERT, ALBERT.   

---

## 8  Пример использования в Transformers

```python
from transformers import pipeline
fill = pipeline("fill-mask", model="bert-base-uncased")
fill("Natural Language Processing will [MASK] the world.")
```

Код загружает пред‑обученный чекпойнт и возвращает топ‑5 вариантов замены маски.   

---

## 9  Практические риски
BERT обучается на огромных необработанных корпусах, поэтому он наследует статистические перекосы этих данных — в его эмбеддингах и предсказаниях обнаруживаются гендерные и расовые стереотипы (например, женские имена чаще тянутся к «няне / медсестре», а афро-американские — к негативной лексике).  
Чтобы минимизировать такие риски перед продакшен-внедрением, модель нужно аудитировать и дебиасить: исследовать скрытые представления на наличие «гендерного направления», измерить метрики справедливости и применить методы корректировки (проекционное обнуление, адаптивные штрафы к biased-примерам, дополнительное дообучение на сбалансированных выборках)    

## 10  Расширенные задачи предобучения и варианты MLM  

> **Основные проблемы базового MLM**  
> Классическое MLM BERT маскирует ≤ 15 % позиций, поэтому модель «учится» догадываться, какие токены скрыты, но одновременно **запоминает сам маркер `[MASK]`** — в продакшене этот токен никогда не встречается. Ниже приведены ключевые подходы, которые решают проблему «утечки» и/или улучшают использование контекста.

### 10.1  Dynamic Masking (RoBERTa)  
BERT фиксирует маску при первом прочтении примера; **RoBERTa** генерирует *новую* маску каждый проход эпохи, позволяя модели видеть все токены как контекстные и как целевые — это повышает стабильность и уменьшает переобучение на `[MASK]`.  

### 10.2  Whole-Word Masking  
Вместо случайных подслов WordPiece маскируются **целые слова**, что делает задачу сложнее и приближает к реальным ошибкам автокоррекции; результат — +1–2 BLEU/F1 на SQuAD и GLUE.  

### 10.3  SpanBERT: маскирование спанов  
- Маскируются не одиночные токены, а *непрерывные* фрагменты длиной 2–5 токенов.  
- Вводится **Span-Boundary Objective (SBO)**: каждый токен внутри скрытого промежутка $x_s,\dots,x_e$ предсказывается лишь по эмбеддингам рамки $x_{s-1},x_{e+1}$.  
  $$
  \mathcal{L}_{\text{SBO}}
  = -\!\!\sum_{i=s}^{e}\!\log P_\theta\!\bigl(x_i\mid x_{s-1},x_{e+1},p_{i-s+1}\bigr)
  $$  
  Такой сигнал улучшает задачи, чувствительные к границам сущностей (NER, SRL) на 1–2 F1 пункта.  

### 10.4  ELECTRA: Replaced-Token Detection  
MLM тратит вычисления на «видимые» токены. **ELECTRA** обучает мини-генератор $G$ замещать некоторые позиции «правдоподобными» словами, а дискриминатор $D$ классифицирует каждую позицию как *исходная / заменённая*:  
$$
\mathcal{L}_{\text{RTD}}
= -\sum_{i=1}^{n}\Bigl[y_i\log D(x_i)+(1-y_i)\log\bigl(1-D(x_i)\bigr)\Bigr].
$$  
RTD использует *все* токены как сигнал и достигает того же качества при 1/4 вычислений BERT.  

### 10.5  Sentence/Token Order Objectives  
- **ALBERT** заменяет NSP на **Sentence Order Prediction (SOP)**: модель различает пару *(A B)* и *(B A)*, что лучше учит межпредложную связность.  
- **XLNet** предлагает **Permutation LM**: предсказывает токен по произвольному порядку перестановок, избегая `[MASK]` и сохраняя двунаправленность контекста.  

### 10.6  Denoising & Span Corruption (T5 / BART / MASS)  
Модели Seq2Seq скрывают *несколько случайных спанов* и восстанавливают полный текст:  
$$
\mathcal{L}_{\text{denoise}}
= -\log P_\theta\!\bigl(\text{original}\mid\text{corrupted}\bigr).
$$  
- **T5** маскирует 15 % слов «сентинелами» `<extra_id_k>` и обучает декодер «заполнять» пропуски.  
- **BART** сочетает случайные удалённые, зашумленные, permuted и span-masking виды шума, давая +1 ROUGE-L в суммаризации.  
- **MASS** маскирует фрагмент только в энкодере, а декодер генерирует его целиком — эффективно для NMT и G2T задач.  

### 10.7  DeBERTa v3 & R3L  
Использует смесь **Replaced, Removed and Rotated Language Modeling (R3L)** целей, объединяя RTD с сэмплированным удалением и поворотом субслов для повышения робастности представлений.  

### 10.8  UniLM: Унификация задач LM  
**UniLM** объединяет uni-, bi- и seq2seq-направленные маски в одной модели, переключаясь между ними через attention-маски — это позволяет одним весам решать генерацию, понимание и перевод.  

