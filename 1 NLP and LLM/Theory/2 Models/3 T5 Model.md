# T5 Model 

---

URL: [https://medium.com/@gagangupta_82781/understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b](https://medium.com/@gagangupta_82781/understanding-the-t5-model-a-comprehensive-guide-b4d5c02c234b)



## 1 Введение и научный контекст

Модель **T5 (Text‑to‑Text Transfer Transformer)** стала реперной точкой в эволюции *seq‑to‑seq*‑подходов, поскольку перевела **все** задачи NLP в универсальную форму «текст → текст». Такой формализм устраняет границы между задачами и позволяет воспринимать всю предметную область как обучение условного распределения $p_\theta(y\mid x)$ на огромном корпусе.

Основные научные императивы T5:

- **Унификация формулировок** — сведение классификации, разметки, QA и MT к одному интерфейсу;
- **Трансферное мегамасштабное обучение** — следование парадигме *pre‑train → fine‑tune*;
- **Скалирование модели** — исследование кривых скалирования вплоть до 11 B параметров, что согласуется с законодательными результатами.

---

## 2 Архитектурные детали

### 2.1 Токенизация SentencePiece (Unigram LM)

* Обучающий корпус. На корпусе C4 ($\approx 750$ GB чистого англоязычного текста) оптимизируется unigram language model SentencePiece, то есть вероятностная модель словаря $\mathcal V$ и правдоподобия последовательности $p(x)=\prod_i p(v_i)$, где $v_i\in\mathcal V$ — выбранные субслова.

* Размер словаря. $|\mathcal V|=32,000$, из них 100 «служебных» токенов зарезервированы под sentinel-метки $\langle extra\_id\_k \rangle$, применяемые при span-corruption (см. § 3).

* Свойства.

  * Морфологическая выразительность — короткие и редкие слова тенденciозно разбиваются на меньшие юнитсы;

  * Стабильность N-грамм — частотные корни/аффиксы фиксируются целиком, что уменьшает энтропию модели и ускоряет сходимость;

  * Subword regularization — во время предобучения с некоторой вероятностью выбирается не оптимальная, а альтернативная сегментация, повышая устойчивость к шуму.

> Итог. SentencePiece обеспечивает баланс: меньше <UNK>, чем в word-level, и меньше усложнение позиции, чем в char-level.


### 2.2 Encoder‑Decoder Transformer
---
#### 2.2.1 Структура уровня $l$

Для $l=1,\dots,L$ энкодерный блок вычисляет
$$
\begin{aligned}
\tilde H^{(l)} &= \operatorname{MHSA}\!\bigl(\operatorname{LN}(H^{(l-1)})\bigr) + H^{(l-1)},\\[6pt]
H^{(l)} &= \operatorname{FFN}\!\bigl(\operatorname{LN}(\tilde H^{(l)})\bigr) + \tilde H^{(l)}.
\end{aligned}
$$

где

* $\operatorname{MHSA}(Q)=\textstyle\sum_{h=1}^H \operatorname{softmax}!\left(\frac{Q_hK_h^\top}{\sqrt{d_k}}\right) V_h,W_h^O$;
* $\operatorname{FFN}(x)=W_2,\;$ $\sigma(W_1x),\;$ $\sigma=\text{ReLU}$ или GELU;
* **Pre-LN** — нормализация до каждого саб-слоя стабилизирует градиенты в глубоком ($L\le 24$) стеке;
* **Residual** — суммирование входа и выхода каждого саб-слоя, обеспечивая дифференцируемый путь $O(L)$.
---
#### 2.2.2 Отличия энкодера и декодера
| Компонент                             | Энкодер                                 | Декодер                                                                  |
| ------------------------------------- | --------------------------------------- | ------------------------------------------------------------------------ |
| Self-Attention                        | Полное                                  | Causal маска $M_c$ (triangular)                                       |
| Cross-Attention \$\operatorname{CA}\$ | —                                       | $\operatorname{CA}(Q{=}\operatorname{LN}(Y^{(l-1)}),K{=}V{=}H^{(L)})$ |
| Позиционное смещение                  | *Learned relative* (T5-style) для обоих |                                                                          |

---
### 2.Prefix-LM (совмещённый режим внимания)

T5 может работать в двух режимах:

1. **Fully Masked LM (BERT-подобное denoising)** — декодер видит только сгенерированные до текущего шага токены-ответы.
2. **Prefix-LM** — позиции, помеченные как префикс, доступны через полное внимание; всё, что следует за специальным разделителем $\langle extra\_id \rangle$, отрабатывается автогрессивно.

Тем самым одна и та же архитектура охватывает задачи классификации, условного генеративного моделирования и seq-to-seq — достаточно разметить префикс и цель соответствующим образом.

---

## 3 Предобучение: Span‑Corruption Objective

Вдохновлённый BART‑образным *text‑infilling*, T5 оперирует **спанами** переменной длины. Пусть вход $x$ длины $n$. Выбирается набор непересекающихся сегментов ${s_k}_{k=1}^{K}$, где длина $|s_k|\sim\operatorname{Geom}(p)$, а суммарная маскируемая доля равна $\beta\approx0.15$.

### 3.1 Алгоритм маскирования

1. **Выбор длин спанов**. Для каждой позиции $i$ с вероятностью $p$ начало спана, длина $\ell\sim\operatorname{Geom}(p)$ (усечённая: $\ell\le 10$).
2. **Замена**. Сегмент $s_k{=}(i,\dots,i+\ell-1)$ вырезается и заменяется единственным sentinel-токеном $\langle extra\_id\_k \rangle$ ($k$ нумеруется по порядку масок в тексте).
3. **Целевая последовательность**. Декодеру подаётся конкатенация всех спанов в том же порядке, каждый предваряется своим sentinel-токеном:
$$
\begin{aligned}
\text{Input:}\;&\texttt{The quick}~\langle\!\text{extra\_id\_0}\rangle~\texttt{fox}~\langle\!\text{extra\_id\_1}\rangle~\texttt{dog.}\\[6pt]
\text{Target:}\;&\langle\!\text{extra\_id\_0}\rangle~\texttt{brown lazy}~\langle\!\text{extra\_id\_1}\rangle~\texttt{jumps over}
\end{aligned}
$$
4. **Массовая доля**. Суммарно маскируется $\beta \approx 0.15$ символов — эмпирический компромисс между задачами восстановления и предсказания длинного контекста.

### 3.2 Функция потерь
Пусть $y$ — целевая последовательность длины $m$. Потеря — автогрессивная кросс-энтропия
$$
\mathcal{L}(\theta)= - \sum_{t=1}^{|y|} \log p_\theta \bigl( y_t \mid y_{<t}, x_{\text{masked}} \bigr),
$$

где $x_{\text{masked}}$ — вход с sentinel-заглушками. Благодаря сквозной передаче $x$ через энкодер и приставке sentinel-токенов между спанами модель одновременно:
* локализует позиции пропусков (за счёт уникального индекса $k$);
* mвосстанавливает содержимое;
* mсохраняет порядковую информацию без громоздкого *per-span* copy-mechanism.

### 3.3 Корпус C4

*C4* (Colossal Clean Crawled Corpus) объёмом \~750 GB отфильтрован по правилам *bad‐words*, *boiler‑plate removal* и длина‐based heuristic, что снижает лингво‑мусор и отчасти митигает bias.

### 3.4 Интуиция и связь с другими целями
* **Text-infilling (BART)**. Маскируются целые фразы вместо единичных единиц — ближе к реальным «редакциям» текста.

* **Permuted LM (XLNet)**. Sentinel-кодирование устраняет необходимость сложных перестановок позиций: вся информация о границах спанов в их индексах.

* **Prefix-LM**. Если выбрать $\beta = 1$ и маскировать хвост $x_{t\ge t_0}$, получаем чисто автогрессивное обучение seq-to-seq, а при $\beta \ll 1$ — режим BERT-стиля.

> **Ключевая идея**: объединить в одной цели преимущества masked и causal обучения, сделав T5 «универсальным солдатом» для текстовых задач.


---

## 4 Параметрические семейства

| Модель   | Параметры (θ) | Слои E/D | d\_model | d\_ff | Heads |
| -------- | ------------- | -------- | -------- | ----- | ----- |
| T5‑Small | 60 M          | 6/6      | 512      | 1024  | 8     |
| T5‑Base  | 220 M         | 12/12    | 768      | 3072  | 12    |
| T5‑Large | 770 M         | 24/24    | 1024     | 4096  | 16    |
| T5‑3B    | 3 B           | 24/24    | 1024     | 4096  | 32    |
| T5‑11B   | 11 B          | 24/24    | 1024     | 4096  | 32    |

Эти профили используются для эмпирической оценки кривых *loss vs compute* и демонстрируют сублинейную отдачу после ≈3 B параметров, что коррелирует с результатами каплановских кривых.

---

## 5 Сравнительный анализ

| Аспект          | BERT                    | GPT‑2/3           | T5              |
| --------------- | ----------------------- | ----------------- | --------------- |
| Архитектура     | Encoder                 | Decoder‑only      | Encoder‑Decoder |
| Обучение        | Masked LM               | Next‑Token LM     | Span‑Corruption |
| Интерфейс задач | Расчёт скрытого вектора | Порождение токена | Текст → текст   |

T5 сочетает плюсы BERT‑подобного денойзинга (robust representations) и GPT‑подобной генерации, но избегает *exposure bias* типичного seq2seq, так как fine‑tune происходит на конкретной task‑prompt.

---

## 6 Типовые downstream‑приложения

1. **Суммаризация** — T5‑11B на XSum достигает ROUGE‑L = 44.3 без специализированного корпуса;
2. **Машинный перевод** — конкурентна Transformer‑Big (WMT14 En‑De BLEU = 30+);
3. **Вопрос‑ответ** — EM/F1 на SQuAD v2 превышает 90 при кастомном prompting;
4. **Reasoning‑Benchmarks** (SuperGLUE, MMLU) — существенный рост после instruction‑tuning (FLAN‑T5).

---

## 7 Ограничения и научные вызовы

- **Квадратичная сложность** внимания $\mathcal{O}(n^2)$ ограничивает контекст ≲ 2K токенов; активные исследования — FlashAttention‑2, Longformer, Hyena.
- **Computational Overhead**: обучение T5‑11B требует TPU‑v3‑512 ≈ 1.6 M часов FLOPs.
- **Semantic Drift / Hallucination**: проблема усиливается при low‑resource fine‑tune; методы контроля — R‑detach, DPO, RLHF.
- **Data Bias**: лимитированная фильтрация C4 приводит к генерации токсичных и гендерно‑несбалансированных выходов.

---

## 8 Эволюция и наследники

| Модель         | Ключевая модификация                      | Год   |
| -------------- | ----------------------------------------- | ----- |
| **T5 1.1**     | Улучшенные нормировки, больший batch size |  2021 |
| **mT5**        | Полноценный multilingual (101 язык)       |  2021 |
| **FLAN‑T5**    | Instruction fine‑tune на 1.8 K задач      |  2022 |
| **UL2‑T5**     | Унификация R‑, S‑, N‑denoise objectives   |  2023 |
| **Gemma‑Inst** | Лёгкая instr‑gating, open‑source          |  2025 |

---

## 9 Заключение

T5 институционализировала тезис «*language tasks are text generation*», выровняв поле NLP под единый loss и емкий интерфейс. Современные работы смещаются к смешанным objective‑ам, удешевлению контекстной обработки и композиции генерации с retrieval‑механизмами (RAG). Ключевой вопрос — как совместить *scaling laws* с энергоэффективностью и этическими ограничениями.
