
# GPT Architecture 

URL: https://dugas.ch/artificial_curiosity/GPT_architecture.html

---

## 0 Краткое резюме
Статья Дугаса пошагово разбирает архитектуру GPT‑3, начиная с формата входа/выхода и заканчивая полной схемой из 96 трансформер‑блоков. Автор — визуализатор, поэтому основное внимание уделено **линейной алгебре** (матрицы размеров 2048 × d) и приёму «сверни всё в один рисунок». Главные выводы — *двойное* использование весов Embedding / Softmax, единственная меж‑токенная операция $QK^\top$, огромная размерность скрытого пространства ( $d_\text{model}=12\,288$ ) и важность позиции маски в автогрессивности.

---

## 1 Постановка задачи
GPT — **casual decoder‑only Transformer**: на входе фиксированная последовательность из $N=2048$ токенов, на выходе — вектор вероятностей для *каждой* позиции (но для генерации интересует только последняя).

$$
\underbrace{(x_1,\dots,x_N)}_{\text{sequence}}
\;\xrightarrow{\text{GPT}}\;
\bigl(p_1,\dots,p_N\bigr), \quad
p_i\in\Delta^{V-1},
$$
где $V=50\,257$ — размер словаря BPE.

---

## 2 Токенизация и one‑hot код
GPT‑3 использует **byte‑level BPE**; строка «Not all heroes wear capes» разделяется на `[Not, all, heroes, wear, cap, es]` с id `[3673,477,…]`.  
Каждый id превращается в one‑hot вектор $\mathbf{o}\in\mathbb R^{V}$ (одна «1»).

---

## 3 Семантическая проекция — Embedding  
Матрица эмбеддинга $W_E\in\mathbb R^{V\times d_\text{model}}$ переводит one‑hot в плотный вектор:

$$
O_{N\times V}\;W_E\;=\;E_{N\times d_\text{model}},\quad
d_\text{model}=12\,288\,\text{для GPT‑3}.
$$

---

## 4 Позиционное кодирование  
Для каждой позиции $pos$ вычисляют синусоиды разной частоты:

$$
\mathrm{PE}_{(pos,2i)}=\sin\!\frac{pos}{10000^{2i/d_\text{model}}},
\quad$$
$$\mathrm{PE}_{(pos,2i+1)}=\cos\!\frac{pos}{10000^{2i/d_\text{model}}}.
$$

Матрицы $E$ и $P$ одинакового размера $N\times d_\text{model}$ и **складываются**.

---

## 5 Scaled Dot‑Product Attention  

1. Линейные проекции:
   $$
   Q=EW^Q,\;K=EW^K,\;V=EW^V,\quad
   W^\star \in \mathbb R^{d_\text{model}\times d_k},\ d_k=128.
   $$
2. Меж‑токенное взаимодействие — **единственный шаг, где строки матрицы «видят» друг друга**:
   $$
   A=\text{softmax}\!\Bigl(\tfrac{QK^\top}{\sqrt{d_k}}\Bigr).
   $$
3. Смешивание значений: $Z=AV$.

---

## 6 Multi‑Head Attention  
Процесс из § 5 выполняется $H=96$ раз (по кол‑ву голов), результаты конкатенируются и проецируются назад в $d_\text{model}$.

---

## 7 Feed‑Forward блок
Двуслойная MLP одинаково применяется к каждой строке:

$$
\mathrm{FFN}(x)=\max(0,xW_1+b_1)\;W_2+b_2,
\quad
\dim W_1 = d_\text{model}\times4d_\text{model}.
$$

---

## 8 Add & Norm (Pre‑LN)  
Каждый под‑блок заключён в Residual + LayerNorm; начиная с GPT‑2 нормировка перемещена *до* под‑блока и добавлена финальная LayerNorm в конце стека.

---

## 9 Стеки и глубина  
GPT‑3‑175B содержит **96** повторений:  
`[Pre‑LN] → MHA → Add → Pre‑LN → FFN → Add`.

---

## 10 Декодирование и weight‑tying  
Выходной тензор $E^\star$ размером $N\times d_\text{model}$ умножается на **транспонированную** матрицу эмбеддинга $W_E^\top$ — переиспользование весов экономит 615 M параметров:

$$
\hat y = \text{softmax}\bigl(E^\star W_E^\top\bigr)\in\mathbb R^{N\times V}.
$$

---

## 11 Генерация
Начиная с префикса $(x_1,\dots,x_m)$, модель *автогрессивно* предсказывает $x_{m+1}$, добавляет его к контексту и повторяет. На практике применяют top‑k / nucleus sampling ($p$) для разнообразия.

---

## 12 Sparse Attention  
GPT-3 чередует плотные (dense) и разреженные (sparse) слои внимания: в чётных слоях каждый токен видит все остальные, а в нечётных — только фиксированное «окно» соседей плюс периодические «стрыдовые» (strided) позиции — идея заимствована из работы Sparse Transformer.

* Локальное окно (banded local, например $w=128$) обеспечивает высокое разрешение для ближайшего контекста.

* Strided-heads (шаг $s=128$) периодически «перепрыгивают» через всю последовательность, сохраняя информацию о дальних зависимостях.

* Суммарная сложность падает с $O(N^{2})$ до $O(N,w)$, что даёт ≈ 5× экономию памяти и времени на контексте 2048 токенов, позволяя держать 96 слоёв и 175 B параметров в одном forward-проходе.

> **Практический итог**: плотные слои дают глобальный контекст, sparse — линейную/квазилинейную сложность. Такое чередование оказалось простым и эффективным компромиссом, на котором позже построены Longformer, BigBird и Flash-Attention 2.
---

## 13 Ключевые инсайты  
| Инсайт                                       | Расшифровка                                                                                                                                                                                                                                                                      |
| -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Каузальность vs BERT**                     | GPT-3 использует левотреугольную маску $M_{ij}=-\infty$ при $j>i$ — каждое внимание смотрит ТОЛЬКО в прошлое; у BERT маски нет, поэтому он двунаправлен → MLM.                                                                                            |
| **Shared-Q/K/V weights**                    | В большинстве реализаций GPT-стека каждая голова имеет *свою* пару $(W_h^Q,W_h^K,W_h^V)$, но ядро CUDA объединяет их в один батч-матрмул ради скорости; «единую матрицу» следует понимать как оптимизацию вычислений, а не как фактическое тождество весов.|
| **Embedding ↔ Softmax tie**                  | Параметры $W_E$ используются дважды (input & output). Кроме экономии памяти это создаёт «информационный канал» между head-слоями и логитами, повышая переформулирование редких токенов.                                                          |
| **$QK^\top$ — главный “O($N^2$)” шаг**   | Время и память растут квадратично только здесь; все остальные операции (FFN — $O(Nd)$, линейки — $O(Nd^2)$) масштабируются линейно/квази-линейно по длине. Именно поэтому sparse-patterns таргетируют именно $QK^\top$.            |
| **Невероятный $d_{\text{model}}=12,288$** | Такая ширина хранит «рабочую память» о 2048 токенах: если бы $d$ было меньше, модель теряла бы информацию или требовала больше слоёв. Большой $d$ также увеличивает количество параметров FFN (\~2/3 от общего числа).                               |

---