# Sharded Training и ZeRO

Когда модель становится слишком большой, чтобы хранить все параметры, градиенты и состояния оптимизатора на каждой GPU, стандартные подходы не справляются. Решение — **ZeRO** (Zero Redundancy Optimizer) из DeepSpeed: он распределяет все необходимые данные между устройствами, устраняя дублирование.

---

## Что делает ZeRO

Базовая идея ZeRO — **шардировать** ключевые компоненты обучения:

| Компонент              | Обычное обучение | ZeRO Stage 1 | ZeRO Stage 2 | ZeRO Stage 3 |
|------------------------|------------------|--------------|--------------|---------------|
| Параметры модели       | Копируются       | Копируются   | Копируются   | **Шардируются** |
| Градиенты              | Копируются       | Копируются   | **Шардируются** | **Шардируются** |
| Состояния оптимизатора | Копируются       | **Шардируются** | **Шардируются** | **Шардируются** |

> Таким образом, ZeRO Stage 3 минимизирует память на GPU до теоретического предела, позволяя обучать модели на сотни миллиардов параметров.

Также ZeRO оптимизирует использование **активаций, временных буферов и других временных объектов**, включая их offload и переиспользование.

---

## Комбинирование с другими схемами

- **ZeRO Stage 1** хорошо сочетается с **TP и PP** — т.к. шардируются только optimizer state, а остальные компоненты остаются на месте.
- **Stage 2/3** — менее совместимы с pipeline parallelism, потому что требуют частой агрегации градиентов с микробатчей, что увеличивает overhead при большом числе чанков.

> Хорошая комбинация — **ZeRO-3 + TP**, где TP применяется внутри одного хоста (на 8 GPU), а ZeRO-3 между хостами (через DDP).

---

## ZeRO Offload

Одно из важных расширений — **ZeRO Offload**:

- **Оптимизаторные состояния и градиенты выгружаются на CPU** (host memory).
- Обновление параметров также выполняется на CPU.
- Используется специальный оптимизатор `DeepSpeedCPUAdam`, который быстрее обычного `torch.optim.Adam` на CPU **в 5 раз**.

> Это даёт возможность обучать модели, которые не помещаются даже при ZeRO‑3, за счёт переноса нагрузки в RAM.

---

## ZeRO Infinity

**ZeRO-Infinity** — следующий шаг масштабирования, в котором:

1. Используется **ZeRO-3** как основа.
2. Все параметры, градиенты и состояния могут быть выгружены в **NVMe-диск**, а не только в CPU.
3. Применяется **Infinity Offload Engine** — библиотека с поддержкой pinned memory и оптимизированных пересылок.
4. Включён **чекпойнтинг активаций** с выгрузкой на CPU.
5. **Memory-centric operator tiling** — разбиение операций для экономии памяти.

Также реализованы оптимизации по пропускной способности:

- Используется **all-gather вместо broadcast**, что уменьшает накладные расходы при сборке параметров на CPU.
- Пересылки **прячутся за вычислениями**, если batch size достаточно большой.
- Специализированная библиотека **DeepNVME** ускоряет работу с NVMe-дисками.

---

## Когда использовать ZeRO

- Когда модель слишком велика для стандартного DDP/PP/TP.
- Когда доступно много CPU памяти или быстрые NVMe-диски.
- Когда важна **минимизация GPU-памяти** при сохранении функциональности.

---

## Вывод

**ZeRO и его расширения (Offload, Infinity)** — фундаментальные инструменты для масштабирования моделей на сотни миллиардов параметров. Они позволяют минимизировать дублирование, гибко распределять память между CPU, GPU и NVMe, и работают лучше, чем чистый Model Parallel.

> В современных кластерах лучший стек выглядит как: **ZeRO‑3 + TP (внутри хоста) + DDP (между хостами)** + опционально offload и Infinity при нехватке памяти.

