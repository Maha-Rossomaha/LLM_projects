# Quantization types

## 1 Data format

### 1.0 FP-представление: знак + экспонента + мантисса
- **Знак** - 1 бит
- **Экспонента** - N bit (со сдвигом)
- **Мантисса** - M bit (нормализованная дробная часть)

#### Преимущества
- Позволяет представлять как очень маленькие, так и очень большие числа.
- Высокая точность благодаря длинной мантиссе.

#### Недостатки
- Занимает много памяти. 
- Вычислительно тяжелее, особенно на low-resource устройствах 

### 1.1 FP32 — стандартная точность

* Используется во время обучения и inference по умолчанию.
* Представляет числа с плавающей запятой, 32 бита: 1 бит знак, 8 бит экспонента, 23 бита мантисса.
* Обеспечивает высокую точность, но требует много памяти и вычислений.
* Применяется для обучения моделей и в задачах, где важна максимальная точность.

### 1.2 FP16 — половинная точность

* 16-битное представление float: 1 бит знак, 5 бит экспонента, 10 бит мантисса.
* Уменьшает использование памяти и ускоряет матричные операции.
* Широко используется для mixed precision training.
* Требует поддержки со стороны оборудования (например, Tensor Cores).

### 1.3 BF16 — Brain Float 16

* 16-битное float-представление: 1 бит знак, **8 бит экспонента** (как у FP32), 7 бит мантисса.
* Лучше устойчивость к градиентам, чем FP16.
* Применяется в Google TPU, Intel CPU и в PyTorch/XLA.
* Часто используется в training и inference вместо FP32 для экономии ресурсов без потери стабильности.

### 1.4 INT8 — квантование 8 бит

* Целочисленный формат, диапазон обычно \[-128, 127] или \[0, 255].
* Требует scale и zero-point для преобразования float → int и обратно.
* Используется как компромисс между точностью и скоростью.
* Поддерживается многими фреймворками: PyTorch, ONNX, TensorRT, TFLite, bitsandbytes.

### 1.5 INT4 — 4-битное квантование

* Используется для inference сверхбольших моделей (например, LLaMA 65B).
* Позволяет запускать 65B модель на 1 GPU 16 ГБ.
* Реализуется через weight-only quantization или GPTQ/AWQ.
* Требует компенсации (например, group-wise scaling, activation-aware methods).

### 1.6 INT2 и Binary

* Экстремальные формы квантования: 2 бита или 1 бит на вес.
* Могут использоваться в специализированных задачах (например, BNN — binary neural networks).
* Сильно ухудшают точность, применимы только в случае очень простых задач или edge inference.

---

## 2 Quantization mechanic

Квантование — это приближение вещественных весов модели (или активаций) с помощью дискретных (обычно целочисленных) представлений, чтобы уменьшить объем памяти и ускорить вычисления.

### 2.1 Базовая формула

$$
w_{float} \approx scale \times (w_{int} - zero)
$$

* **$w_{float}$** — исходное значение с плавающей точкой.
* **$w_{int}$** — квантованное целочисленное значение.
* **scale** — коэффициент преобразования, определяющий шаг между уровнями квантования.
* **zero-point** — смещение, позволяющее правильно аппроксимировать ноль в целочисленном представлении (обычно 128 для uint8).

Если zero-point не используется (симметричное квантование), формула упрощается до:

$$
w_{float} \approx scale \times w_{int}
$$

### 2.2 Уровни применения scale и zero-point

* **Per-tensor** — один scale и zero-point на весь тензор или слой.

  * Простая реализация, низкие затраты по памяти.
  * Может быть недостаточно гибким для гетерогенных данных.

* **Per-channel** — отдельный scale и/или zero-point для каждого выходного канала (чаще используется в сверточных слоях).

  * Выше точность, особенно в CNN и больших MLP-блоках.
  * Требует больше памяти, но хорошо поддерживается в фреймворках.

* **Group-wise** — веса разбиваются на блоки (например, по 128 значений), и для каждого блока используется один scale и codebook.

  * Популярен в GPTQ и AWQ.
  * Компромисс между точностью и затратами на хранение.

### 2.3 Uniform quantization

* Значения разбиваются на **равномерные интервалы**, scale одинаковый по всему диапазону.
* Обычно применяется для INT8 (и выше).
* Прост в реализации: умножение/деление и округление.
* Поддерживает вышеуказанные уровни (per-tensor, per-channel).
* Пример фреймворков: PyTorch QAT, ONNX Runtime, TFLite.

### 2.4 Non-uniform quantization

* Вместо фиксированного шага применяется **таблица соответствий** (codebook):
  $w_{float} \approx codebook[w_{int}]$
* Часто используется в 4-битных и 2-битных схемах (GPTQ, AWQ).
* Codebook содержит float-значения, а веса кодируются как индексы.
* Обычно применяется **weight-only**, без квантования активаций.
* Позволяет достичь **высокой точности** при сильном сжатии.

### 2.5 Weight-only vs Activation quant

#### Weight-only quantization

* Квантуются только **веса модели**, активации остаются в float (FP16/FP32).
* Часто используется в LLM (например, GPTQ, AWQ).
* Не требует модификации вычислительного графа forward-pass.
* Простая реализация и высокая совместимость с существующими фреймворками.
* Обычно применимо к fully-connected слоям и attention.

**Плюсы:**

* Очень низкий overhead
* Хорошо работает при INT4/INT2
* Не требует калибровки данных

**Минусы:**

* Использование float-активаций ограничивает экономию памяти и пропускную способность

#### Activation quantization

* Квантуются **временные значения** (outputs промежуточных слоёв, attention outputs, hidden states и т.д.)
* Применяется в full quantization и quantization-aware training (QAT)
* Более чувствительна к ошибкам, особенно при небольшом диапазоне значений
* Требует тонкой настройки scale/zero-point **на каждом слое и направлении (input/output)**

**Проблемы и особенности:**

* **Динамический диапазон** активаций может меняться от батча к батчу → нужны адаптивные scale.
* Влияние ReLU / GELU: нелинейности могут создавать плотные пики значений.
* **Outlier-эффекты**: несколько экстремальных значений могут испортить весь scale, если брать min/max → часто используется percentile-квантование (например, 99.99%).
* **Переполнение**: INT8 может не вместить large activations → возможны ошибки при накоплении градиентов или при длинных последовательностях.
* **Сложность масштабируемости**: особенно тяжело на длинных sequence-to-sequence моделях с attention.
* Требуется **calibration dataset** — небольшое подмножество реальных данных, на котором собирается статистика активаций.
* На практике почти всегда используется **per-channel quant** для активаций, иногда даже per-group.

**Подходы:**

* **Static activation quantization (post-training):**

  * Калибровка на выборке
  * Используется фиксированный scale в дальнейшем
* **Dynamic activation quantization:**

  * Scale считается во время выполнения (например, на уровне layer norm / batch)
  * Часто используется в Transformer-инференсе с INT8

**Фреймворки:**

* PyTorch `torch.quantization`, `torch.ao.quantization`
* TensorRT INT8
* TFLite QAT + PTQ

**Вывод:**

* Weight-only проще и надёжнее, особенно для inference.
* Activation quant требует аккуратной настройки, но даёт больший выигрыш по памяти и скорости в полной INT8/INT4-инференс-системе. 
 
> **Вывод:** выбор метода зависит от требований к точности, скорости и доступности обучающих данных. Weight-only non-uniform методы (GPTQ, AWQ) идеально подходят для inference LLM без переобучения.
