## Конспект: Byte-level BPE и его реализация на Python

## 1. Что такое byte-level BPE?

Алгоритм Byte-Pair Encoding (BPE) изначально был разработан для сжатия текста путём итеративного объединения самых частотных пар символов. В модифицированной версии для NLP BPE агрегирует подстроки до тех пор, пока не достигнет заданного размера словаря. Byte-level BPE предварительно кодирует входной текст в UTF-8 и рассматривает каждую кодовую единицу (байт) как отдельный символ исходного словаря, что гарантирует поддержку любых символов без использования токена UNK.

## 2. Мотивация и преимущества

Byte-level BPE устраняет проблему неуниверсальности словарей, созданных на уровне символов или слов, поскольку любой символ UTF-8 может быть представлен одним или несколькими байтами. Это особенно важно для мультилингвальных приложений и разнообразных доменов, где набор символов выходит за рамки ASCII. Кроме того, устранение UNK-символа повышает полноту обратного декодирования, сохраняя точность реконструкции исходного текста.

## 3. Алгоритм обучения byte-level BPE

1. **Инициализация**: собрать корпус и закодировать его в байты UTF-8, каждое значение байта является начальным токеном.
2. **Подсчёт частот**: подсчитать частоты всех смежных пар токенов (байт) на всём корпусе.
3. **Слияние**: найти наиболее частую пару и объединить её в новый токен; обновить частоты вокруг новых токенов.
4. **Повтор**: шаги 2–3 повторять, пока размер словаря не достигнет целевого значения.
5. **Добавление специальных токенов**: вставить метки начала/конца последовательности и другие по необходимости.

## 4. Пример реализации на Python

```python
from tokenizers import ByteLevelBPETokenizer
from tokenizers.trainers import BpeTrainer
from tokenizers.processors import BertProcessing

# 1. Инициализация
tokenizer = ByteLevelBPETokenizer()

# 2. Настройка тренера
trainer = BpeTrainer(
    vocab_size=30000,
    min_frequency=2,
    special_tokens=["<s>","<pad>","</s>","<unk>","<mask>"]
)

# 3. Обучение на файлах корпуса
tokenizer.train(files=["path/to/corpus.txt"], trainer=trainer)

# 4. Пост‑обработка (пример для BERT-подобных моделей)
tokenizer._tokenizer.post_processor = BertProcessing(
    ("</s>", tokenizer.token_to_id("</s>")),
    ("<s>", tokenizer.token_to_id("<s>"))
)

# 5. Сохранение
tokenizer.save_model("path/to/output_dir")
```

## 5. Валидация и использование
После обучения рекомендуется проверить:
- При прямой и обратной токенизации исходные тексты восстанавливаются без изменений.
- Распределение длин токенов и покрытие специализированных терминов.

Для использования:
```python
from tokenizers import ByteLevelBPETokenizer
# Загрузка обученного
tokenizer = ByteLevelBPETokenizer(
    "path/to/output_dir/vocab.json",
    "path/to/output_dir/merges.txt"
)
ids = tokenizer.encode("Пример текста").ids
print(tokenizer.decode(ids))
```