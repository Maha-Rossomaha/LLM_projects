# Конспект: Принципы построения и переобучения токенизатора

## 1. Основы устройства токенизатора

Токенизатор преобразует входной текст в последовательность идентификаторов, обеспечивая сопоставление между текстом и моделью. Его конвейер состоит из пяти основных этапов:

1. **Нормализация**
   - Приведение символов к единому виду (разложение сложных символов, приведение к нижнему регистру, удаление лишних пробелов и акцентов).
2. **Предтокенизация**
   - Первый разбиение текста на базовые фрагменты (пробелы, разделители, байты).
3. **Модель сегментации**
   - Статистический алгоритм, определяющий набор элементов (токенов) и правило их разбиения; обучается на корпусе, формируя словарь фиксированного размера.
4. **Пост‑процессинг**
   - Вставка специальных меток (начало/конец последовательности, разделители для пары текстов и т. п.) и формирование окончательной структуры токенов.
5. **Декодер**
   - Обратное преобразование идентификаторов токенов в читаемый текст, включая объединение фрагментов и удаление внутренних служебных символов.

> **Проверка корректности**: после прямой и обратной обработки текст должен восстанавливаться без изменений; длина токенов и покрытие важных терминов оцениваются на примерах из целевого домена.

## 2. Переобучение токенизатора на основе готового

Переобучение (доработка) токенизатора позволяет сохранить выбранную архитектуру и набор служебных токенов, одновременно адаптировав словарь под новые данные.

### 2.1 Когда это нужно

- Смена языка или специального домена (например, медицина, юриспруденция, код).
- Требуется контроль размера словаря для оптимизации скорости и памяти.
- Нужна совместимость с уже настроенной моделью и её спецификой токенов.

### 2.2 Подготовка корпуса

- Собрать набор текстов, отражающий целевую область.
- Оптимизировать чтение батчами, чтобы экономить оперативную память.

### 2.3 Тренировка нового словаря

```python
from transformers import AutoTokenizer

# Загрузка существующего токенизатора
base_tokenizer = AutoTokenizer.from_pretrained("<имя_модели>")

# Обучение нового словаря на подготовленных текстах
new_tokenizer = base_tokenizer.train_new_from_iterator(
    iterator=корпус,
    vocab_size=ЖЕЛАЕМЫЙ_РАЗМЕР_СЛОВАРЯ
)
```

- `train_new_from_iterator` выполняет детерминированную статистическую процедуру, результат которой зависит только от корпуса и настроек.

### 2.4 Валидация результатов

- Сравнить длину закодированных примеров до и после;
- Проверить, что важные терминологии не раскалываются некорректно;
- Убедиться, что токенизация + детокенизация даёт исходный текст.

### 2.5 Сохранение и использование

```python
new_tokenizer.save_pretrained("путь_к_токенизатору")
```

- После сохранения загрузить токенизатор уже с новым словарём, передать в модель или опубликовать для совместного использования.

---

**Ключевые моменты**

- Конвейер токенизатора разбит на независимые этапы, каждый можно настраивать.
- Переобучение словаря не затрагивает логику сегментации и служебные токены.
- Детальная валидация необходима для корректной работы в целевом домене.

## 3. Популярные алгоритмы токенизации

Ниже перечислены основные подходы в хронологическом порядке:

1. **Whitespace-токенизация** — простое разбиение по пробелам и пунктуации.
2. **Символьная (Character-level)** — каждый символ считается отдельным токеном.
3. **Правила морфологического анализа** — использование языковых правил для выделения корней, аффиксов.
4. **Byte-Pair Encoding (BPE)** — статистическое слияние частых пар символов / байт.
5. **WordPiece** — вариант BPE с условной вероятностной моделью для выбора слияний.
6. **Unigram Language Model** — отбор подслов по вероятностной модели единого униграмма.
7. **Byte-level BPE** — расширение BPE, работающее на байтовом уровне для поддержки любых символов.

