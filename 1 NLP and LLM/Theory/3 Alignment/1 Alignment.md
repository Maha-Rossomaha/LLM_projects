# 1 Зачем нужно согласование (alignment)

* **Проблема**: В данных для обучения языковых моделей отсутствует явный лосс на такие характеристики, как:

  * креативность,
  * правдивость,
  * безопасность,
  * корректность кода,
  * полезность ответа.

* **Цель**: Научить модель выдавать не просто вероятностно правдоподобный, а соответствующий заданным целям и нормам ответ (aligned output).

* **Мотивация**: Без согласования модель может генерировать ответы, которые формально соответствуют запросу, но при этом нарушают этические, юридические или технические ограничения.

* **Вторая причина**: Согласование также **улучшает общее качество модели**.
  Исследование OpenAI показало, что модели, обученные с помощью RLHF (в частности, PPO и PPO-ptx), стабильно выигрывают у модели, обученной только через SFT, на всех масштабах от 1.3B до 175B параметров.

## Полезные термины

1. **Pretraining** — предобучение модели на огромном, разнородном корпусе текстов. Это позволяет модели выучить общие языковые закономерности, факты и стили из больших объёмов данных без конкретной задачи.

2. **Instruction fine-tuning (IFT)** — обучение модели следовать инструкциям. В отличие от предобучения, где модель продолжает текст, здесь она учится распознавать структуру "запрос → ответ" и давать релевантный, связный отклик.

3. **Supervised fine-tuning (SFT)** — дообучение модели на размеченных примерах задач: например, Named Entity Recognition (NER), классификация, генерация ответа. Модель по-прежнему обучается предсказывать следующий токен, но теперь в рамках конкретной задачи.

> Все эти этапы используют предсказание следующего токена с функцией потерь cross-entropy.
>
> Instruction fine-tuning — частный случай SFT. Поэтому под SFT часто подразумевают обучение модели отвечать на вопросы по парам "инструкция → ответ".

## Замечание о человеческой разметке

* Давать абсолютные (точечные) оценки качества ответа трудно — разные аннотаторы могут интерпретировать критерии по-разному.
* Гораздо проще сравнивать два или более ответа между собой и выбирать лучший — **относительная оценка**.
* Такие ранжирования можно перевести в числовые значения через систему ELO или другие схемы нормализации:

  * Пример: A4 > A2 > A1 > A3 → ELO-оценки → нормализованные значения от 1.0 до 0.0
* Это позволяет собрать два типа датасетов:

  1. (вопрос, ответ, оценка)
  2. (вопрос, лучший\_ответ, худший\_ответ)

# 2 Этапы обучения согласованной модели (pipeline alignment)

### Шаг 1: Pretraining

* Обучение модели на массивном корпусе данных (до триллиона токенов).
* Данные зачастую некачественные, взяты из интернета.
* Цель — научить модель статистике языка: предсказывать следующий токен (next-token prediction).

### Шаг 2: Supervised Fine-Tuning (SFT)

* Начало этапа RLHF.
* Разметка: люди создают хорошие ответы на промпты.
* Модель дообучается на этих примерах prompt → response.
* Это формирует базовую модель, которая уже может следовать инструкциям.
* Объём данных: 10K–100K примеров.

### Шаг 3: Обучение reward-модели

* На каждый промпт генерируются несколько ответов (с помощью модели из SFT).
* Люди ранжируют ответы по качеству.
* Reward-модель обучается воспроизводить эти предпочтения.
* Объём данных: 100K–1M сравнений (prompt, лучший ответ, худший ответ).

### Шаг 4: Обучение финальной модели с помощью RL

* Reward-модель + модель из SFT используются в reinforcement learning.
* Алгоритм (обычно PPO) обучает новую модель, которая оптимизирует ожидаемое вознаграждение.
* Результат — модель, согласованная с человеческими предпочтениями (aligned model).
* Объём данных: 10K–100K промптов.

> Примеры моделей: InstructGPT, ChatGPT, Claude, StableVicuna  
> Открытые реализации: Falcon, Pythia, StableLM, Dolly-v2 и др.


# 3 Reward-модели

## 3.1 Point-wise Reward Model

* Reward-модель (RM) принимает на вход текст (например, промпт и ответ), и возвращает **одно число** — скалярную награду.
* Такой подход называется **point-wise**: каждое наблюдение обучается независимо, как в регрессии.
* Датасет содержит тройки вида: (x, y\_w, y\_l), где:

  * `x` — входной текст (промпт),
  * `y_w` — лучший ответ (winner),
  * `y_l` — худший ответ (loser).

### Обучение:

Reward-модель обучается так, чтобы вероятность предпочтения `y_w` над `y_l` была как можно выше:

$$
\mathbb{P}_\psi(y_w > y_l | x) = \frac{\exp(r_\psi(x, y_w))}{\exp(r_\psi(x, y_w)) + \exp(r_\psi(x, y_l))} = \sigma(r_\psi(x, y_w) - r_\psi(x, y_l))
$$

* Здесь `r_ψ(x, y)` — предсказанная награда,
* `σ` — сигмоида (см. график).

Это позволяет превратить относительные предпочтения в непрерывную функцию награды.

### Функция потерь:

Reward-модель обучается с помощью log-loss, где модель максимизирует вероятность правильного ранжирования:

$$
\mathcal{L}(r_\psi) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{rm}}\left[\log \sigma(r_\psi(x, y_w) - r_\psi(x, y_l))\right]
$$

* Модель штрафуется, если присваивает худшему ответу вышее значение, чем лучшему.
* Такая форма лосса аналогична бинарной логистической регрессии на разности reward-оценок.

---

## 3.2 Pair-wise Reward Model

* Вместо предсказания скалярной награды для каждого ответа отдельно, модель сразу сравнивает **два ответа** и выбирает лучший.
* На вход она получает строку вида:

  ```
  <T> Текст запроса
  <A> Ответ X
  <B> Ответ Y
  ```
* На выходе — метка (например, `A` или `B`), указывающая, какой из ответов предпочтительнее.

### Особенности:

* Модель обучается как классификатор: задача — предсказать, какой из двух ответов лучше.
* Подходит, когда важен **выбор**, а не оценка качества в абсолютных числах.
* Часто используется, когда хочется напрямую имитировать поведение аннотаторов.

### Преимущества:

* Не требует построения шкалы наград (в отличие от point-wise подхода).
* Может быть устойчивее при шумных предпочтениях в данных.

### Недостатки:

* Для получения глобального порядка между ответами нужно делать все возможные попарные сравнения.
* Это приводит к **квадратичной сложности** по числу кандидатов и росту вычислительных затрат при большом числе ответов.

---
# 4 RL алгоритмы
### Почему нужен RL

* Мы хотим напрямую максимизировать reward:
  $\max \text{Reward}(\text{LLM.generate}(x))$

* Но возникает проблема:

  * `LLM.generate(x)` — это **недифференцируемая** операция: она выдаёт дискретные токены через сэмплирование.
  * Reward тоже определяется **на дискретных сэмплах**, а не на вероятностных распределениях.

* Поэтому стандартная градиентная оптимизация не работает.

#### Вывод:

Чтобы оптимизировать поведение модели под награду, нам нужен **усиленный способ обучения** — **reinforcement learning (RL)**.

### Дополнительно: KL-дивергенция

* Мера удалённости двух распределений $P$ и $Q$
* **Неотрицательная**: всегда $\geq 0$
* **Несимметричная**: $KL(P\|\|Q) \neq KL(Q\|\|P)$
* Интерпретируется как **дополнительная информация**, которую потребовалось бы учесть, если использовать $Q$ вместо $P$
* В случае LLM: $X$ — это токены, а $P(x), Q(x)$ — вероятности генерации токена $x$ в двух политиках

$$
KL(P \|\| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$


## 4.1 Proximal Policy Optimization (PPO)

#### Что у нас есть:

* Reward-модель $r_\phi$
* Политика (policy), инициализированная из дообученной LLM после IFT: $\pi_\theta$
* Датасет промптов $D$

#### Идея PPO:

1. Берём промпт $x \in D$
2. Генерируем для него ответ $y \sim \pi_\theta$
3. Оцениваем ответ через reward-модель: $r_\phi(x, y)$
4. Обновляем параметры $\pi_\theta$, чтобы увеличивать награду
5. При этом ограничиваем, насколько сильно политика $\pi_\theta$ может измениться — добавляем штраф за отклонение от исходной (старой) версии модели

#### Зачем это нужно:

* Без ограничений модель может **переобучиться на отдельные примеры**, теряя обобщающую способность
* Ограничение ("proximal") делает шаги обучения более **стабильными и предсказуемыми**

#### Использование KL-дивергенции
* В PPO KL-дивергенция используется как регуляризатор: она ограничивает, насколько новая политика может отклоняться от старой
* Это помогает сохранять **стабильность обучения** и предотвращает деградацию поведения модели

### PPO: общий обзор процесса обучения
![alt_text](../0%20images/image_1.png)

#### 1. Rollout

* **Query**: отправляем запрос (например, "This movie is") в языковую модель (LM)
* **LM**: текущая активная модель генерирует **response** (например, "really great!")

#### 2. Evaluation

* **Query + Response** отправляется в **reward model**
* Reward-модель (обучена на человеческих предпочтениях, правилах или классификаторах) возвращает скалярную **награду** — насколько хороший ответ

#### 3. Optimization (PPO)

* Сравниваются **log-пробабилити** ответа из двух моделей:

  * **Active model** $\pi_\theta$: текущая политика
  * **Reference model** $\pi_\text{ref}$: зафиксированная модель до RL (например, после SFT)

* Вычисляется:

  * **Reward** за текущий ответ
  * **KL-дивергенция** между текущей и референсной моделью
  * **Скорректированная награда** = `reward - β × KL`

* С помощью policy gradient (градиентной оптимизации по политике) веса $\pi_\theta$ обновляются таким образом, чтобы увеличивать награду, но не уходить слишком далеко от начального поведения

> Это и есть суть **Proximal Policy Optimization** — обновлять политику осторожно, с ограничением по KL
