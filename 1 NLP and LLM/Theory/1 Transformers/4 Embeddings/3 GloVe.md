# –ö–æ–Ω—Å–ø–µ–∫—Ç —Å—Ç–∞—Ç—å–∏ ¬´Global Vectors for Word Representation¬ª (Pennington et al., 2014)

URL:  
üîó [¬´Global Vectors for Word Representation¬ª](https://aclanthology.org/D14-1162/)

## 1. –í–≤–µ–¥–µ–Ω–∏–µ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏—è
GloVe –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ (Word2Vec CBOW/Skip-gram), —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö: —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏–º—É—é –ª–µ–∫—Å–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

## 2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞

### 2.1. –ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
–ü—É—Å—Ç—å $X$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ $V\times V$, –≥–¥–µ $X_{ij}$ ‚Äî —á–∏—Å–ª–æ, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ $j$ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–ª–æ–≤–∞ $i$ (–≤ –æ–∫–Ω–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞).  
–û–ø—Ä–µ–¥–µ–ª–∏–º:
- $X_i = \sum_k X_{ik}$ ‚Äî –æ–±—â–µ–µ —á–∏—Å–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–ª–æ–≤–∞ $i$.
- $P_{ij} = X_{ij} / X_i$ ‚Äî —É—Å–ª–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å $j$ —Ä—è–¥–æ–º —Å $i$.

### 2.2. –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è
–ò–¥–µ—è: –æ—Ç–Ω–æ—à–µ–Ω–∏–µ $P_{ik}/P_{jk}$ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤ $i$ –∏ $j$ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ $k$.  
–ñ–µ–ª–∞–µ–º–∞—è –º–æ–¥–µ–ª—å:  
$$
F(w_i, \tilde w_j, \tilde b_j, b_i) = w_i^T \tilde w_j + b_i + \tilde b_j \approx \log X_{ij}.
$$  
–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∫–≤–∞–¥—Ä–∞—Ç –æ—à–∏–±–æ–∫:  
$$
J = \sum_{i,j=1}^V f(X_{ij}) \bigl(w_i^T\tilde w_j + b_i + \tilde b_j - \log X_{ij}\bigr)^2.
$$  
–ó–¥–µ—Å—å $f(x)$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è-–≤–µ—Å, –∫–æ—Ç–æ—Ä–∞—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–∫–ª–∞–¥ —Ä–µ–¥–∫–∏—Ö –∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã—Ö –ø–∞—Ä:
$$
f(x) =
\begin{cases}
(x/x_{\text{max}})^\alpha, & x < x_{\text{max}},\\
1, & x \ge x_{\text{max}}.
\end{cases}
$$  
–¢–∏–ø–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: $x_{\text{max}}=100$, $\alpha=3/4$.

## 3. –†–∞–∑–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

1. **–î–≤–µ –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤**  
   - $W\in\mathbb R^{V\times d}$ ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ü–µ–ª–µ–≤—ã—Ö —Å–ª–æ–≤.  
   - $\tilde W\in\mathbb R^{V\times d}$ ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤.  
2. **–°–º–µ—â–µ–Ω–∏—è**  
   - –í–µ–∫—Ç–æ—Ä—ã $b\in\mathbb R^V$ –∏ $\tilde b\in\mathbb R^V$ –¥–ª—è —Å–º–µ—â–µ–Ω–∏–π (bias).  
3. **–û–±—É—á–µ–Ω–∏–µ**  
   - –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: AdaGrad.  
   - –û–±—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ–≤–º–µ—Å—Ç–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –ø–æ –Ω–µ–Ω—É–ª–µ–≤—ã–º $X_{ij}$.  
   - –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∏—à—å –Ω–µ–Ω—É–ª–µ–≤—ã–µ –ø–∞—Ä—ã $(i,j)$, —á—Ç–æ–±—ã –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å –≤—Å—é –º–∞—Ç—Ä–∏—Ü—É $V\times V$.

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ $i$ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∫–∞–∫ —Å—É–º–º–∞:  
$$
\mathbf{v}_i = w_i + \tilde w_i.
$$

## 4. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- **–ö–æ—Ä–ø—É—Å**: –æ–±—ã—á–Ω–æ ‚Äì –í–∏–∫–∏–ø–µ–¥–∏—è, Common Crawl –∏ —Ç. –ø.  
- **–û–∫–æ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä**: 5‚Äì10 —Å–ª–æ–≤ —Å–ª–µ–≤–∞/—Å–ø—Ä–∞–≤–∞.  
- **–ü—Ä–µ- –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞**: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç –∏—Å–∫–ª—é—á–∞—Ç—å—Å—è.

## 5. –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim
from collections import defaultdict

# 1. –°–æ–±–∏—Ä–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å–æ-–≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
def build_cooccurrence(corpus, window_size=5):
    vocab = {w:i for i,w in enumerate(set(corpus))}
    inv_vocab = {i:w for w,i in vocab.items()}
    X = defaultdict(float)
    for idx, word in enumerate(corpus):
        w_i = vocab[word]
        start = max(0, idx - window_size)
        end   = min(len(corpus), idx + window_size + 1)
        for j in range(start, end):
            if j == idx: continue
            w_j = vocab[corpus[j]]
            X[(w_i, w_j)] += 1.0 / abs(j - idx)
    return X, vocab, inv_vocab

# 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å GloVe
class GloVe(nn.Module):
    def __init__(self, vocab_size, emb_dim, x_max=100, alpha=0.75):
        super().__init__()
        self.w  = nn.Embedding(vocab_size, emb_dim)
        self.w_tilde = nn.Embedding(vocab_size, emb_dim)
        self.b  = nn.Embedding(vocab_size, 1)
        self.b_tilde = nn.Embedding(vocab_size, 1)
        self.x_max = x_max
        self.alpha = alpha

    def forward(self, i_idx, j_idx, X_ij):
        w_i = self.w(i_idx)
        w_j = self.w_tilde(j_idx)
        b_i = self.b(i_idx).squeeze()
        b_j = self.b_tilde(j_idx).squeeze()
        f = torch.where(X_ij < self.x_max,
                        (X_ij / self.x_max)**self.alpha,
                        torch.ones_like(X_ij))
        pred = (w_i * w_j).sum(dim=1) + b_i + b_j
        loss = f * (pred - torch.log(X_ij))**2
        return loss.sum()

# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ
def train_glove(corpus, emb_dim=50, epochs=50, lr=0.05):
    X, vocab, inv_vocab = build_cooccurrence(corpus)
    pairs = list(X.items())
    model = GloVe(len(vocab), emb_dim)
    optimizer = optim.Adagrad(model.parameters(), lr=lr)
    for epoch in range(epochs):
        total_loss = 0
        for (i,j), X_ij in pairs:
            i_idx = torch.LongTensor([i])
            j_idx = torch.LongTensor([j])
            X_val = torch.FloatTensor([X_ij])
            optimizer.zero_grad()
            loss = model(i_idx, j_idx, X_val)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")
    embeddings = model.w.weight.data + model.w_tilde.weight.data
    return embeddings, vocab, inv_vocab

if __name__ == "__main__":
    sample_text = "we are what we repeatedly do excellence then is not an act but a habit".lower().split()
    emb, vocab, inv = train_glove(sample_text, emb_dim=20, epochs=100)
    print("Embedding for 'excellence':", emb[vocab["excellence"]])
```
