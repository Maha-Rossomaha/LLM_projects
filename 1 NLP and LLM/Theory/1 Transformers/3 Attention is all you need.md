# –ü–æ–¥—Ä–æ–±–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç: ‚ÄúAttention Is All You Need‚Äù (Vaswani et‚ÄØal., 2017)

URL:  
üîó [Attention is all you need](https://arxiv.org/pdf/1706.03762)

## 0‚ÄØ‚ÄØ–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è  
Transformer¬†‚Äî –ø–µ—Ä–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è **–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ** –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è, –±–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑¬†6¬†–∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä‚Äë–±–ª–æ–∫–æ–≤ –∏¬†6¬†–¥–µ–∫–æ–¥–µ—Ä‚Äë–±–ª–æ–∫–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑¬†–∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç Multi‚ÄëHead Attention –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ‚Äë–Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é Feed‚ÄëForward¬†MLP —Å —Ä–µ–∑–∏–¥—É–∞–ª—å–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –∏ LayerNorm. –í–≤–µ–¥–µ–Ω—ã **Scaled¬†Dot‚ÄëProduct Attention**, —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω–æ–µ **Positional‚ÄØEncoding** –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å¬†–ø—Ä–æ–≥—Ä–µ–≤–æ–º¬†learning‚Äërate. –ù–∞ –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∞ WMT‚Äë14 Transformer –¥–æ—Å—Ç–∏–≥ 28.4‚ÄØBLEU (EN‚ÜíDE) –∏¬†41.8‚ÄØBLEU (EN‚ÜíFR), –ø—Ä–µ–≤–∑–æ–π–¥—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –∏¬†RNN‚Äë–º–æ–¥–µ–ª–∏ –ø—Ä–∏ 3‚Äì4‚Äë–∫—Ä–∞—Ç–Ω–æ–º —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è.

---

## 1‚ÄØ‚ÄØ–í–≤–µ–¥–µ–Ω–∏–µ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏—è  
–î–æ¬†2017¬†–≥. –ª—É—á—à–∏–µ Seq2Seq‚Äë–º–æ–¥–µ–ª–∏ –ø–æ–ª–∞–≥–∞–ª–∏—Å—å –Ω–∞ RNN –∏–ª–∏ CNN¬†—ç–Ω–∫–æ–¥–µ—Ä—ã c‚ÄØattention‚Äë–º–æ–¥—É–ª–µ–º. Transformer —É—Å—Ç—Ä–∞–Ω—è–µ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏¬†–∏–Ω—Ñ–µ—Ä–µ–Ω—Å.

---

## 2‚ÄØ‚ÄØScaled¬†Dot‚ÄëProduct Attention  

$$
\mathrm{Attention}(Q,K,V)=
\operatorname{softmax}\!\Bigl(\tfrac{QK^{\top}}{\sqrt{d_k}}\Bigr)V,
$$

–≥–¥–µ $Q,K,V\in\mathbb{R}^{n\times d_k}$. –î–µ–ª–µ–Ω–∏–µ –Ω–∞¬†$\sqrt{d_k}$ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.

### 2.1‚ÄØ‚ÄØ–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–∏  

$$
\mathrm{MaskedAttn}(Q,K,V)=
\operatorname{softmax}\!\Bigl(\tfrac{QK^{\top}}{\sqrt{d_k}}+M\Bigr)V,
$$

–≥–¥–µ $M_{ij}=-\infty$ –ø—Ä–∏ $j>i$ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–µ—Ç ¬´–≤–∑–≥–ª—è–¥–∞ –≤¬†–±—É–¥—É—â–µ–µ¬ª.

---

## 3‚ÄØ‚ÄØMulti‚ÄëHead¬†Attention  

$$
\mathrm{head}_h=\mathrm{Attention}(QW_h^Q,KW_h^K,VW_h^V),$$
$$\mathrm{MHA}(Q,K,V)=\operatorname{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_H)W^O.
$$

–í –æ—Ä–∏–≥–∏–Ω–∞–ª–µ \(H=8\), \(d_k=64\), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑¬†—Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤.

---

## 4‚ÄØ‚ÄØ–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–ª–æ—á–Ω—ã—Ö –º–æ–¥—É–ª–µ–π  

### 4.1‚ÄØ‚ÄØ–≠–Ω–∫–æ–¥–µ—Ä‚Äë–±–ª–æ–∫  
`[Input] ‚Üí MHA ‚Üí FFN`, –æ–±–∞ –ø–æ–¥—Å–ª–æ—è –æ–∫—Ä—É–∂–µ–Ω—ã Residual‚ÄØ+‚ÄØLayerNorm.

### 4.2‚ÄØ‚ÄØ–î–µ–∫–æ–¥–µ—Ä‚Äë–±–ª–æ–∫  
`[Prev y] ‚Üí Masked¬†MHA ‚Üí Cross¬†MHA (Q –æ—Ç –¥–µ–∫–æ–¥–µ—Ä–∞, K,V –æ—Ç —ç–Ω–∫–æ–¥–µ—Ä–∞) ‚Üí FFN`.

---

## 5‚ÄØ‚ÄØPositional‚ÄØEncoding  

$$
\mathrm{PE}_{(pos,2i)}=\sin\bigl(pos/10000^{2i/d_{\text{model}}}\bigr),\quad$$
$$\mathrm{PE}_{(pos,2i+1)}=\cos\bigl(pos/10000^{2i/d_{\text{model}}}\bigr).
$$

–°–∏–Ω—É—Å–æ–∏–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª–∏ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –∏ –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

---

## 6‚ÄØ‚ÄØPosition‚ÄëWise Feed‚ÄëForward¬†Network  

$$
\mathrm{FFN}(x)=\max(0,xW_1+b_1)\,W_2+b_2,
$$

–≥–¥–µ $W_1\in\mathbb{R}^{d_{\text{model}}\times d_{\text{ff}}},\; d_{\text{ff}}=2048$.

---

## 7‚ÄØ‚ÄØ–°—Ö–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è  

| –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|--------------|----------|
| –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä  | Adam $(\beta_1=0.9,\ \beta_2=0.98)$ |
| Learning‚Äërate | $d_{\text{model}}^{-0.5}\!\cdot\!\min(\text{step}^{-0.5},\,\text{step}\cdot\text{warmup}^{-1.5}),\ \text{warmup}=4000$ |
| Dropout      | 0.1 |
| Label‚Äësmoothing | $\varepsilon=0.1$ |

---

## 8‚ÄØ‚ÄØ–†–µ–∑—É–ª—å—Ç–∞—Ç—ã  

| –ó–∞–¥–∞—á–∞ | BLEU |
|--------|------|
| WMT‚Äë14 EN‚ÜíDE | 28.4 |
| WMT‚Äë14 EN‚ÜíFR | 41.8 |

Transformer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ConvS2S –∏ GNMT, –æ–±—É—á–∞—è—Å—å –≤—Ç—Ä–æ–µ –±—ã—Å—Ç—Ä–µ–µ.

---

## 9‚ÄØ‚ÄØ–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ—Å—Ç—å  

- **Attention**: $O(n^{2}d)$ ‚Äî –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –ø–æ –¥–ª–∏–Ω–µ, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞.  
- **RNN**: $O(nd^{2})$ ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è; Transformer —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –Ω–∞ GPU/TPU.

---

## 10‚ÄØ‚ÄØ–í–ª–∏—è–Ω–∏–µ  
Transformer —Å—Ç–∞–ª —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –≤ NLP –∏ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª GPT, BERT, T5, ViT –∏ –¥—Ä., –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–≤, —á—Ç–æ ¬´attention¬†is¬†all¬†you¬†need¬ª.
