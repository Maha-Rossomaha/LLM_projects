# LLama4_1 Architecture  

URL:  
🔗 [LLama4-1](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)

## 1. Краткое резюме  
Meta представила семейство **Llama 4** — первые открытые модели с нативной мультимодальностью (текст ↔ изображения, аудио, видео) и колоссальными контекстными окнами (до 10 млн токенов). Линейка включает **Scout 17B**, **Maverick 400B** и анонсированный **Behemoth 2T**. Все варианты используют **Mixture‑of‑Experts (MoE)**, активируя лишь 17 B параметров на токен; это даёт качество уровня GPT‑4o с стоимостью inference, сопоставимой с Llama 3.1 70B.  

---

## 2. Модельный ряд  

| Модель | Активных параметров / всего | Экспертов | Контекст | Мультимодальность | Статус |  
|-------|-----------------------------|-----------|----------|-------------------|--------|  
| **Scout 17B‑16E** | 17 B / 109 B | 16 | **10 М** ток. (теорет.)  | Текст + изобр. | В открытом доступе |  
| **Maverick 17B‑128E** | 17 B / 400 B | 128 | **1 М** ток.  | Текст + изобр. | В открытом доступе |  
| **Behemoth (preview)** | 288 B / 2 T | 16 | TBD | Полная мультимодальность (аудио/видео) | В тренинге  |  

> *«Активных» = параметры, задействованные при одном проходе; остальные «спящие» эксперты хранят знания для другого контекста.*  

---

## 3. Архитектура  

### 3.1 Mixture‑of‑Experts  
- Каждый Transformer‑блок содержит $E$ экспертов‑FFN, гейт выбирает топ‑$k=2$ для каждого токена.  
- В Scout $E=16$, в Maverick $E=128$ (бóльшая «глубина» знаний).  
- Активируются лишь 10–20 % параметров → linearly scalable compute и up to 3× throughput на Blackwell B200 GPU.  

### 3.2 Длинный контекст  
- **Sparse Attention + continuation caching** позволяют масштабировать RoPE до 10 М токенов без «обвала» внимания citeturn2search0turn2search4.  
- Needle‑in‑Haystack тесты показывают ~88 % top‑k accuracy на 8 М токенов (Scout).  

### 3.3 Прочие изменения  
| Компонент | Llama 3.1 | **Llama 4** | Польза |  
|-----------|-----------|-------------|--------|  
| Attention | GQA | **Multi‑Head + Sparse Routing** | экономия памяти ∝ context |  
| FFN | Dense SwiGLU | **MoE SwiGLU** | +качество при тех же FLOP |  
| Pos. Encoding | RoPE+NTK | **RoPE 2× range + cache** | 10 М контекст |  
| Quant | FP8 | **INT4/FP6** ready | 1× H100 для Scout |  

---

## 4. Обучение  

| Этап | Токены | Комментарии |  
|------|--------|-------------|  
| Pre‑train (Scout) | 40 Т+ публичных и лицензированных данных | Early‑fusion multimodal (текст+изобр.) |  
| Pre‑train (Maverick) | 22 Т (co‑distill from Behemoth) | Сокращённый цикл за счёт teacher‑student |  
| SFT | 3 М человеческих и синтет. инструкций | focus: tool‑use, vision‑QA, reasoning |  
| Alignment | DPO + Safety‑RL (Llama Guard 4) | снижена токсичность на 35 % vs 3.1 |  

---

## 5. Результаты бенчмарков (из блога Meta)  

| Тест | Scout Instruct | Maverick Instruct | GPT‑4o (ref) | Claude 3.7 S. |  
|------|---------------|-------------------|--------------|---------------|  
| MMLU | **75 %**  | 88 %  | 90 % | 89 % |  
| GPQA | 73 % | **83 %** | 82 % | 80 % |  
| MMMU (vision) | **48 %** | 58 % | 57 % | 56 % |  
| ImageNet Zero‑shot | 76 % | 80 % | 83 % | 82 % |  

*Meta утверждает, что Scout уже превосходит Gemini 2 Pro в код‑тестах и reasoning.*  

---

## 6. Экосистема и доступность  
- **Hugging Face, Cloudflare Workers AI, OCI, Together AI** выложили веса Scout/Maverick в день релиза.  
- **NVIDIA TensorRT‑LLM** даёт 40 K tps на B200 для Scout, 30 K tps — Maverick.  
- Integrations: Databricks, Snowflake, Groq, Perplexity AI добавили Llama 4 в поисковые агенты.  
- Лицензия Llama 4 сохраняет open‑weight философию, но EU‑юзерам требуется отдельное соглашение.  

---

## 7. Ограничения и дискуссия  
1. **10 М контекст «виртуален»** — качество падает после ~256 K без fine‑tune, предупреждают исследователи.  
2. **Лицензия** вводит EU‑исключение и коммерческий порог >700 M MAU, что вызвало споры об «open‑source» статусе.  
3. Мультимодальная генерация (изобр./аудио) пока output‑only; видео‑вывод заявлен в Behemoth.  
4. Safety research продолжается: возможность «jailbreak» через скрытую маршрутизацию экспертов.  

---

## 8. Что это даёт разработчикам  
- **Ультрадлинный контекст** может заменить сложный RAG в ряде сценариев, но требует балансировки cost/latency.  
- **MoE** = frontier‑качество на потребительских GPU (Scout INT4 ≈24 GB).  
- Нативная мультимодальность открывает zero‑shot Vision‑QA и document OCR без спец‑моделей.  
- Совместима с **Tool‑use API** (в черновике), поэтому «агенты» могут комбинировать текст, зрение и действия.  