# Обзор функций активации в трансформерах и LLM: Аналитический подход

Функции активации формируют основу нелинейного моделирования в архитектурах глубокого обучения. Их выбор критически влияет на обучаемость, устойчивость градиентов и аппроксимирующую способность моделей. Ниже представлен детальный анализ функций активации, преимущественно используемых в языковых моделях большого масштаба (LLM), с упором на их математические свойства и архитектурные применения.

---

## 1. Пороговая функция (Step / Threshold)

**Формула:**

$$
f(x) = \begin{cases}1 & x > 0 \\ 0 & x \leq 0\end{cases}
$$

**Исторический контекст:** Базовая нелинейность перцептронов первого поколения. Не является гладкой и не допускает обучения через обратное распространение ошибки.

---

## 2. Сигмоида

**Формула:**

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**Достоинства:** Дифференцируемость, интерпретируемость как вероятность. **Недостатки:** Необратимая насыщаемость при больших |x|, смещение среднего активационного значения, высокая чувствительность к инициализации весов.

```python
sigmoid = lambda x: 1 / (1 + torch.exp(-x))
```

---

## 3. Гиперболический тангенс (tanh)

**Формула:**

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**Характеристики:** Центрированное распределение выходов, что способствует более эффективному градиентному потоку. Однако затухание градиентов сохраняется при |x| > 2.

```python
tanh = torch.tanh
```

---

## 4. ReLU (Rectified Linear Unit)

**Формула:**

$$
\text{ReLU}(x) = \max(0, x)
$$

**Преимущества:** Простота, вычислительная эффективность, отсутствие насыщения в положительной полуплоскости. **Проблемы:** Обнуление градиентов на отрицательной области ("dying neurons"), при котором нейроны с отрицательным входом перестают обновлять веса из-за нулевого градиента. Это может привести к снижению выразительной способности модели и замедлению сходимости, особенно при неудачной инициализации или высоких темпах обучения.

```python
relu = torch.nn.ReLU()
```

---

## 5. Leaky ReLU / PReLU

**Формула:**

$$
\text{LeakyReLU}(x) = \begin{cases}x & x > 0 \\ \alpha x & x \leq 0\end{cases}
$$

**Модификации ReLU:** Сохраняют градиенты в отрицательной области. В PReLU параметр \(\alpha\) обучается, обеспечивая адаптивность.

```python
leaky_relu = torch.nn.LeakyReLU(negative_slope=0.01)
prelu = torch.nn.PReLU()
```

---

## 6. ELU (Exponential Linear Unit)

**Формула:**

$$
\text{ELU}(x) = \begin{cases}x & x > 0 \\ \alpha (e^x - 1) & x \leq 0\end{cases}
$$

**Контекст:** Обеспечивает гладкий переход в нуле, стабилизирует распределения активаций и ускоряет обучение глубоких сетей.

```python
elu = torch.nn.ELU(alpha=1.0)
```

---

## 7. Softplus

**Формула:**

$$
\text{Softplus}(x) = \ln(1 + e^x)
$$

**Интерпретация:** Гладкая аппроксимация ReLU, дифференцируема на всей числовой прямой. Используется в задачах, требующих строго положительных выходов.

```python
softplus = torch.nn.Softplus()
```

---

## 8. GELU (Gaussian Error Linear Unit)

**Формула:**

$$
\text{GELU}(x) = x \cdot \Phi(x), \quad \Phi(x) = \text{CDF стандартного нормального распределения}
$$

**Приближение:**

$$
\text{GELU}(x) \approx 0.5x(1 + \tanh(\sqrt{2/\pi}(x + 0.044715x^3)))
$$

**Архитектурное применение:** Стандарт в моделях BERT, GPT и LLaMA. Интерпретируется как стохастическое включение нейрона с вероятностью \$\Phi(x)\$. Обеспечивает улучшенную регуляризацию.

```python
gelu = torch.nn.GELU()
```

---

## 9. SiLU / Swish

**Формула:**

$$
\text{SiLU}(x) = x \cdot \sigma(x)
$$

**Свойства:** Гладкая альтернатива ReLU, не имеет насыщения и демонстрирует улучшенную производительность на задачах NLP и CV. **Применение:** Используется в модели EfficientNet и некоторых трансформерах.

```python
silu = torch.nn.SiLU()
```

---

## 10. Mish

**Формула:**

$$
\text{Mish}(x) = x \cdot \tanh(\ln(1 + e^x))
$$

**Особенности:** Саморегуляризующаяся активация. Предлагает баланс между гладкостью и экспрессивностью, но требует больше вычислений.

```python
def mish(x):
    return x * torch.tanh(torch.nn.functional.softplus(x))
```

---

## 11. Gated Linear Units (GLU) и вариации: SwiGLU, GEGLU, ReGLU

**Формула (SwiGLU):**

$$
\text{SwiGLU}(x_1, x_2) = \text{SiLU}(x_1) \cdot x_2
$$

**Применение:** Являются структурной основой FFN-блоков в современных трансформерах, включая PaLM, LLaMA, и Mixtral. Улучшают селективность активаций и представляют собой пример увеличения выразительной мощности через модульность (гейтинг).

```python
def swiglu(x1, x2):
    return torch.nn.functional.silu(x1) * x2
```

---

## 12. RAF (Rational Activation Function)

**Формула:**

$$
\text{RAF}(x) = \frac{\sum a_i x^i}{1 + \sum b_j x^j}
$$

**Описание:** Активация, параметризуемая рациональными функциями. Потенциально универсальна в аппроксимации и может адаптироваться к специфике задачи в процессе обучения. Находится в фазе теоретического и эмпирического изучения.

---

## Выводы и перспективы

- В трансформерах доминируют гладкие функции: **GELU**, **SiLU**, **SwiGLU** благодаря их стабильности, эффективности и способности к аппроксимации вероятностных включений.
- **GLU-стратегии** сочетаются с архитектурным паттерном Mixture-of-Experts, усиливая модульность и управляемость потока информации.
- **RAF** и **Mish** — перспективные направления исследований, потенциально предлагающие адаптивные, контекстно-зависимые активации.
- Классические активации остаются релевантными в задачах с ограниченными ресурсами или в качестве вспомогательных механизмов.

