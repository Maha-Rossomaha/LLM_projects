# Конспект статьи «Papers Explained 64: Mistral 7B» (DAIR.AI)

URL:  
🔗 [Mistral 7B](https://medium.com/dair-ai/papers-explained-mistral-7b-b9632dedf580)

## 1. Ключевые выводы

- **Mistral 7B** — открытая LLM на 7,3 млрд параметров, демонстрирующая производительность на уровне или выше моделей Llama 2 13B и Llama 1 34B при существенно меньших вычислительных затратах.
- Два ядра эффективности: **Grouped‑Query Attention (GQA)** ускоряет декодирование, а **Sliding‑Window Attention (SWA)** масштабирует контекст, сохраняя линейную сложность и экономию памяти.
- Обучена «с нуля»; опубликована базовая версия **v0.1** (8k контекст) и варианты **v0.2** (32k контекст, без SWA) и **v0.3** (поддержка tokenizer v3, 32 768 токенов словаря, function calling).
- Fine‑tuned‑версия **Mistral 7B Instruct** лидирует среди всех 7B‑LLM на MT‑Bench и предпочтительнее Llama 2 13B Chat в человеческом сравнении.
- Распространяется под свободной лицензией **Apache 2.0**.

## 2. Цель работы

Создать компактную модель, которая:

1. **Соперничает** с более крупными LLM по качеству.
2. **Дешевле** в серверных расходах (память, latency).
3. **Гибко** дообучается и встраивается в различные пайплайны.

## 3. Архитектура

Mistral 7B пересматривает классический Transformer, чтобы снять два ключевых барьера практического применения LLM: рост \$O(n^2)\$ в self‑attention и линейный рост KV‑кеша при автодекодировании. Команда использует четыре взаимодополняющих техники.

### 3.1 Grouped‑Query Attention (GQA)

- **Идея.** Вычислять ключи/значения (\$K,V\$) не для каждой из \$H\_q\$ query‑голов, а лишь для меньшего числа \$H\_{kv}\$ и делиться ими внутри группы из \$g=H\_q / H\_{kv}\$ queries.
- **Форма тензоров.** \$Q\in\mathbb R^{B\times L\times H\_q\times d}\$, \$K, V\in\mathbb R^{B\times L\times H\_{kv}\times d}\$. Для Mistral: \$H\_q=32\$, \$H\_{kv}=8\$, \$g=4\$.
- **Сложность.** Память и вычисления на этапе «decode‑step» уменьшаются почти в \$g\$ раз, поскольку хранится и умножается меньше KV‑пар.
- **Эмпирика.** По данным авторов, GQA даёт \$\approx\$4× экономию VRAM и до 1.4× ускорение при равном качестве на MT‑Bench.

### 3.2 Sliding‑Window Attention (SWA)

- **Окно \$W\$.** Каждый слой обрабатывает только последние \$W\$ токенов (в Mistral \$W=4096\$).
- **Передача информации.** Стек из \$K\$ слоёв «скользит» поверх входа: токен, выпавший из окна слоя \$\ell\$, всё ещё может взаимодействовать с текущими токенами через слой \$\ell+1\$. Тем самым эффективное «покрытие» равно \$W\cdot K\$ без квадратичных затрат.
- **Практика.** Контекст свыше 128 k достигается при 16 k токенах фактической последовательности, сохраняя время инференса почти линейным от её длины.
- **Сравнение.** В SWA нет итераций RoPE‑рескейла как в LongRoPE и Flash‑Attention‑2 не требует доп. патчей — окно реализовано как простое маскирование.

### 3.3 Rolling Buffer KV‑Cache

- **Проблема.** При генерации длиной \$n\$ традиционный KV‑кеш растёт \$O(n)\$; на 32k токенах это сотни МБ на каждый GPU.
- **Решение.** Кольцевой буфер фиксированного размера \$W\$ — старые KV перезаписываются по модулю \$W\$, а сдвиги компенсируются индексом в маске внимания.
- **Итог.** Экономия VRAM \$\approx8\times\$ при 32k последовательности; скорость не проседает, т.к. обращение идёт по непрерывной памяти.

### 3.4 Prefill & Chunking

- **Prefill.** На этапе «prompt prefill» (teacher forcing) длинный запрос всё равно потребовал бы \$O(n^2)\$ матриц.
- **Chunking.** Разбиваем prompt на чанки длиной \$W\$ и загружаем их поочерёдно, реиспользуя буфер KV.
- **Ускорение.** Для 32k prompt предзаполнение сокращается с \~5 с до <=2 с на A100 80GB.

## 4. Данные и обучение

> Авторы не раскрывают полный состав датасета, но подчёркивают, что использованы только публичные корпуса. Модель обучалась «с нуля» без дообучения на проприетарных данных.

- Аппаратное ускорение: *FlashAttention v2* и *xFormers*.
- Стратегия *checkpoint‑average* для сглаживания колебаний качества.
