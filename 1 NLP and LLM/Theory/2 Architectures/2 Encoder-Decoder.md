# Архитектурные шаблоны трансформеров: Encoder‑only, Decoder‑only и Encoder‑Decoder

---

## 1. Encoder‑only

**Описание:**  
Состоит только из энкодерных слоёв — многоголовое self-attention bidirectional, без маски. Пример: BERT.

**Обучение:**  
- Маскировка случайных токенов (Masked LM):  
$$
\mathcal{L}_{MLM} = -\sum_{t \in M} \log P(x_t | x_{\setminus M})
$$  
- Возможная Next Sentence Prediction.

**Плюсы:**  
- Глубокое двунаправленное представление.  
- Высокая эффективность на GLUE/SuperGLUE: меньше параметров при сопоставимой NLU-качестве.

**Минусы:**  
- Не предназначено для автогенерации текста.  
- Стек используется только для представлений → требует отдельный декодер для генеративных задач.

---

## 2. Decoder‑only

**Описание:**  
Только causal self‑attention: каждый токен зависит от предшествующих. Примеры: GPT‑серия, LLaMA.

**Обучение:**  
- Causal LM:  
$$
\mathcal{L}_{CLM} = -\sum_{t} \log P(x_t \mid x_{<t})
$$

**Плюсы:**  
- Естественная генерация текста, простота инференса.  
- Отличная zero‑/few‑shot генерация и generalization.

**Минусы:**  
- Обучение последовательное → менее эффективно.  
- Для NLU- и трансфервых задач требует много параметров.

---

## 3. Encoder‑Decoder (Seq2Seq)

**Описание:**  
Классическая seq2seq: энкодер + декодер с cross-attention. Примеры: T5, BART.

**Обучение:**  
- Маскирование входа и цель на output; может быть span-masked.  
$$
\mathcal{L}_{Seq2Seq} = -\sum_t \log P(y_t \mid y_{<t}, EncoderOutput(x))
$$

**Плюсы:**  
- Универсальность: перевод, суммаризация, Q&A.  
- Эффективнее causal‑LM для трансформационных задач и иногда быстрее на edge (меньше latency / throughput).

**Минусы:**  
- Сложнее и ресурсозатратнее.  
- Требует разметки пар «вход→выход».

---

## 4. Формулы в резюме

- **Encoder‑only (MLM):**  
  $\mathcal{L}_{MLM} = -\sum_{t \in M} \log P(x_t \mid x_{\setminus M})$
- **Decoder‑only (CLM):**  
  $\mathcal{L}_{CLM} = -\sum_{t} \log P(x_t \mid x_{<t})$
- **Encoder‑Decoder (seq2seq):**  
  $\mathcal{L}_{Seq2Seq} = -\sum_{t} \log P(y_t \mid y_{<t}, Encoder(x))$

---

## 5. Сравнительная таблица

| Архитектура         | Контекст            | Генерация | Применение                                 | Эффективность/Сложность              |
|---------------------|---------------------|-----------|--------------------------------------------|--------------------------------------|
| **Encoder‑only**    | двунаправленный     | нет         | Понимание текста, классификация, NLU       | Параллельное обучение, NLU‑эффективно |
| **Decoder‑only**    | только прошлое      | да        | Генерация, zero-/few-shot, чаты, код       | Простое, но требует параметров       |
| **Encoder‑Decoder** | полный вход + прошлое | да       | Перевод, суммаризация, Q&A, seq2seq‑задачи | Универсально, но ресурсоемко        |

---

### 6. Как выбрать?

- **NLU только:** берём encoder-only (BERT‑подобные).
- **Только генерация:** decoder‑only (GPT‑подобные).
- **Преобразование текста:** encoder-decoder (T5/BART).
