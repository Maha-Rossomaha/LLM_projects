# Сравнение языковых моделей: Causal vs Masked vs Seq2Seq

---

## 1. Causal Language Modeling (CLM)

**Описание:**  
Автoregressive модель, обученная предсказывать следующий токен на основе предыдущих.

**Постановка задачи:**  
Для входной последовательности $x = x_1,\: \dots, x_T$:
$$
\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^{T} \log Px_t \mid x_{<t}
$$

**Механизм:**  
Self‑attention с маской causal (*только левые токены*) — модель видит только предыдущие токены, без «заглядывания вперёд».

**Плюсы:**  
- Природный способ генерации текста от начала к концу.  
- Простота реализации на GPT‑подобных моделях.

**Минусы:**  
- Использует только левый контекст → ограничено для задач, где нужен полный контекст.  
- Ошибки накапливаются через весь процесс генерации.

---

## 2. Masked Language Modeling (MLM)

**Описание:**  
Denoising-подход: случайно маскируются токены, и модель восстанавливает их, учитывая оба контекста.

**Постановка задачи:**  
С маскированием множества позиций $M$:
$$
\mathcal{L}_{\text{MLM}} = -\sum_{t \in M} \log Px_t \mid x_{\setminus M}
$$

Обычно 15% токенов маскируется, но для больших моделей могут быть выборки до 40%.

**Механизм:**  
Bidirectional attention: модель видит информацию слева и справа от маскированного слова.

**Плюсы:**  
- Богатое понимание контекста благодаря двунаправленной encoder‑архитектуре.  
- Отлично подходит для задач понимания языка и классификации.

**Минусы:**  
- Не создана для автогенной генерации текста.  
- Нужен последовательный декодер или seq2seq для генерации.

---

## 3. Sequence-to-Sequence (Seq2Seq)

**Описание:**  
Encoder‑decoder архитектура: энкодер извлекает представление входа, декодер генеративно создаёт выход.

**Постановка задачи:**  
$$
\mathcal{L}_{\text{Seq2Seq}} = -\sum_{t=1}^{T'} \log Py_t \mid y_{<t}, \text{EncoderOutput}(x)
$$

где $x$ — входной текст, а $y$ — выводимой текст.

**Механизм:**  
- Энкодер: bidirectional self‑attention.  
- Декодер: causal attention + cross‑attention к энкодеру.

**Плюсы:**  
- Combines strengths of both causal и masked.  
- Идеален для перевода, суммаризации, Q&A.

**Минусы:**  
- Сложнее и ресурсоёмко в обучении.  
- Требует меток вход–выход.

---

## 4. Табличное сравнение

| Модель       | Контекст     | Подходит для генерации | Бенефиты                      | Ограничения                        |
|--------------|--------------|-------------------------|-------------------------------|------------------------------------|
| **CLM**      | левый только | да                   | Лёгкая пред+инференс          | Нет правого контекста, накопление ошибок |
| **MLM**      | двунаправлен | нет                  | Сильное понимание контекста  | Генерация требует доп. архитектуру |
| **Seq2Seq**  | полный       | да                   | Универсальна (перевод, суммиризация) | Сложнее, требует настраиваемые данные |

---

## 5. Примеры из Hugging Face

```python
from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM

causal = AutoModelForCausalLM.from_pretrained("gpt2")
mlm = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
seq2seq = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
