# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: Causal vs Masked vs Seq2Seq

URL:  
üîó [HF LM](https://huggingface.co/course/chapter1)  
üîó [Understanding Causal LLM‚Äôs, Masked LLM‚Äôs, and Seq2Seq](https://medium.com/%40tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)  

## 1. Causal Language Modeling (CLM)

**–û–ø–∏—Å–∞–Ω–∏–µ:**  
–ê–≤—Ço—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö.

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏:**  
–î–ª—è –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ $x = x_1,\: \dots, x_T$:
$$
\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^{T} \log Px_t \mid x_{<t}
$$

**–ú–µ—Ö–∞–Ω–∏–∑–º:**  
Self‚Äëattention —Å –º–∞—Å–∫–æ–π causal (*—Ç–æ–ª—å–∫–æ –ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã*) ‚Äî –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã, –±–µ–∑ ¬´–∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤–ø–µ—Ä—ë–¥¬ª.

**–ü–ª—é—Å—ã:**  
- –ü—Ä–∏—Ä–æ–¥–Ω—ã–π —Å–ø–æ—Å–æ–± –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –Ω–∞—á–∞–ª–∞ –∫ –∫–æ–Ω—Ü—É.  
- –ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ GPT‚Äë–ø–æ–¥–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö.

**–ú–∏–Ω—É—Å—ã:**  
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.  
- –û—à–∏–±–∫–∏ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

---

## 2. Masked Language Modeling (MLM)

**–û–ø–∏—Å–∞–Ω–∏–µ:**  
Denoising-–ø–æ–¥—Ö–æ–¥: —Å–ª—É—á–∞–π–Ω–æ –º–∞—Å–∫–∏—Ä—É—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã, –∏ –º–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Ö, —É—á–∏—Ç—ã–≤–∞—è –æ–±–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏:**  
–° –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø–æ–∑–∏—Ü–∏–π $M$:
$$
\mathcal{L}_{\text{MLM}} = -\sum_{t \in M} \log Px_t \mid x_{\setminus M}
$$

–û–±—ã—á–Ω–æ 15% —Ç–æ–∫–µ–Ω–æ–≤ –º–∞—Å–∫–∏—Ä—É–µ—Ç—Å—è, –Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –º–æ–≥—É—Ç –±—ã—Ç—å –≤—ã–±–æ—Ä–∫–∏ –¥–æ 40%.

**–ú–µ—Ö–∞–Ω–∏–∑–º:**  
Bidirectional attention: –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞ –æ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞.

**–ü–ª—é—Å—ã:**  
- –ë–æ–≥–∞—Ç–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π encoder‚Äë–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ.  
- –û—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

**–ú–∏–Ω—É—Å—ã:**  
- –ù–µ —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–≥–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.  
- –ù—É–∂–µ–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –∏–ª–∏ seq2seq –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

---

## 3. Sequence-to-Sequence (Seq2Seq)

**–û–ø–∏—Å–∞–Ω–∏–µ:**  
Encoder‚Äëdecoder –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: —ç–Ω–∫–æ–¥–µ—Ä –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–∞, –¥–µ–∫–æ–¥–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–æ–∑–¥–∞—ë—Ç –≤—ã—Ö–æ–¥.

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏:**  
$$
\mathcal{L}_{\text{Seq2Seq}} = -\sum_{t=1}^{T'} \log Py_t \mid y_{<t}, \text{EncoderOutput}(x)
$$

–≥–¥–µ $x$ ‚Äî –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç, –∞ $y$ ‚Äî –≤—ã–≤–æ–¥–∏–º–æ–π —Ç–µ–∫—Å—Ç.

**–ú–µ—Ö–∞–Ω–∏–∑–º:**  
- –≠–Ω–∫–æ–¥–µ—Ä: bidirectional self‚Äëattention.  
- –î–µ–∫–æ–¥–µ—Ä: causal attention + cross‚Äëattention –∫ —ç–Ω–∫–æ–¥–µ—Ä—É.

**–ü–ª—é—Å—ã:**  
- –°–æ—á–µ—Ç–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ causal –∏ masked.  
- –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏, Q&A.

**–ú–∏–Ω—É—Å—ã:**  
- –°–ª–æ–∂–Ω–µ–µ –∏ —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–æ –≤ –æ–±—É—á–µ–Ω–∏–∏.  
- –¢—Ä–µ–±—É–µ—Ç –º–µ—Ç–æ–∫ –≤—Ö–æ–¥‚Äì–≤—ã—Ö–æ–¥.

---

## 4. –¢–∞–±–ª–∏—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

| –ú–æ–¥–µ–ª—å       | –ö–æ–Ω—Ç–µ–∫—Å—Ç     | –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ | –ë–µ–Ω–µ—Ñ–∏—Ç—ã                      | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è                        |
|--------------|--------------|-------------------------|-------------------------------|------------------------------------|
| **CLM**      | –ª–µ–≤—ã–π —Ç–æ–ª—å–∫–æ | –¥–∞                   | –õ—ë–≥–∫–∞—è –ø—Ä–µ–¥+–∏–Ω—Ñ–µ—Ä–µ–Ω—Å          | –ù–µ—Ç –ø—Ä–∞–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ |
| **MLM**      | –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω | –Ω–µ—Ç                  | –°–∏–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞  | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø. –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É |
| **Seq2Seq**  | –ø–æ–ª–Ω—ã–π       | –¥–∞                   | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞ (–ø–µ—Ä–µ–≤–æ–¥, —Å—É–º–º–∏—Ä–∏–∑–∞—Ü–∏—è) | –°–ª–æ–∂–Ω–µ–µ, —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ |

---

## 5. –ü—Ä–∏–º–µ—Ä—ã –∏–∑ Hugging Face

```python
from transformers import AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM

causal = AutoModelForCausalLM.from_pretrained("gpt2")
mlm = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
seq2seq = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
