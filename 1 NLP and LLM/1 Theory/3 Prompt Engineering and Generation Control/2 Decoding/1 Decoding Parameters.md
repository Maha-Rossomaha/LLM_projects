# Параметры декодирования в генерации текста (LLM)

## Общее

Параметры декодирования управляют тем, **как LLM выбирает следующий токен** при генерации. Это определяет баланс между **детерминизмом, разнообразием, связностью и длиной**.

Параметры делятся на 3 группы:

- **Стохастические (определяют форму распределения вероятностей)**
- **Повторяемость и разнообразие**
- **Стратегии останова**

---

## 1. Стохастические параметры

### temperature

**Определение:** сглаживает/усиливает вероятностное распределение логитов.

- $p_i \propto \exp(\frac{\text{logit}_i}{T})$
- $T=1$ — без изменений, $T < 1$ — пик более выражен (модель становится более уверенной), $T > 1$ — распределение становится более плоским

**Рекомендации:**

- $T=0.7$ — часто используется по умолчанию
- $T=0$ — делает выбор детерминированным (всегда выбирается самый вероятный токен)

### top_p (nucleus sampling)

**Определение:** выбирается минимальное множество токенов с суммарной вероятностью ≥ *p*, затем выбор случайный из них.

- Пример: при `top_p = 0.9`, отбрасываются самые маловероятные токены, если они не входят в 90% массы вероятности

**Рекомендации:**

- `top_p = 0.9` — баланс разнообразия и качества
- `top_p = 1.0` — выбор из полного распределения (по сути — отключен)

### top_k

**Определение:** обрезает распределение, оставляя только *k* наиболее вероятных токенов. Выбор происходит только из них.

- Пример: `top_k = 40` — только 40 токенов-кандидатов

**Важно:** top_p и top_k можно комбинировать. Используется либо один, либо оба (приоритет — самый строгий фильтр).

---

## 2. Повторяемость и разнообразие

### repetition_penalty

**Определение:** штрафует логиты токенов, которые уже были в сгенерированном тексте.

- $\text{logit}_i \leftarrow \text{logit}_i / \text{penalty}$, если токен уже встречался

**Рекомендации:**

- `repetition_penalty = 1.0` — без штрафа
- `1.1 – 1.3` — мягкое снижение повторов

### no_repeat_ngram_size

**Определение:** запрещает повторение n-грамм заданной длины.

- Пример: `no_repeat_ngram_size = 3` — фраза “поэтому он сказал, что” больше не появится повторно

**Рекомендации:**

- `3 – 5` — чаще всего
- Полезен при генерации длинных связных текстов

### frequency_penalty / presence_penalty (только OpenAI)

#### frequency_penalty

**Штрафует часто встречающиеся токены** — пропорционально количеству появлений.

- Подавляет дублирование слов с высокой частотой (например, «очень», «что»)

#### presence_penalty

**Штрафует просто за факт появления токена ранее** — независимо от количества.

- Способствует появлению новых слов/идей

**Рекомендации:**

- `frequency_penalty = 0.5`, `presence_penalty = 0.5` — умеренное разнообразие
- Можно отдельно настраивать: presence → для идей, frequency → для слов

---

## 3. Стратегии останова

### max_new_tokens / max_length

**Определение:** жёсткое ограничение на длину генерации.

- `max_new_tokens` — количество новых токенов поверх входа
- `max_length` — общее количество (включая prompt)

**Рекомендации:**

- Явно задавать `max_new_tokens`, чтобы избежать чрезмерного расхода токенов

### stop_sequences

**Определение:** список токенов или строк, при встрече которых генерация останавливается.

- Пример: `["\nHuman:", "\nUser:"]` — остановка, если начинается новый диалоговый блок

**Важно:** работает в string-space, чувствительно к токенизации

### eos_token_id

**Определение:** токен, сигнализирующий о конце текста. Часто присутствует в обучении.

- Некоторые модели (GPT) обучены завершать ответ этим токеном
- Может быть задан вручную для fine-tuned моделей

---

## Рекомендации по настройке

- Для **максимального качества**: `temperature=0.7`, `top_p=0.9`, `repetition_penalty=1.1`, `no_repeat_ngram_size=3`
- Для **детерминированного ответа**: `temperature=0`, `top_k=1`, без штрафов
- Для **творческой генерации**: `temperature=1.0`, `top_p=0.95`, `presence_penalty=0.6`
