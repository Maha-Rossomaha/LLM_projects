# Chain-of-Thought (CoT) prompting

## Мотивация

Большие языковые модели часто дают неверный ответ на логические задачи, если их просят сразу выдать финальный результат. Это связано с отсутствием способности к многопроходному рассуждению.

**Chain-of-Thought (CoT)** – техника prompting-а, в которой модель сначала проговаривает промежуточные шаги рассуждения перед выдачей ответа. Это улучшает:

- логическое следование,
- арифметику,
- интерпретируемость ответов.

## Пример

**Без CoT:**

> В коробке 3 красных и 5 синих шаров. Сколько всего шаров?\
> Ответ: 5

**С CoT:**

> В коробке 3 красных и 5 синих шаров. Сколько всего шаров?\
> Сначала сложим количество красных и синих шаров: 3 + 5 = 8.\
> Ответ: 8

## Формулировка

Моделью генерируется цепочка промежуточных шагов рассуждений:

$$
\text{CoT}(x) = (s_1, s_2, ..., s_T, y),
$$

где $s_t$ — шаг рассуждения, $y$ — финальный ответ.

Общая схема prompting:

> **Вопрос:** ...\
> **Рассуждение:** ...\
> **Ответ:** ...

## Подходы к CoT prompting

### 1. Manual CoT (ручной prompting)

Ты сам пишешь несколько примеров, в которых есть логическая цепочка.

> Вопрос: У Миши 2 яблока, он купил ещё 3. Сколько у него стало?\
> Рассуждение: Было 2, купил 3, итого 2+3=5.\
> Ответ: 5

Используется как few-shot prompting.

### 2. Auto-CoT (автоматическая генерация цепочек)

- Модель сначала генерирует рассуждения на примерах.
- Затем используется в zero-shot стиле: "Давай подумаем шаг за шагом" (*Let's think step by step*).

### 3. Scratchpad

Для сложных задач (например, арифметика, программирование) можно давать промежуточные переменные, вычисления, подзадачи:

> Вопрос: Что больше: 25 × 12 или 30 × 10?\
> Вычислим: 25×12=300, 30×10=300. Оба равны.\
> Ответ: одинаково

## Почему это работает

- Модель учится распознавать шаблоны рассуждений.
- CoT активирует **индукционные головы** (induction heads) — механизмы attention, отслеживающие шаблон "вопрос → цепочка рассуждений → ответ".
- Делает reasoning **дискретным и пошаговым**, снижая вероятность галлюцинаций.

## Теоретическая основа

CoT можно рассматривать как разновидность **in-context learning**, где примеры включают цепочки reasoning-шагов.

Это связано с идеей "Program Induction": модель не просто предсказывает ответ, а **симулирует исполнение программы** (последовательности логических операций).

## Формальные задачи, где CoT эффективен

- Арифметика (GSM8K)
- Логика (Logical Deduction)
- Commonsense reasoning (CSQA, StrategyQA)
- Математика (MATH, ARC)
- Игры (Sudoku, Towers of Hanoi)

## Расширения

- **Self-consistency decoding**: сэмплируем несколько CoT, выбираем самый частый ответ.
- **Tree-of-Thought (ToT)**: вместо линейной цепочки строится дерево решений.
- **Least-to-Most Prompting**: разбиваем сложную задачу на подзадачи (противоположность CoT → сначала ответ, потом объяснение).

## Пример prompt-а с CoT

```text
Вопрос: У Саши 10 конфет. Он съел 4, потом купил ещё 3. Сколько конфет у него теперь?
Рассуждение: Сначала у него было 10. Он съел 4, осталось 6. Потом он купил 3, стало 9.
Ответ: 9
```

## Связь с архитектурой моделей

- CoT не требует изменения весов модели.
- Поведение модели зависит от **attention** к предыдущим токенам.
- Используется **in-context generalization**.

## Плюсы

- Повышает точность на reasoning-задачах.
- Делает ответы модели более интерпретируемыми.
- Лучше переносится на unseen задачи.

## Минусы

- Увеличивает длину prompt-а и latency.
- Не всегда помогает на простых задачах.
- Вводит шум, если рассуждения некорректны.
