# Token-Level Logit Analysis

## 1. Что это такое

**Token-level logit analysis** — метод анализа предсказаний языковой модели на уровне отдельных токенов. Он позволяет понять:

* какие токены модель считала наиболее вероятными на каждом шаге генерации,
* как распределялись вероятности между альтернативами,
* почему был выбран именно этот токен.

## 2. Как это работает

В стандартной архитектуре трансформера последний слой выдаёт **логиты** — необработанные оценки вероятностей для каждого токена словаря.

Процесс:

1. Модель обрабатывает вход и выдаёт логиты: $\text{logits} \in \mathbb{R}^{V}$, где $V$ — размер словаря.
2. Применяется softmax: $p_i = \frac{e^{\text{logits}i}}{\sum{j} e^{\text{logits}_j}}$.
3. Выбирается токен с максимальной вероятностью (или по правилам сэмплинга — temperature, top-k, top-p).

## 3. Зачем это нужно

* **Интерпретация**: понять, какие слова модель рассматривала как альтернативы.
* **Отладка**: выяснить, на каком этапе выбор "сошёл с рельс".
* **Обучение**: проверка корректности loss-функции и обучения.
* **Сравнение моделей**: как разные модели распределяют вероятности по токенам.

## 4. Пример анализа в коде

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

prompt = "The capital of France is"
inputs = tokenizer(prompt, return_tensors='pt')
outputs = model(**inputs)

# Логиты для последнего токена
last_logits = outputs.logits[0, -1, :]
probs = torch.softmax(last_logits, dim=-1)

# Топ-5 токенов
top_probs, top_indices = torch.topk(probs, 5)
for idx, prob in zip(top_indices, top_probs):
    print(tokenizer.decode([idx]), float(prob))
```

## 5. Применимость

* **RAG и QA-системы**: анализировать, откуда появляются неверные ответы.
* **Файнтюнинг**: убедиться, что модель смещает вероятности в сторону доменных терминов.
* **Детекция галлюцинаций**: при сильном рассеянии вероятностей модель может быть неуверенной.
* **Анализ temperature/top-k**: как параметры генерации влияют на выбор.

## 6. Плюсы и минусы

**Плюсы:**

* Даёт чёткое представление о выборе модели.
* Позволяет выявить неочевидные альтернативы.
* Может применяться без модификации модели.

**Минусы:**

* Не объясняет причин внутренних решений (только финальное распределение).
* При большом словаре анализ всех токенов может быть трудоёмким.
* Требует осторожности при интерпретации: высокие логиты ≠ "важность" токена в семантическом смысле.

## 7. Ограничения и предостережения

* Модель может иметь высокие логиты для токенов из-за статистики корпуса, а не контекстной релевантности.
* Логиты чувствительны к нормализации и масштабированию.
* Для длинных последовательностей изменения на ранних шагах могут накапливаться.
