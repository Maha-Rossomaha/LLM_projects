# Layer-Wise Relevance Propagation и методы атрибуции (Integrated Gradients, SHAP) для LLM

## 1. Зачем нужны методы атрибуции

Методы атрибуции помогают понять, **почему** модель сделала конкретный предсказанный выбор, распределяя «важность» между входными токенами или признаками.

В задачах с LLM это полезно для:

* Интерпретации предсказаний.
* Обнаружения смещений и некорректных зависимостей.
* Повышения доверия к модели (explainable AI).
* Отладки и выбора данных для дообучения.

---

## 2. Layer-Wise Relevance Propagation (LRP)

**Идея:** LRP проходит по слоям модели «в обратном направлении», распределяя релевантность от выхода к входу, чтобы понять вклад каждого признака.

**Как работает:**

1. Начинаем с полной релевантности на выходе (например, 1 для предсказанного класса).
2. На каждом слое релевантность распределяется обратно пропорционально весам и активациям.
3. В итоге получаем «карту релевантности» по входным токенам.

**Плюсы:**

* Показывает вклад каждого токена в итоговое решение.
* Подходит для сложных архитектур (в т.ч. трансформеров).

**Минусы:**

* Реализация для LLM сложнее, чем для CNN.
* Требует доступа к весам и активациям на каждом слое.

---

## 3. Integrated Gradients (IG)

**Идея:** Оценивает вклад токена, интегрируя градиенты предсказания по пути от «базового» ввода (обычно нулевого) до текущего.

**Алгоритм:**

1. Определить базовый ввод (baseline).
2. Линейно интерполировать между baseline и реальным вводом.
3. На каждом шаге вычислять градиенты по входным эмбеддингам.
4. Усреднить градиенты и умножить на разницу между вводом и baseline.

**Плюсы:**

* Теоретически обоснованный метод (соответствует аксиомам атрибуции).
* Подходит для любых дифференцируемых моделей.

**Минусы:**

* Медленный при большом числе шагов интеграции.
* Чувствителен к выбору baseline.

**Пример с Captum:**

```python
from captum.attr import IntegratedGradients

model.eval()
ig = IntegratedGradients(model)

attributions, delta = ig.attribute(inputs_embeds, target=pred_label, return_convergence_delta=True)
```

---

## 4. SHAP (SHapley Additive exPlanations)

**Идея:** На основе теории игр оценивает вклад каждого признака (токена), усредняя по всем комбинациям признаков.

**Особенности:**

* SHAP значения показывают, насколько добавление токена изменяет предсказание.
* Можно использовать KernelSHAP (модель-агностик) или DeepSHAP (учитывает архитектуру).

**Плюсы:**

* Интуитивно понятен (связь с кооперативными играми).
* Есть готовые реализации для глубоких моделей.

**Минусы:**

* Очень вычислительно затратен при большом числе токенов.
* Приближённые методы могут терять точность.

**Пример с Transformers + SHAP:**

```python
import shap
explainer = shap.Explainer(model, tokenizer)
shap_values = explainer("The capital of France is Paris")
shap.plots.text(shap_values)
```

---

## 5. Применимость в LLM

* **Диагностика галлюцинаций**: какие токены контекста влияли на галлюцинаторный ответ.
* **RAG**: проверка, опирается ли генерация на retrieved документы.
* **Файнтюнинг**: оценка, использует ли модель доменные термины.

---

## 6. Плюсы и минусы в контексте LLM

**Плюсы:**

* Повышают прозрачность работы модели.
* Можно выявить нежелательные зависимости.

**Минусы:**

* Высокая вычислительная стоимость.
* Ограничения по интерпретации для длинных контекстов.
* Возможны разные результаты при разных параметрах.
