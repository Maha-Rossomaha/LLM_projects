# Attention Tracing и BertViz

## 1. Зачем нужен attention tracing

Attention tracing позволяет исследовать, как трансформер распределяет внимание между токенами входной последовательности. Это полезно для:

* **Интерпретации**: понять, какие слова/символы влияют на предсказание.
* **Отладки**: выявить, где модель "смотрит не туда" и допускает ошибки.
* **Анализа ошибок**: обнаружить смещения, чрезмерное внимание к незначимым токенам.
* **Обучения и исследования**: изучение поведения разных голов и слоёв.

## 2. Основы механизма self-attention

В self-attention каждый токен вычисляет веса важности по отношению к другим токенам:

* **Матрицы Q, K, V**: Query, Key и Value.
* **Attention weights**: $softmax(\frac{QK^T}{\sqrt{d\_k}})$ — распределение внимания.
* **Multi-head attention**: несколько независимых голов, каждая может учиться выделять разные зависимости.

Результат: для каждого токена получаем вектор, агрегирующий информацию от других токенов с учётом их весов.

## 3. Как выглядит attention tracing

Attention tracing обычно представляют в виде:

* **Тепловых карт**: строки — токены-запросы, столбцы — токены-ключи, цвет = вес внимания.
* **Интерактивных диаграмм**: линии, соединяющие токены в зависимости от их внимания.
* **Слоёво-головных матриц**: переключение между слоями и головами.

## 4. BertViz — инструмент для визуализации attention

**BertViz** — популярная библиотека для визуализации внимания в моделях на базе Transformers.

**Возможности:**

* Просмотр внимания по слоям и головам.
* Поддержка encoder-only, decoder-only и encoder-decoder моделей.
* Интерактивная HTML-визуализация в Jupyter Notebook.
* Сравнение внимания в разных токенах.

**Пример использования:**

```python
from transformers import BertTokenizer, BertModel
from bertviz import head_view

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)

sentence = "The cat sat on the mat"
inputs = tokenizer(sentence, return_tensors='pt')
outputs = model(**inputs)
attention = outputs.attentions  # список тензоров [n_layers, batch, n_heads, seq_len, seq_len]

head_view(attention, tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))
```

## 5. Практические кейсы

* **Диагностика смещений**: например, если модель при классификации отзывов уделяет внимание только эмоциональным словам, игнорируя контекст.
* **Отладка RAG**: проверка, использует ли генератор релевантные части документа.
* **Сравнение fine-tuned и base моделей**: изменения в паттернах внимания.

## 6. Ограничения и предостережения

* Высокое внимание не всегда означает высокую важность признака.
* Модели могут кодировать важность в других внутренних представлениях, а не только в весах внимания.
* Визуализация требует аккуратной интерпретации, особенно в больших моделях.
