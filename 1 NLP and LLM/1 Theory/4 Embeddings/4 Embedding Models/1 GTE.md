# General Text Embedding (GTE)

## Общее описание

**GTE (General Text Embedding)** — это серия универсальных моделей для генерации эмбеддингов текстов. Модель обучена с использованием большого корпуса парных текстов, включая переводные, парафразные и семантически связанные предложения.

Цель GTE — обеспечить **высококачественные универсальные эмбеддинги**, применимые в retrieval, классификации, кластеризации и других downstream-задачах.

---

## Архитектура и особенности

* Используется **BERT-like encoder** — чаще всего BERT-base или RoBERTa-base, который обеспечивает хорошую сбалансированность между скоростью и качеством. Архитектура типична для bi-encoder систем: отдельный encoder обрабатывает входной текст, не взаимодействуя с другими примерами в батче.

* **Mean pooling**: после получения скрытых представлений токенов (hidden states), финальный эмбеддинг получается усреднением по всем токенам. Это даёт устойчивое и компактное представление предложения, в отличие от \[CLS]-токена, который может быть менее надёжен без дополнительного fine-tuning.

* **MSE Loss между нормализованными векторами**: вместо softmax (как в InfoNCE), GTE минимизирует среднеквадратичное отклонение между L2-нормированными эмбеддингами положительных пар. Это позволяет модели оперировать в более стабильной embedding-геометрии и быть совместимой с cosine similarity как основной метрикой.

* Обучение с **in-batch negatives**: для каждого запроса остальные примеры в батче считаются негативами. Такой подход даёт богатый градиентный сигнал без необходимости внешнего семплирования негативов, особенно при большом batch size.

* Модель изначально **оптимизирована под cosine similarity** — все эмбеддинги L2-нормируются, и все задачи валидации строятся с cosine-метрикой, что позволяет легко использовать модель в retrieval, clustering и zero-shot matching.

---

## Почему GTE хорошо работает

* Поддерживает **мультиязычные сценарии** (особенно китайский и английский)
* Отличный **zero-shot performance** в retrieval-задачах без дообучения
* Высокая совместимость с **Open-source retrieval toolkits** (например, FAISS, BEIR, FlagEmbedding)
* Поддержка **quantization, ONNX, FastEmbedding** для продакшена

---

## Применение

* **Retrieval:** dense search по embedding'ам запросов и документов
* **Semantic matching:** paraphrase detection, duplicate finding
* **Clustering:** тематическая группировка документов
* **Re-ranking:** как слой после BM25 (двухэтапный поиск)
* **LLM integration:** быстрый отбор кандидатов до генерации

---

## Сравнение с аналогами

| Модель    | Ядро         | Pretraining              | Особенности                    |
| --------- | ------------ | ------------------------ | ------------------------------ |
| GTE-small | BERT         | Pairwise, MSE loss       | Быстро и компактно             |
| GTE-base  | RoBERTa      | Large pair corpus        | Balanced accuracy/speed        |
| E5-base   | Encoder-only | Text2Text task-formatted | Более general-purpose          |
| BGE-base  | BERT         | Supervised (multitask)   | Хорошо работает под re-ranking |
