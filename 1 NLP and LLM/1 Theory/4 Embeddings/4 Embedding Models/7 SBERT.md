# Sentence-BERT (SBERT) 

## Общее описание

**SBERT (Sentence-BERT)** — это модификация архитектуры BERT, разработанная для эффективного получения семантических эмбеддингов предложений и текстов. В отличие от стандартного BERT, SBERT оптимизирован для задач сравнения текстов и поиска, позволяя генерировать фиксированные векторы, пригодные для использования с cosine similarity или другими метриками расстояния.

---

## Архитектура и особенности

* **Base-архитектура**: модифицированный BERT или RoBERTa, адаптированный для работы в конфигурациях *siamese* и *triplet*. В таких схемах один и тот же encoder используется для кодирования разных входов (запросов и документов), что гарантирует согласованное embedding-пространство.
* **Pooling**: чаще всего применяется mean pooling по всем токенам, что даёт устойчивое представление даже для длинных предложений; при необходимости можно использовать \[CLS]-токен или max pooling в зависимости от задачи.
* **Обучение**:

  * **Siamese network**: два (или более) идентичных энкодера с общими весами обрабатывают пару входов, а затем сравниваются их эмбеддинги.
  * **Triplet loss / Cosine similarity loss**: функции потерь подбираются под задачу — triplet loss учит модель сокращать расстояние между anchor и positive и увеличивать его с negative, а cosine similarity loss фокусируется на максимизации косинусного сходства для релевантных пар.
* Возможность дообучения на собственных парах текстов или специализированных датасетах (например, в конкретном домене или языке).
* Поддержка множества предобученных моделей, включая мультиязычные и компактные варианты для продакшена.

---

## Преимущества

* **Высокая точность** в задачах semantic textual similarity (STS), paraphrase mining, retrieval.
* **Эффективность**: генерация векторов возможна оффлайн, быстрый поиск через FAISS/Annoy.
* **Гибкость**: легко fine-tune под конкретный домен.
* **Многоязычность**: есть модели с поддержкой десятков языков (например, `distiluse-base-multilingual-cased-v1`).

---

## Применение

* **Semantic search** — dense retrieval с использованием cosine similarity.
* **Clustering** — группировка текстов по тематике.
* **Paraphrase mining** — поиск дубликатов и перефразированных предложений.
* **Reranking** — улучшение порядка результатов после первичного поиска.
* **Recommendation** — подбор похожих документов или продуктов.

---

## Варианты моделей

* `all-MiniLM-L6-v2` — компактная и быстрая модель.
* `multi-qa-MiniLM-L6-cos-v1` — оптимизирована под QA retrieval.
* `paraphrase-multilingual-MiniLM-L12-v2` — многоязычная модель для парафраз.
