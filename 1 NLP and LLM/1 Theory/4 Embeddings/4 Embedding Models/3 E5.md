# Embedding from Instructions (E5)

## Общее описание

**E5** — это серия embedding-моделей, разработанных исследователями из Microsoft Research, обученных в стиле **text-to-text** с использованием **instruction tuning**. E5 расшифровывается как "**Embedding from Encoder-Only Models using Instructions**" и предназначена для универсального использования в retrieval и semantic matching.

Главная идея — обучить энкодер, который воспринимает вход не просто как текст, а как **инструкцию**, где уточняется задача (например: "query: ...", "passage: ..."). Это позволяет использовать одну и ту же модель во множестве контекстов.

---

## Архитектура и особенности

* **Базируется на encoder-only архитектурах** (BERT, RoBERTa, DeBERTa), что обеспечивает высокую скорость инференса и совместимость с существующими NLP-пайплайнами.
* Применяется **mean pooling** поверх всех токенов, что даёт более устойчивые представления по сравнению с использованием только \[CLS]-токена, особенно при длинных или нерегулярных входах.
* **Входы промптируются** с помощью явных префиксов (например, "query: ...", "passage: ..."), чтобы модель чётко различала роль текста — запрос это или документ.
* **Обучение комбинирует supervised contrastive loss и pairwise scoring loss**: первый усиливает различимость позитивов и негативов, второй оптимизирует точную релевантность пар.
* Благодаря архитектурным решениям и разнообразию обучающих данных, модель **демонстрирует высокое качество в zero-shot retrieval**, часто без необходимости дополнительного fine-tuning.
* Все эмбеддинги **L2-нормированы** и рассчитаны на работу с **cosine similarity** как основной метрикой близости, что упрощает интеграцию в retrieval и clustering системы.

---

## Instruction-style prompting

E5 опирается на формат текстов с префиксами-инструкциями, что позволяет модели лучше различать тип входа и применяемую задачу.

Примеры:

* `query: как лечить диабет?`
* `passage: Инсулин — это гормон, который регулирует уровень сахара в крови...`

Это делает модель особенно полезной для multi-purpose retrieval-сценариев.

---

## Применение

* **Zero-shot dense retrieval**
* **Semantic similarity / matching**
* **Clustering / topic mining**
* **Semantic reranking** (до подачи в cross-encoder)
* **Multilingual search** (в версиях `e5-multilingual-*`)

---

## Варианты моделей

* `e5-small`, `e5-base`, `e5-large`
* `e5-multilingual` — для многоязычных задач
* `e5-chat`, `e5-mistral` — адаптированы под диалоговые LLM или кастомные источники
