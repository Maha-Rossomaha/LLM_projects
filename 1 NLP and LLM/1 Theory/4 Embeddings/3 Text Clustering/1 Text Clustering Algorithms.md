# Text Clustering Algorithms
## Проблематика и мотивация кластеризации

Кластеризация текстов необходима для решения двух ключевых задач:

1. **Диагностика слабых мест классификатора.**  
   Распределение вероятностей модели на обучающей и тестовой выборках помогает выявить зону ошибок — например, область низкой уверенности (в районе порога 0.5), где классы смешиваются. Кластеризация текстов в этой зоне может помочь понять, какие именно типы данных сбивают модель с толку.

2. **Группировка похожих текстов в real-time.**  
   В продакшн-сценариях часто требуется находить схожие тексты "на лету" (например, для рекомендаций, модерации или автоподсказок). Это требует кластеризации в потоковом режиме — быстрых эмбеддингов и методов, устойчивых к обновлению кластера без переобучения (например, online KMeans или HNSW).

## Алгоритмы кластеризации: краткий обзор

**K-Means**  
- Делит данные на `k` кластеров, минимизируя сумму квадратов расстояний до центров.
- Алгоритм: случайный выбор центров → назначение точек ближайшему центру → пересчёт центров → повтор до сходимости.
- Требует заранее знать `k`, чувствителен к выбросам и форме кластеров.

**DBSCAN**  
- Группирует точки в плотные области на основе двух параметров: `eps` (радиус) и `min_samples` (порог плотности).
- Точки делятся на: ядро (достаточно соседей), граничные (рядом с ядром), шум.
- Не требует задания числа кластеров, устойчив к выбросам, подходит для кластеров произвольной формы.

### HDBSCAN

**HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)** — один из самых популярных алгоритмов для кластеризации текстов в оффлайн-режиме, особенно когда плотность данных варьируется.

**Принцип работы:**
1. Строит граф ближайших соседей и преобразует его в минимальное остовное дерево.
2. Формирует иерархию кластеров путём последовательного удаления наименее плотных связей.
3. Из иерархии извлекает наиболее устойчивые кластеры — те, которые существуют в широком диапазоне плотностей.
4. Присваивает каждой точке вероятность принадлежности к кластеру (soft clustering).

**Преимущества:**
- Автоматически определяет количество кластеров.
- Обнаруживает кластеры произвольной формы и разной плотности.
- Не требует выбора радиуса `eps`.
- Хорошо справляется с выбросами и шумом.

**Недостатки:**
- Более ресурсоёмкий по сравнению с KMeans и DBSCAN.
- Требует подбора параметров `min_samples` и `min_cluster_size`.

## Методы для real-time кластеризации

Некоторые алгоритмы кластеризации адаптированы для работы в реальном времени или с большими потоками данных. Вот два наиболее применяемых метода:

### Mini-Batch K-Means

Упрощённый вариант K-Means, использующий малые подвыборки (mini-batches) для итеративного обновления центров кластеров.  
**Преимущества:**
- Быстрее обычного K-Means.
- Подходит для больших объёмов и потоковых данных.
- Обновление кластеров происходит инкрементально.

**Недостатки:**
- Всё ещё требует заранее заданного `k`.
- Чувствителен к выбросам и структуре данных.

### Agglomerative Clustering (иерархическая агломерация)

Bottom-up подход: каждая точка начинает как отдельный кластер, затем пары кластеров объединяются на основе расстояния.  
**Преимущества:**
- Хорошо работает для небольших батчей или подготовленных данных.
- Обнаруживает вложенную структуру (например, темы и подтемы).

**Недостатки:**
- Сложность O(n²), плохо масштабируется.
- Непригоден для полной потоковой обработки без адаптаций.

### Denoising Autoencoders (DAE)

- Позволяют обучить автоэнкодер сжатия эмбеддинга так, чтобы он мог восстанавливать исходное представление даже после искажения входа (добавления шума).
- Это помогает модели сохранять только **ключевые, устойчивые** признаки, отбрасывая шум и избыточную информацию.
- Подход неявно формирует компактное и интерпретируемое скрытое состояние.
- Поддерживает нелинейные преобразования и гибкую архитектуру, что делает его подходящим для работы со сложными текстовыми структурами.


### Truncated SVD (t-SVD)

- Используется в NLP в основном для работы с большими разреженными матрицами, такими как TF-IDF.
- Является линейным методом снижения размерности, аналогичным PCA, но работает на разреженных данных.
- Применим в случаях, когда нельзя использовать PCA из-за плотного представления.
