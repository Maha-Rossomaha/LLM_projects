# Dense Embeddings

## Определение

**Dense embeddings** — это плотные векторные представления текста, в которых каждый элемент вектора является вещественным числом. Такие векторы обучаются так, чтобы отражать **семантическое сходство** между объектами (словами, предложениями, документами): близкие по смыслу тексты проецируются в близкие точки пространства.

В отличие от sparse представлений (например, TF-IDF или BM25), где большинство значений — нули, dense embeddings содержат **компактную, непрерывную и распределённую информацию**.

---

## Где используются dense embeddings

- **Semantic Search / Dense Retrieval** — поиск по смысловой близости, а не по ключевым словам
- **Recommendation Systems** — поиск похожих пользователей, товаров, интересов
- **Clustering / Topic Mining** — группировка текстов по смыслу
- **Retrieval-Augmented Generation (RAG)** — подбор релевантных документов для генерации
- **Duplicate / paraphrase detection** — сравнение семантики фраз или документов
- **Few-shot transfer** — использование универсальных представлений в новых задачах без переобучения

---

## Как формируются dense embeddings

1. **Языковая модель** (BERT, RoBERTa, GTE, E5, BGE и др.) обрабатывает входной текст
2. Эмбеддинг формируется через **агрегацию токенов** (mean-pooling, CLS token и т.д.)
3. Результат — вектор фиксированной размерности (например, 384, 768 или 1024)

Dense embeddings могут быть:

- **Token-level** — для задач sequence labeling, attention
- **Sentence-level / document-level** — для retrieval и similarity

---

## Архитектуры

- **Bi-Encoder (Dual Encoder)** — отдельное кодирование query и документа, быстрая инференция и возможность индексировать миллионы векторов
- **Cross-Encoder** — совместное кодирование пары (query, document), высокая точность, используется для reranking
- **Asymmetric Models** — разные encoder’ы для запросов и документов (например, query-focused + doc-focused)

---

## Примеры моделей

| Модель     | Тип                         | Особенности                                         |
| ---------- | --------------------------- | --------------------------------------------------- |
| **E5**     | Instruction-tuned           | Универсальность задач, поддержка query/doc режимов  |
| **BGE**    | Instruction-tuned           | Высокие результаты на MTEB, компактные варианты     |
| **GTE**    | Universal encoder           | Простой, стабильный, ориентирован на production use |
| **MiniLM** | Lightweight encoder         | Подходит для edge-инференса, быстрая генерация      |
| **GTR**    | Google Text Representations | Применим в retrieval и QA                           |

---

## Dense vs Sparse embeddings

| Характеристика            | Dense                       | Sparse (TF-IDF, BM25)         |
| ------------------------- | --------------------------- | ----------------------------- |
| Представление             | Вещественный вектор         | Огромный разреженный вектор   |
| Размерность               | 256–1024                    | До сотен тысяч (словарь)      |
| Семантика                 | Учитывает значение          | Поверхностное совпадение слов |
| Эффективность поиска      | Быстрый ANN                 | Высокая точность по словам    |
| Обучение                  | Требует предобучения модели | Без обучения                  |
| Устойчивость к перефразам | Высокая                     | Низкая                        |

---

## Достоинства

- Семантическая близость отражается в расстояниях (cosine, dot product)
- Универсальность: один encoder работает на разных задачах
- Высокая эффективность при использовании ANN-индексов (FAISS, ScaNN, HNSW)
- Устойчивость к переформулировкам, синонимам и шуму
- Возможность сжатия и квантования (int8, PQ, OPQ)

---

## Ограничения

- Требуют вычислительных ресурсов при обучении и инференсе
- Потенциальные потери точности по сравнению с hybrid или lexical search в узких доменах
- Ограниченная интерпретируемость
- Зависимость от качества предобучения и формулировок

---

## Практические рекомендации

- Используйте instruction-based модели (E5, BGE), если нужны универсальные представления
- Комбинируйте dense + sparse в hybrid search (например, RRF fusion)
- Следите за нормализацией векторов (L2-norm) перед поиском
- Применяйте PQ/OPQ для масштабирования на миллиардные коллекции
- Обновляйте эмбеддинги при смене домена или модели (shadow reindex)
