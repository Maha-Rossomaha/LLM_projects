# Sparse Embeddings

## Определение

**Sparse embeddings** — это разреженные векторные представления текста, в которых большинство компонент вектора равны нулю. Каждый вектор формируется на основе **набора признаков**, где каждый признак соответствует слову, н-грамме или иной единице из словаря. Примеры: **TF-IDF**, **BM25**, **BM25+**, **SPLADE** (learned sparse), **uniCOIL**, **TILDE**.

Такие векторы хорошо отражают **лексическую форму запроса**, эффективны при малом объёме данных, интерпретируемы и не требуют обучения (в классическом варианте).

---

## Где применяются sparse embeddings

- **Lexical Search / Keyword Search** — классический полнотекстовый поиск
- **Baseline retrieval** — быстрое извлечение начального пула документов
- **Hybrid Search** — в комбинации с dense embeddings (весовая или ранговая фузия)
- **Ретроиндексация и правовая экспертиза** — где важна дословность и интерпретируемость
- **Search safety (filtering, sensitive match)** — для точных совпадений по слову

---

## Типы sparse embeddings

1. **TF-IDF** — частотный счётчик слов, взвешенный по обратной частоте в корпусе
2. **BM25** — улучшенный вариант TF-IDF, учитывающий длину документа и насыщаемость термов (term saturation)
3. **BM25+** — добавляет смещение для устранения нулевого эффекта
4. **Learned sparse** (SPLADE, uniCOIL):
   - Эмбеддинги обучаются, но остаются разреженными
   - Обеспечивают баланс между интерпретируемостью и обучаемостью

---

## Преимущества sparse embeddings

- **Интерпретируемость** — каждый индекс соответствует слову или признаку
- **Быстрота** — классические движки (Elasticsearch, Lucene) оптимизированы под sparse
- **Отсутствие зависимости от обучения** — можно применять без обучения модели
- **Точность по ключевым словам** — хорошая при match по важным термам
- **Устойчивость к доменному дрейфу** — не требуют переобучения при смене контента

---

## Ограничения

- **Семантическая близость не учитывается** — "кот" и "кошка" далеко друг от друга
- **Чувствительность к формулировкам** — per-word matching, слабая устойчивость к перефразам
- **Огромная размерность** — тысячи или миллионы признаков (словарь)
- **Низкая переносимость** — вектора зависят от языка и токенизации

---

## Сравнение с dense embeddings

| Характеристика            | Sparse                         | Dense                             |
| ------------------------- | ------------------------------ | --------------------------------- |
| Размерность               | Очень высокая (до 1 млн+)      | 256–1024                          |
| Тип значений              | Целочисленные или вещественные | Вещественные                      |
| Интерпретируемость        | Высокая                        | Низкая                            |
| Учебный процесс           | Обычно не требует обучения     | Требует предобучения              |
| Устойчивость к перефразам | Низкая                         | Высокая                           |
| Используемые движки       | Elasticsearch, Lucene          | FAISS, Qdrant, ScaNN              |
| Поддержка мультиязычия    | Ограниченная                   | Высокая (через multilingual LLMs) |

---

## Примеры технологий и инструментов

- **Elasticsearch / OpenSearch** — полнотекстовые движки с поддержкой BM25 и TF-IDF
- **Lucene** — основа многих search-систем
- **SPLADE v2** — обучаемые sparse embeddings
- **uniCOIL** — sparse + contextualized approach (MS MARCO)
- **TILDE** — обучаемые sparse модели с focus on tail recall
- **Weaviate Hybrid**, **Pinecone Hybrid Search**, **OpenSearch KNN + BM25** — гибридные фреймворки

---

## Практические рекомендации

- Для высокоточного baseline retrieval — используйте BM25 или TF-IDF
- В RAG и QA системах — комбинируйте с dense retrieval (например, через Reciprocal Rank Fusion)
- Для explainability — используйте sparse модели (особенно SPLADE)
- Для мультиязычного поиска — используйте dense-составляющую
- Для масштабных систем — храните sparse индексы отдельно от dense (разные шард-структуры)
