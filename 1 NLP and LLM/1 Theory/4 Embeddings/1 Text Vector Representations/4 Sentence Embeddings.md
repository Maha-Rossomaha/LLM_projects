# Sentence Embeddings: принципы, архитектуры и применение

## Определение

**Sentence embeddings** — это векторные представления целых предложений, абзацев или коротких текстов, сохраняющие их смысловое содержание. В отличие от токен-уровневых представлений, sentence embedding кодирует **всю семантику входного текста** в фиксированный вектор фиксированной размерности (обычно 256–1024).

Такие эмбеддинги применяются для задач семантического поиска, кластеризации, классификации, выявления перефразов, оценки схожести, дедупликации и других.

---

## Как формируются sentence embeddings

### Архитектуры

1. **Bi-Encoder (Dual Encoder)**

   - Отдельное кодирование запроса и документа (симметрично или асимметрично)
   - Поддерживает эффективный поиск через векторные БД
   - Примеры: BERT + mean pooling, Sentence-BERT (SBERT), E5, GTE

2. **Cross-Encoder**

   - Объединяет пару предложений и прогоняет через LLM
   - Более точный, но не даёт готового sentence embedding (используется для reranking)

3. **Pooling стратегии**

   - CLS token
   - Mean pooling по токенам (без padding)
   - Max pooling
   - Attention-weighted pooling

4. **Инструкционно-обученные модели (Instruction-tuned)**

   - Принимают не только текст, но и задание (instruction)
   - Пример: `“Represent the question for retrieval: …”`
   - Модели: Instructor, BGE, GTE

---

## Мотивация использования

- Представление целого текста как одного вектора позволяет:
  - Выполнять **семантическое сравнение** (semantic similarity)
  - Строить **поисковые индексы** (ANN, FAISS, Qdrant)
  - Выполнять кластеризацию, классификацию, сравнение и агрегацию текстов
  - Повышать эффективность в Zero-shot и Few-shot задачах

Sentence embeddings особенно важны в retrieval-сценариях, где нужно быстро найти близкий по смыслу документ или предложение.

---

## Области применения

- **Semantic Search / Dense Retrieval**
- **Retrieval-Augmented Generation (RAG)**
- **Кластеризация и тематическое разбиение**
- **Обнаружение дубликатов и перефразов**
- **Re-ranking кандидатов (в паре с cross-encoder)**
- **Zero-shot классификация / entailment**

---

## Примеры моделей

| Модель         | Характеристики                           | Особенности                                 |
| -------------- | ---------------------------------------- | ------------------------------------------- |
| **SBERT**      | Bi-encoder, на основе BERT               | Классическая реализация sentence embeddings |
| **E5**         | Instruction-tuned, многоцелевое обучение | Поддержка разных задач через промпты        |
| **GTE**        | Модель от Google, оптимизирована под RAG | Поддержка коротких и длинных текстов        |
| **BGE**        | Поддерживает query/doc режимы            | Высокая эффективность на MTEB               |
| **MiniLM**     | Компактная модель                        | Подходит для edge-приложений                |
| **LaBSE**      | Многоязычный sentence encoder (Google)   | Выравнивание представлений на разных языках |
| **Instructor** | Instruction-based embeddings             | Высокая адаптивность к задачам              |

---

## Плюсы

- Универсальные векторы: один encoder — множество downstream задач
- Высокая производительность в retrieval-задачах
- Возможность предобучения и дообучения на собственных данных
- Совместимость с векторными базами данных (Qdrant, FAISS, Weaviate)
- Часто стабильны даже без fine-tuning

---

## Минусы и ограничения

- Потеря информации по сравнению с токен-уровневым взаимодействием (cross-encoder)
- Выбор pooling-стратегии влияет на качество (особенно для длинных текстов)
- Чувствительны к формулировкам и длине контекста
- Ограничения по размеру входа (обычно до 512 токенов)
- При асимметричных задачах требуется отдельное обучение (query ≠ document)

---

## Практические рекомендации

- Используйте **instruction-based** модели (E5, GTE, BGE) для большей гибкости
- При длинных документах делайте **chunking + mean pooling**
- Для низколатентного поиска применяйте **bi-encoder + ANN**
- Для reranking результатов — **cross-encoder**
- Для многоязычных сценариев — **LaBSE / mBERT + pooling**

---

## Бенчмарки и ресурсы

- **MTEB (Massive Text Embedding Benchmark)** — сравнение моделей по retrieval, classification, clustering, reranking

- **Sentence-Transformers** (официальная библиотека для SBERT и др.)

---

## Заключение

Sentence embeddings являются одним из ключевых компонентов в современных системах поиска, рекомендаций и классификации. Они обеспечивают компактные, сравнимые и переносимые представления текстов, позволяя строить масштабируемые NLP-приложения. Правильный выбор модели, pooling-стратегии и архитектуры — критичен для получения качественных результатов в зависимости от задачи.
