# Contextual Embeddings

## Определение и отличие от статических эмбеддингов

**Contextual embeddings** — это векторные представления слов, фраз или предложений, зависящие от контекста, в котором они встречаются. В отличие от статических эмбеддингов (например, Word2Vec, GloVe), где каждое слово имеет одно фиксированное представление вне зависимости от употребления, контекстные эмбеддинги варьируются в зависимости от окружающего текста.

Пример различия:

- В предложении "He sat on the river bank", слово "bank" будет интерпретироваться как берег.
- В предложении "She opened a bank account", "bank" будет интерпретировано как финансовое учреждение.

Такой подход реализован впервые в моделях типа **ELMo** и стал стандартом после появления **BERT**, на базе которого построено большинство современных языковых моделей.

---

## Механизм формирования

Contextual embeddings формируются внутри трансформерных архитектур (BERT, RoBERTa, LLaMA и др.) за счёт self-attention-механизма, при котором представление каждого токена зависит от всех остальных токенов входной последовательности.

Типы эмбеддингов:

- **Token-level** — вектор для каждого токена, используется в задачах sequence labeling (NER, POS tagging)
- **Sentence-level** — агрегированное представление (например, через mean/max pooling или CLS token), используется в retrieval, классификации, кластеризации
- **Document-level** — формируется за счёт агрегации sentence embeddings или через long-context модели (например, Longformer, BigBird, RoPE-based LLM)

---

## Мотивация использования

Ограничения статических эмбеддингов:

- Не различают омонимы
- Игнорируют порядок и зависимость слов в предложении
- Недостаточно чувствительны к семантическому контексту

Преимущества контекстных:

- Динамически адаптируются к смысловому окружению
- Позволяют решать более сложные семантические задачи (вопросно-ответные системы, тематическая близость, мультиязычные сценарии)
- Улучшают переносимость между задачами и доменами

---

## Основные области применения

### Семантический поиск (Dense Retrieval)

- Векторы для запросов и документов вычисляются отдельно и сравниваются с помощью cosine similarity или dot product.
- Используются модели: **E5**, **BGE**, **GTE**, **MiniLM**.

### Retrieval-Augmented Generation (RAG)

- Контекстные эмбеддинги позволяют эффективно индексировать текстовые чанки.
- Используются при генерации ответов с опорой на внешние источники знаний.

### Zero-shot и few-shot перенос

- Благодаря обучению на обширных корпусах, контекстные эмбеддинги показывают высокую переносимость даже без дополнительного обучения на целевой задаче.

### Кластеризация и дедупликация

- Объекты группируются по смысловой близости, а не по лексической схожести.
- Обнаруживаются семантически схожие тексты, даже если они переформулированы.

### Semantic similarity и sentence scoring

- Оценка степени смысловой близости между парами текстов (например, paraphrase mining).

---

## Достоинства

- Учитывают контекст и изменяющееся значение слов в разных ситуациях
- Способствуют улучшению качества в retrieval-задачах и RAG-пайплайнах
- Легко интегрируются в многоязычные и мультимодальные модели
- Обеспечивают высокую переносимость (transferability) без необходимости обучения с нуля
- Поддерживают симметричные и асимметричные сценарии (раздельные энкодеры для query и документа)

---

## Ограничения

- Вычислительно более затратны, чем статические эмбеддинги
- Интерпретируемость векторов затруднена: сложно объяснить, за что отвечает каждая координата
- Чувствительны к форматированию, длине и способу агрегации эмбеддингов
- Качество зависит от архитектуры, размера модели и качества предобучения

---

## Популярные модели и инструменты

- **Sentence-Transformers** — библиотека для получения sentence-level embeddings (на базе моделей BERT, RoBERTa, MiniLM и др.)
- **E5 / BGE / GTE** — специализированные модели для retrieval
- **Instructor** — модели с использованием инструкций при обучении эмбеддингов
- **LaBSE**, **USE** — мультиязычные модели от Google
- **MTEB Benchmark** — стандартный бенчмарк для сравнения моделей эмбеддингов на множестве задач (semantic search, classification, clustering и т.д.)