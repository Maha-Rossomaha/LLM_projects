# Triplet Loss и его вариации

## 1. Общая идея

**Triplet Loss** — это функция потерь для contrastive learning, направленная на то, чтобы эмбеддинг **якоря (anchor)** был ближе к **позитиву (positive)**, чем к **негативу (negative)** хотя бы на заданное расстояние (margin).

Пример использования: обучение sentence embeddings, face recognition, product matching, intent classification.

---

## 2. Формула Triplet Loss

$$
L = \max\left(0, \text{sim}(A, P) - \text{sim}(A, N) + \text{margin}\right)
$$

где:

* $A$ — anchor (якорный пример)
* $P$ — позитивный пример (с тем же классом / смыслом)
* $N$ — негативный пример (другой класс / контекст)
* $\text{sim}(\cdot, \cdot)$ — обычно cosine или dot product
* $\text{margin}$ — положительное число, гарантирующее разрыв

---

## Вариации Triplet Mining

### 1. Random Negatives

* Негативы выбираются случайно из других классов
* Наиболее простая, но малоэффективная стратегия

### 2. Semi-Hard Negatives

* $N$ ближе к $A$, чем $P$, но не ближе на margin
* Обеспечивают сильный градиент и устойчивость к переобучению

### 3. Hardest Negatives

* $N$ максимально приближен к $A$
* Могут вызвать **collapsing** или неустойчивое обучение

---

## Примеры триплетов

**Anchor:** "как выбрать беспроводные наушники"

* **Positive:** "лучшие Bluetooth-наушники для телефона"
* **Negative (easy):** "купить ноутбук 2023"
* **Negative (semi-hard):** "наушники для офиса"
* **Negative (hard):** "обзор беспроводных TWS гарнитур"

---

## Сравнение с InfoNCE

| Характеристика           | Triplet Loss              | InfoNCE                                |
| ------------------------ | ------------------------- | -------------------------------------- |
| Тип сэмплирования        | Один негатив на якорь     | Множество негативов (in-batch, bank)   |
| Градиентный сигнал       | Слабее (одно сравнение)   | Сильнее (много негативов)              |
| Вычислительная сложность | Ниже                      | Выше (особенно при больших батчах)     |
| Сходимость               | Медленнее                 | Быстрее при хорошо выбранных негативов |
| Применение               | Fine-grained distinctions | Retrieval, bi-encoder training         |

**Вывод:** Triplet Loss лучше подходит для задач с точной разметкой и жёсткой семантической границей (например, person ID, intent disambiguation).

---

## Когда использовать Triplet вместо InfoNCE

* Объекты хорошо размечены по классам (например, кластеризация или классификация)
* Ограничен размер батча, нет возможности использовать in-batch negatives
* Нужно контролировать **margin между классами**
* Сильная симметрия между anchor и positive (например, в парных задачах)

---

## Практические советы

* Используйте **semi-hard mining** для устойчивого обучения
* Применяйте **batch all** или **batch hard** стратегии внутри батча
* Отслеживайте **долю триплетов с ненулевым лоссом**
* Начинайте с меньшего margin (0.2–0.5) и постепенно увеличивайте
