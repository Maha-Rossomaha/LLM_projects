# Contrastive Learning Intro


## 1. Что такое Contrastive Learning

**Contrastive Learning** — это класс методов обучения, в которых модель обучается различать похожие и непохожие объекты. Цель — спроецировать входные данные в такое пространство признаков (embedding space), где семантически близкие объекты оказываются ближе друг к другу, а семантически далёкие — дальше.

Это особенно полезно, когда у нас нет ярко размеченных данных, но можно сгенерировать пары «положительных» и «отрицательных» примеров:

* **Положительная пара (positive pair)** — два представления одного и того же объекта (например, два переформулированных запроса с одинаковым смыслом).
* **Отрицательная пара (negative pair)** — представления разных объектов (например, два вопроса, не связанных между собой семантически).

Contrastive Learning активно применяется для обучения sentence embeddings в LLM, для дообучения retriever'ов в QA-системах и других NLP-задачах.

---

## 2. Embedding Space

### 2.1 Определение

**Embedding space** — это многомерное векторное пространство, в котором объекты (вопросы, документы, фразы) представлены в виде плотных векторов фиксированной размерности. Эти векторы называются эмбеддингами.

Каждая точка в embedding space представляет объект таким образом, что геометрические отношения между точками отражают семантические или контекстные отношения между самими объектами.

Примеры:

* Запросы "Как поменять пароль в Гугле?" и "Как восстановить доступ к Google аккаунту?" будут близки друг к другу.
* Запрос "Что такое квантовая запутанность?" будет далёк от "Лучшие рецепты борща".

### 2.2 Почему важно адаптировать embedding space

Contrastive Learning работает напрямую в embedding space, и именно его структура определяет, насколько хорошо модель умеет различать объекты. Однако embedding space, полученное без адаптации, может обладать рядом проблем:

#### 2.2.1 Плохая кластеризация

Без обучения embedding space может не группировать тексты по смыслу. Например, технический вопрос и шутка на ту же тему могут оказаться далеко друг от друга, несмотря на общий контекст.

#### 2.2.2 Слишком плотное сжатие (collapse)

Если модель не получает достаточно отрицательных примеров, она может спроецировать все объекты в одну и ту же точку. Это приводит к коллапсу — все эмбеддинги одинаковые, различия между объектами теряются.

#### 2.2.3 Доминирование поверхностных признаков

Embedding space может начать учитывать только лексическое сходство, игнорируя смысл. Например, запросы *«Как настроить сетевой роутер»* и *«История развития сетевого протокола TCP/IP»* могут оказаться ближе друг к другу, чем запрос *«Как установить принтер в Windows»*. Несмотря на то что первый и второй относятся к разным задачам (прикладная настройка vs исторический обзор), модель может переоценивать совпадение слова *«сетевой»* и игнорировать, что тематика и цель запроса различаются.

#### 2.2.4 Неинвариантность к переформулировкам

Embedding space должен быть устойчив к разным формулировкам одного смысла. Без обучения с положительными парами модель может не понимать, что "Как сбросить пароль?" и "Не помню пароль, что делать?" — это одно и то же.

### 2.3 Адаптация embedding space

Contrastive Learning решает все эти проблемы следующим образом:

* Положительные пары приближают эмбеддинги разных формулировок одного и того же смысла.
* Отрицательные пары отталкивают эмбеддинги смыслово различных объектов.
* Loss-функции (например, NT-Xent, Triplet Loss, InfoNCE) оптимизируют геометрию embedding space.

В результате пространство начинает:

* выделять устойчивые, инвариантные признаки;
* эффективно группировать тексты по смыслу;
* улучшать качество задач типа кластеризации, retrieval, reranking и т.д.

---

## 3. Contrastive Loss Functions

### 3.1 InfoNCE Loss (Noise Contrastive Estimation)

Одна из наиболее распространённых функций потерь для обучения bi-encoder retrieval моделей (например, в E5, GTE, CLIP).

#### 3.1.1 Формула:

$$
L = -\log\left( \frac{\exp(\frac{\text{sim}(q, d^+)}{\tau})}{\exp(\frac{\text{sim}(q, d^+)}{\tau}) + \sum_j \exp(\frac{\text{sim}(q, d^-)}{\tau})} \right)
$$


Где:

* `q` — запрос (query)
* `d⁺` — позитивный документ (релевантный)
* `d⁻_j` — негативные документы
* `sim(x, y)` — функция близости (обычно cosine similarity или dot product)
* $\tau$ — температурный параметр

#### 3.1.2 Смысл:

Модель должна:

* максимизировать сходство `sim(q, d⁺)`
* минимизировать сходство `sim(q, d⁻_j)` для всех негативов

#### 3.1.3 Температура

* Делит логиты: $\ell=\text{sim}/\tau$. Меньше $\tau$ → **резче softmax** (более «жёсткие» вероятности); больше $\tau$ → мягче.
* Масштабирует градиенты **обратно пропорционально**:

  $$
  \frac{\partial L}{\partial \,\operatorname{sim}(q,k_i)} = \frac{p_i - y_i}{\tau},\quad y_0=1,\ y_{i>0}=0.
  $$

  То есть $1/\tau$ усиливает сигналы обучения.
* Эквивалент «маржи»: при фиксированных нормах уменьшение $\tau$ требует **большей разницы** между $q\cdot k^+$ и $q\cdot k^-,$, что действует как увеличение margin.

#### 3.1.4 Влияние на представления

* **Слишком маленькое $\tau$**: обучение может стать нестабильным, доминируют «самые трудные» негативы, риск коллапса/переобучения.
* **Слишком большое $\tau$**: распределение вероятностей слишком ровное, сигнал слабый, различающая способность падает.
* С ростом числа негативов $K$ обычно полезно **слегка уменьшать $\tau$**.


#### 3.1.5 Пример:

```
Запрос: "дешёвые беспроводные наушники"
Позитив: "TWS Xiaomi AirDots"
Негативы: "проводные наушники Sony", "игровая мышка"
```

Модель учится приближать эмбеддинг запроса к "TWS Xiaomi AirDots" и отдалять от других.

#### 3.1.6 Особенности:

* Поддерживает батчевую негативную генерацию (in-batch negatives)
* Используется в SimCLR, CLIP, Sentence-T5, E5
* Прост в реализации

---

### 3.2 Triplet Loss

Triplet Loss формирует обучающие примеры в виде троек:

* Anchor (A)
* Positive (P)
* Negative (N)

#### 3.2.1 Формула:

$$
L = \max(0,\; \text{sim}(A, P) - \text{sim}(A, N) + \text{margin})
$$

Где:

* `sim` — мера близости (обычно dot или cosine)
* `margin` — минимальный разрыв, который модель должна обеспечить

#### 3.2.2 Смысл:

* Модель минимизирует потерю, если позитив ближе к якорю, чем негатив, хотя бы на `margin`

#### 3.2.3 Пример:

```
Anchor: "какие наушники купить для спортзала"
Positive: "беспроводные спортивные наушники JBL"
Negative: "наушники для студийной записи"
```

#### 3.2.4 Особенности:

* Triplet Loss работает с одной тройкой за раз, поэтому он плохо использует батч: каждая тройка независима, и градиенты не видят все другие примеры в батче. Это снижает эффективность обучения
* Тонкая настройка margin критична
* Неустойчив к плохим негативам (например, если N слишком похож на P)

---

### 3.3 Supervised Contrastive Loss

Расширяет InfoNCE для использования **множества позитивных примеров одного класса**, особенно в случае полной аннотации (supervised setting).

#### 3.3.1 Формула:

(упрощённая форма без температуры)

$$
L = -\sum_i \sum_{p \in P(i)} \log\left( \frac{\exp(\text{sim}(z_i, z_p))}{\sum_{a \ne i} \exp(\text{sim}(z_i, z_a))} \right)
$$


Где:

* `z_i` — эмбеддинг примера `i`
* `P(i)` — позитивные примеры того же класса, что и `i`

#### 3.3.2 Смысл:

* Каждый пример должен быть ближе ко всем позитивным примерам из того же класса, чем ко всем негативным

#### 3.3.3 Пример:

```
Класс: "наушники"
Позитивы: "беспроводные наушники", "TWS гарнитура", "вкладыши Sony"
Негативы: "ноутбук", "мышка", "смарт-часы"
```

#### 3.3.4 Особенности:

* Эффективен в мультимодальных или многоязычных сценариях
* Может использоваться при обучении sentence embeddings с label supervision
* Применяется в SimCSE (supervised), SCL, DeCLUTR и др.

---

### 3.4 Сравнение

| Loss Function          | Тип обучения     | Кол-во негативов | Подходит для                      |
| ---------------------- | ---------------- | ---------------- | --------------------------------- |
| InfoNCE                | Contrastive      | Много            | Bi-encoder, dense retrieval       |
| Triplet Loss           | Contrastive      | Один             | Простые задачи, с margin-тюнингом |
| Supervised Contrastive | Fully Supervised | Много            | Классификация, мультимодальность  |

---

## 4. Distance Metrics

В задачах поиска, кластеризации и retrieval с использованием эмбеддингов критически важно выбрать подходящую **метрику расстояния** или **функцию близости**. Она определяет, насколько "похожи" векторы, представляющие разные тексты, изображения или другие объекты.

---

### 4.1 Cosine Similarity (косинусная близость)

#### 4.1.1 Определение

Косинусное сходство измеряет угол между двумя векторами:

$$
\text{sim}_{\text{cos}}(x, y) = \frac{x \cdot y}{\|x\| \cdot \|y\|}
$$

Где:

* `x · y` — скалярное произведение
* `||x||` и `||y||` — длины векторов (L2-норма)

#### 4.1.2 Диапазон значений

* От -1 до 1
* При нормализации векторов (L2-norm) — от 0 до 1

#### 4.1.3 Свойства

* Не учитывает длину вектора, только направление
* Устойчив к масштабированию
* Часто используется в retrieval-задачах и contrastive learning

#### 4.1.4 Применение

* Семантический поиск
* Bi-encoder retrieval (E5, BGE, CLIP)
* SimCSE, Sentence-BERT
* Ранжирование кандидатов по схожести

---

### 4.2 Euclidean Distance (евклидово расстояние)

#### 4.2.1 Определение

Обычное L2-расстояние между двумя точками в пространстве:

$$
\text{d}_{\text{euc}}(x, y) = \|x - y\| = \sqrt{\sum_i (x_i - y_i)^2}
$$

#### 4.2.2 Диапазон значений

* От 0 до ∞

#### 4.2.3 Свойства

* Чувствительно к масштабам векторов
* Учитывает как направление, так и длину векторов
* Требует аккуратной нормализации входных данных

#### 4.2.4 Применение

* Кластеризация (например, K-means)
* Обнаружение аномалий
* Визуализация эмбеддингов (t-SNE, UMAP)
* Некоторые архитектуры (e.g. Siamese Networks)

---

### 4.3 Jaccard Similarity / Distance

#### 4.3.1 Определение

Для бинарных или множественных векторов A и B:

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}, \quad \text{d}_{\text{jaccard}} = 1 - J(A, B)
$$

#### 4.3.2 Диапазон значений

* \[0, 1]

#### 4.3.3 Свойства

* Применима к sparse / бинарным векторам
* Устойчивость к вариативности длины
* Не учитывает частоту признаков

#### 4.3.4 Применение

* Sparse embeddings (TF-IDF, BM25)
* Сравнение множеств, тэгов
* Рекомендательные системы

---

### 4.4 Hamming Distance

#### 4.4.1 Определение

$$
\text{d}_{\text{hamming}}(x, y) = \sum_i [x_i \ne y_i]
$$


#### 4.4.2 Диапазон значений

* От 0 до длины вектора

#### 4.4.3 Свойства

* Применяется к бинарным векторам одинаковой длины
* Учитывает точные позиционные отличия

#### 4.4.4 Применение

* Locality-sensitive hashing (LSH)
* SimHash
* Fingerprinting

---

### 4.5 Сравнение

| Метрика            | Подходит для                  | Тип векторов  | Чувствительность к длине |
| ------------------ | ----------------------------- | ------------- | ------------------------ |
| Cosine Similarity  | Semantic search               | Dense         | Нет                      |
| Euclidean Distance | Clustering, Anomaly Detection | Dense         | Да                       |
| Jaccard Similarity | Sparse search, Tag overlap    | Sparse/Binary | Нет                      |
| Hamming Distance   | Hashing, SimHash              | Binary        | Да                       |

---

### 4.6 Практические рекомендации

---

* Для поиска по смыслу и семантики — используйте **cosine similarity** с нормализованными эмбеддингами
* Для задач кластеризации, в которых важны абсолютные расстояния — **евклидово расстояние** (с учетом масштабов)
* Для визуализации и оценки плотности — **euclidean**, но после снижения размерности
* Для обучения contrastive моделей — чаще всего используется **cosine** (внутри InfoNCE, SimCLR)
* Применяйте **Jaccard** для sparse и set-based представлений
* Используйте **Hamming** при работе с бинарными fingerprint’ами