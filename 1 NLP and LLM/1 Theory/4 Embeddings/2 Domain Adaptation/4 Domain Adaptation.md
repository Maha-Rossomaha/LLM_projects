# Domain Adaptation&#x20;

Адаптация эмбеддингов под новый домен — важный шаг при работе с медицинскими, юридическими, техническими и другими специализированными текстами. Даже хорошо обученные LLM могут терять эффективность, если не учитывать специфику терминов, структуры и задач целевого корпуса.

---

## 1. Domain-Adaptive Pretraining (DAPT)

**DAPT** — это дополнительное обучение предобученной языковой модели на **неразмеченном корпусе** из целевого домена с использованием задачи Masked Language Modeling (MLM).

### Цель:

Сформировать устойчивые эмбеддинги доменно-специфичной лексики и структуры текста, сохранив при этом общие свойства модели.

### Как работает:

* Используется существующая MLM-задача (как в оригинальном BERT)
* Корпус не требует разметки (например, тысячи медицинских заключений, судебных актов или технической документации)
* Дообучение производится на этапе перед fine-tune, чтобы модель "впитала" особенности домена

### Примеры:

* Предобучение BERT на текстах медицинских заключений (электронные истории болезни, диагнозы, рецепты)
* Дообучение encoder модели на юридических контрактах и нормативных документах
* DAPT на технических статьях, содержащих специфичную терминологию и форматы (например, патенты, спецификации, научные отчёты)

### Эффект:

* Повышение zero-shot и few-shot производительности в downstream задачах (NER, классификация, retrieval)
* Улучшение качества эмбеддингов редких или ранее незнакомых слов
* Адаптация attention-паттернов к стилю и структуре доменного текста
* Снижение необходимости в большом количестве размеченных данных при последующем fine-tune

---

## 2. Task-Adaptive Pretraining (TAPT)

**TAPT** — это этап дополнительного предобучения модели на текстах, специфичных для конкретной downstream-задачи, но **всё ещё в режиме Masked Language Modeling (MLM)**.

### Цель:

Адаптировать модель не только к общему домену (как в DAPT), но и к структурным и лексическим особенностям конкретной задачи, на которой впоследствии будет производиться fine-tune.

### Как работает:

* Берётся корпус неразмеченных данных, соответствующих задаче (например, истории болезни для классификации диагнозов)
* Выполняется MLM-предобучение, чтобы модель запомнила паттерны, стили, частые словосочетания и шаблоны
* После этого проводится обучение на размеченной задаче

### Примеры применения:

* Классификация медицинских заключений → TAPT на сырых историях болезни
* Классификация юридических документов → TAPT на неразмеченных контрактах
* Ответы на технические запросы → TAPT на логах или тикетах

### Отличие от DAPT:

* DAPT адаптирует модель к **домену в целом**
* TAPT подготавливает модель к **структуре и стилю конкретной задачи**, например: короткие заголовки, шаблонные списки, специфическая терминология

### DAPT vs TAPT:

| Метод | Данные                        | Цель                               |
| ----- | ----------------------------- | ---------------------------------- |
| DAPT  | Общий корпус домена           | Общая адаптация к лексике, стилю   |
| TAPT  | Конкретная задача, unlabelled | Адаптация к task-specific шаблонам |

---

## 3. Заморозка эмбеддингов и словари сущностей

### Проблема редких токенов:

В специализированных доменах встречаются длинные и редкие термины (например, "Бромдигидрохлорфенилбензодиазепин"), которые не входят в исходный словарь модели. Такие слова разбиваются токенизатором на фрагменты (subwords), часто не имеющие смысла вне исходного контекста.

При обучении с задачей MLM это может привести к следующим проблемам:

* модель переучивается на редкие, плохо сегментированные токены,
* токены, ранее стабильно интерпретируемые, смещаются в embedding space,
* универсальность эмбеддингов ухудшается,
* downstream-задачи начинают давать регресс в «общем» языке.

### Возможные решения:

1. **Добавление доменных токенов в словарь**

   * Анализ корпуса (например, через частотность и длину токенов)
   * Добавление полных форм доменных сущностей (лекарства, юридические статьи, технические команды)
   * Повторная инициализация токенизатора и embedding-матрицы

2. **Заморозка эмбеддингов**

   * После 1–2 эпох DAPT можно зафиксировать слой `token_embeddings`, оставив обучаться только encoder
   * Это снижает риск деградации pre-trained слоёв
   * Можно использовать `requires_grad = False` для embedding-слоя

3. **Сброс и переопределение словаря**

   * В случае тяжёлых доменных сдвигов (например, патентные тексты) разумно полностью переобучить embedding-матрицу
   * При этом полезно сохранить позиционные и encoder-слои pretrained

4. **Использование регуляризаторов**

   * Например, L2 penalty на сдвиг pretrained-эмбеддингов
   * Потери на стабильность (embedding drift penalty)

### Практика:

* В медицине часто добавляют названия препаратов, синдромов, МКБ-коды
* В юридической сфере — сокращения и нормы («ст.», «п.», «ГК РФ»)
* В технической — параметры, команды CLI, синтаксис кода

### Вывод:

Работа со словарём и заморозкой эмбеддингов — ключевой аспект устойчивого DAPT. Она позволяет избежать деградации модели и сохранить полезные свойства базовой языковой архитектуры.

---

## 4. Индекс и переобучение

### Проблема согласованности индекса и модели:

После адаптации embedding-модели под новый домен (например, через DAPT или TAPT), меняется расположение точек в embedding-пространстве. Если при этом использовать **старый индекс** (например, FAISS), построенный до дообучения, возникает несогласованность между моделью и retrieval-структурой.

### Что происходит:

* **Embedding space смещается**: эмбеддинги документов и запросов распределяются по-новому
* **Запросы ************`q`************ начинают притягиваться к старым кластерам**, которые уже не соответствуют новым embedding-документам
* **Тренировочный Recall\@K может временно расти**, но **offline-качество деградирует**: модель «подгоняет» запрос под несовместимый FAISS
* Особенно критично при тонкой адаптации (medical, legal), где небольшие сдвиги имеют значение

### Практический пример:

* Дообучил bi-encoder на медицинских документах
* Оценка Recall\@K показывает улучшение
* Но при запуске на проде: релевантные ответы не находятся — модель подстроилась под «тень» старого индекса

### Рекомендации:

* **Обязательно перестраивай индекс** после каждого существенного обновления модели
* Если используешь FAISS или аналог — храни embedding-функции вместе с индексом (versioned index)
* При обучении с retrieval-in-loop не замораживай индекс, а обновляй его итеративно
* Используй устойчивые архитектуры, такие как:

  * **MoCo** (momentum encoder)
  * **RetroMAE** (retrieval-aware pretraining)
  * **DPR + dual momentum**, которые синхронизируют `q` и `d`