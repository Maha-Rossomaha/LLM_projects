# Domain Adaptation&#x20;

Адаптация эмбеддингов под новый домен — важный шаг при работе с медицинскими, юридическими, техническими и другими специализированными текстами. Даже хорошо обученные LLM могут терять эффективность, если не учитывать специфику терминов, структуры и задач целевого корпуса.

---

## 1. DAPT — Domain-Adaptive Pretraining

**Что делаем.** CPT на **широком доменном корпусе** (медицина, право, код и т.д.). Цель — чтобы модель «в целом говорила на языке домена».

**Как:**

* Encoder-only → MLM/span-MLM.
* Decoder-only → каузальное продолжение (иногда UL2-подобные коррапшены).
* Корпус: много, разно, по всей области.
* LR небольшой, шагов умеренно.

**Эффект:**

* Падает перплексия на доменных текстах.
* Растут zero/few-shot по задачам в домене.

---

## 2. TAPT — Task-Adaptive Pretraining

**Что делаем.** CPT на **неразмеченных текстах в точном формате будущей задачи**: те же источники, длины, шаблоны.

**Как:**

* Encoder-only → MLM, но на *узком* корпусе «как в задаче».
* Decoder-only → каузальное CPT на «промпт‑подобных» входах (можно вставлять шаблоны без ответов, чтобы модель выучила форму).
* Маскирование/коррапшен под формат: короткие заголовки, поля форм, «Вопрос: … Контекст: … Ответ: …» (без меток).

**Эффект:**

* Максимальный прирост **именно на целевой задаче**, особенно при малой разметке.
* Падает перплексия на распределении задачи (а не на всём домене).

---

## 3. DAPT vs TAPT — ключевое различие

| Метод    | На каких данных                         | К чему адаптируем                                   | Когда выбирать                                    |
| -------- | --------------------------------------- | --------------------------------------------------- | ------------------------------------------------- |
| **DAPT** | Широкий доменный корпус (без меток)     | К «языку домена» в целом: термины, стиль, синтаксис | Когда доменный сдвиг большой и задач несколько    |
| **TAPT** | Неразмеченные тексты *в формате задачи* | К *входам задачи*: длина, шаблоны, источники        | Когда одна задача, мало разметки, важен её формат |

**Коротко:** DAPT отвечает «понимает ли модель мой домен вообще?», TAPT — «понимает ли модель именно такие входы, как в моей задаче?».

---

## 4. Общий пайплайн

Рекомендуемая цепочка:

1. **DAPT (CPT)** → широкая доменная подстройка.
2. **TAPT (CPT)** → узкая подстройка под формат задачи.
3. **SFT** → обучение на размеченной задаче (классификация/QA/RAG‑компоненты и т.п.).
4. **Alignment** (опционально) → DPO/RLHF для поведенческих предпочтений.

Почему это не стирается:

* DAPT/TAPT и SFT оптимизируют **разные цели**; при разумных LR/шагаx знания не «смываются», а *достраиваются*.
* Чтобы снизить риск забывания, в SFT/Alignment:

  * использовать меньший LR и раннюю остановку;
  * применять PEFT (LoRA/QLoRA), замораживая основу;
  * при Alignment — легкую фазу с ограниченным числом шагов;
  * по необходимости — *regularization-by-replay*: небольшой микс доменных текстов как несупервизорный auxiliary (иногда называют «stability mix»).

---

### 5.1 Типичные проблемы и предохранители

* **Catastrophic forgetting:** слишком долгий SFT/Alignment или высокий LR → держать LR низким, шаги ограниченными, можно LoRA.
* **Слишком узкий TAPT:** падает переносимость внутри домена → ограничивать длительность TAPT, по необходимости смешивать 10–20% доменных текстов.
* **Дупликация с train/dev/test:** жёсткий dedup, чтобы не утекали тексты.

---

### 5.2 Итоговые правила выбора

* **Есть сильный доменный сдвиг и несколько задач** → DAPT обязательно; TAPT — по задачам с особыми форматами.
* **Одна задача и мало лейблов** → TAPT даст лучший ROI; DAPT опционален, если домен близок к общему.
* **Бюджет позволяет** → делаем: **DAPT → TAPT → SFT → Alignment (лёгкий)**.
* **Бюджета мало** → **TAPT‑лайт** на источнике разметки + аккуратный SFT (LoRA).


---

## 6. Заморозка эмбеддингов и словари сущностей

### Проблема редких токенов:

В специализированных доменах встречаются длинные и редкие термины (например, "Бромдигидрохлорфенилбензодиазепин"), которые не входят в исходный словарь модели. Такие слова разбиваются токенизатором на фрагменты (subwords), часто не имеющие смысла вне исходного контекста.

При обучении с задачей MLM это может привести к следующим проблемам:

* модель переучивается на редкие, плохо сегментированные токены,
* токены, ранее стабильно интерпретируемые, смещаются в embedding space,
* универсальность эмбеддингов ухудшается,
* downstream-задачи начинают давать регресс в «общем» языке.

### Возможные решения:

1. **Добавление доменных токенов в словарь**

   * Анализ корпуса (например, через частотность и длину токенов)
   * Добавление полных форм доменных сущностей (лекарства, юридические статьи, технические команды)
   * Повторная инициализация токенизатора и embedding-матрицы

2. **Заморозка эмбеддингов**

   * После 1–2 эпох DAPT можно зафиксировать слой `token_embeddings`, оставив обучаться только encoder
   * Это снижает риск деградации pre-trained слоёв
   * Можно использовать `requires_grad = False` для embedding-слоя

3. **Сброс и переопределение словаря**

   * В случае тяжёлых доменных сдвигов (например, патентные тексты) разумно полностью переобучить embedding-матрицу
   * При этом полезно сохранить позиционные и encoder-слои pretrained

4. **Использование регуляризаторов**

   * Например, L2 penalty на сдвиг pretrained-эмбеддингов
   * Потери на стабильность (embedding drift penalty)

### Практика:

* В медицине часто добавляют названия препаратов, синдромов, МКБ-коды
* В юридической сфере — сокращения и нормы («ст.», «п.», «ГК РФ»)
* В технической — параметры, команды CLI, синтаксис кода

### Вывод:

Работа со словарём и заморозкой эмбеддингов — ключевой аспект устойчивого DAPT. Она позволяет избежать деградации модели и сохранить полезные свойства базовой языковой архитектуры.

---

## 7. Индекс и переобучение

### Проблема согласованности индекса и модели:

После адаптации embedding-модели под новый домен (например, через DAPT или TAPT), меняется расположение точек в embedding-пространстве. Если при этом использовать **старый индекс** (например, FAISS), построенный до дообучения, возникает несогласованность между моделью и retrieval-структурой.

### Что происходит:

* **Embedding space смещается**: эмбеддинги документов и запросов распределяются по-новому
* **Запросы** q` **начинают притягиваться к старым кластерам**, которые уже не соответствуют новым embedding-документам
* **Тренировочный Recall\@K может временно расти**, но **offline-качество деградирует**: модель «подгоняет» запрос под несовместимый FAISS
* Особенно критично при тонкой адаптации (medical, legal), где небольшие сдвиги имеют значение

### Практический пример:

* Дообучил bi-encoder на медицинских документах
* Оценка Recall\@K показывает улучшение
* Но при запуске на проде: релевантные ответы не находятся — модель подстроилась под «тень» старого индекса

### Рекомендации:

* **Обязательно перестраивай индекс** после каждого существенного обновления модели
* Если используешь FAISS или аналог — храни embedding-функции вместе с индексом (versioned index)
* При обучении с retrieval-in-loop не замораживай индекс, а обновляй его итеративно
* Используй устойчивые архитектуры, такие как:

  * **MoCo** (momentum encoder)
  * **RetroMAE** (retrieval-aware pretraining)
  * **DPR + dual momentum**, которые синхронизируют `q` и `d`