# Negatives Mining

## 1. In-Batch Negatives
### 1.1 Общая идея
В задачах обучения retrieval-моделей с использованием contrastive loss (например, InfoNCE), модель должна обучиться сопоставлять запрос (query) с релевантным документом (positive) и отдалять его от нерелевантных документов (negatives) в embedding-пространстве.
Для этого необходимы хорошие негативные примеры — документы, которые не соответствуют запросу, но при этом максимально похожи по структуре, лексике или теме, чтобы обучение было «сложным» (hard negatives).

### 1.2 Что такое In-Batch Negatives
In-batch negatives — это техника, при которой все остальные документы в текущем батче используются в качестве негативных примеров для каждого запроса.

#### Пример:

Пусть в батче 4 пары:

```
(q₁, d₁⁺), (q₂, d₂⁺), (q₃, d₃⁺), (q₄, d₄⁺)
```

Для `q₁`:

* **Positive:** `d₁⁺`
* **Negatives:** `d₂⁺`, `d₃⁺`, `d₄⁺`

И так далее для каждого запроса.

### 1.3 Формулировка в InfoNCE:

$$
L = -\log\left( \frac{\exp(\text{sim}(q, d^+))}{\sum_{j=1}^N \exp(\text{sim}(q, d_j))} \right)
$$

Где `d_j` — все документы из батча (включая позитивный)


### 1.4 Преимущества In-Batch Sampling

* **Бесплатные негативы**: не нужно извлекать или хранить дополнительные примеры
* **Эффективность**: позволяет обрабатывать десятки негативов за один проход
* **Обобщаемость**: в батчах часто оказываются релевантные, но нерелевантные по аннотации примеры, что повышает устойчивость модели
* **Совместимость с InfoNCE**: используется в CLIP, SimCLR, GTR, E5

### 1.5 Недостатки и ограничения

* Негативы могут быть **легкими** (easy negatives), т.е. очевидно нерелевантными
* В одном батче редко оказываются действительно **трудные негативы**
* При небольших размерах батча — ограниченное разнообразие негативов

---

## 2. Hard and Soft Negatives
### 2.1 Определения
#### 2.1.1 **Hard Negatives**
**Hard negatives** — это нерелевантные документы, которые **семантически близки к запросу**, но не являются правильным ответом.

#### Пример:

Запрос: "apple customer service phone"

* **Positive:** "Apple support hotline"
* **Hard negative:** "Apple store location", "Apple iPhone specifications"

Такие примеры сбивают модель с толку, потому что они лексически и тематически похожи, но не дают правильного ответа.

#### 2.1.2 **Soft Negatives**

**Soft negatives** — это нерелевантные примеры, которые ближе к запросу, чем случайные негативы, но менее опасны, чем hard.

#### Пример:

Запрос: "беспроводные наушники для спорта"

* **Soft negative:** "беспроводные наушники для офиса"
* **Hard negative:** "TWS наушники для бега (но не спортивные)"


### 2.2 Почему это важно?

Обучение с **только easy negatives** (например, случайных документов) быстро приводит к **снижению градиента** — модель уже умеет их различать. Hard и soft negatives обеспечивают более информативный loss и улучшают обобщающую способность.

### 2.3 Источники негативов

| Тип                | Как получают                         |
| ------------------ | ------------------------------------ |
| Easy negatives     | Случайная выборка нерелевантных пар  |
| In-batch negatives | Пары из других запросов внутри батча |
| Hard negatives     | Top-K из dense или BM25 индекса      |
| Soft negatives     | Отфильтрованные top-K (по семантике) |

---

### 2.4 Как использовать в обучении

#### 2.4.1 Explicit Hard Negatives

* Добавлять вручную или через BM25/dense retrieval
* Пример: `(q, d⁺, d⁻_hard)` в Triplet Loss

#### 2.4.2 Semi-Hard Mining

* Оставлять только те негативы, которые ближе к запросу, чем другие, но не ближе, чем позитив

#### 2.4.3 Curriculum Learning

* Постепенное добавление всё более трудных негативов по ходу обучения

#### 2.4.4 Hybrid Strategies

* Сочетание in-batch и hard negatives в одном батче
* Используется, например, в DPR и RocketQA

### 2.5 Риски и ограничения

* **False negatives:** выбранный hard negative может на самом деле быть релевантным, особенно при неточной аннотации
* **Mode collapse:** при чрезмерно агрессивных hard negatives модель может перестать различать классы
* **Compute cost:** требует внешнего индекса или модели для отбора негативов

### 2.6 Рекомендации

* Используйте hard negatives, если:

  * У вас есть внешний retriever (BM25 или dense)
  * Размер батча недостаточен для разнообразия in-batch
* Добавляйте soft negatives для баланса и стабильности
* В retrieval-задачах применяйте hard negatives с margin-based losses (Triplet, InfoNCE)

---

## 3. ACNE: Asymmetric Contrastive Negative Example Mining

**ACNE (Asymmetric Contrastive Negative Example mining)** — это стратегия негативного сэмплирования, при которой используется **асимметрия** между сторонами запроса и документа в contrastive learning.

Вместо использования пар (q, d) симметрично с обеих сторон, ACNE применяет **одностороннее сэмплирование негативов** — например, используя только эмбеддинг запроса (`q`) как якорь и выбирая трудные негативы среди документов (`d⁻`) по близости в embedding space.

### 3.1 Мотивация

* В классическом InfoNCE все примеры обрабатываются симметрично: запрос и документ взаимозаменяемы.
* Однако в retrieval-сценариях, особенно при использовании **разных энкодеров для** `q`**и** `d` (асимметричный bi-encoder), такая симметрия может быть нежелательна.
* ACNE позволяет:

  * Избегать ложной симметрии между `q` и `d`
  * Сконцентрироваться на "сложных" негативных документах по отношению к одному якорю
  * Повысить устойчивость модели и улучшить градиенты

### 3.2 Как работает ACNE

#### 3.2.1 Алгоритм:

1. Выбирается якорь (обычно `query`)
2. Из внешнего индекса (BM25, dense index) подбираются top-K ближайших `d⁻`
3. Из них отфильтровываются `d⁺`, полученные по аннотации
4. Остальные — hard negatives

#### 3.2.2 Пример:

Для запроса `q`:

* **Positive:** `d⁺`
* **Negatives:** выбираются среди всех документов, \*\*которые близки к \*\*`, но не являются `

Для самого документа `d⁺` такие негативы не рассматриваются — они подбираются только относительно `q`, то есть **асимметрично**.

### 3.3 Где применяется

* Retrieval модели с раздельными encoder'ами (query/document)
* Сценарии, где запрос и документ имеют разные стили (например, short vs long text)
* Модели, в которых требуется контролируемое negatives mining, без риска "зеркальных" ошибок

### 3.4 Преимущества

* **Асимметрия** соответствует реальному workflow retrieval-систем
* **Более качественные негативы** — выбираются с учётом сложности запроса
* **Уменьшение ложных пар** — документ не используется как якорь, что снижает риск false positives

### 3.5 Ограничения

* Требуется внешний индекс (BM25 или dense) для предварительного поиска
* Не подходит для симметричных задач (например, paraphrase mining)
* Может пропустить трудные `query`-ориентированные негативы, если использовать только `d` в качестве якоря

---

## 4. MoCo: Momentum Contrast
**MoCo (Momentum Contrast**) — это метод contrastive learning, ориентированный на эффективное обучение представлений при большом числе негативных примеров, особенно в условиях ограниченного размера батча.
Ключевые компоненты:
- **Два энкодера:** `encoder_q` (query encoder) и `encoder_k` (key encoder)

* **Обновление** `encoder_k` **по EMA:**

$$
\texttt{encoder}_k \leftarrow m \cdot \texttt{encoder}_k + (1 - m) \cdot \texttt{encoder}_q
$$

где `m` — коэффициент импульса (обычно 0.999)

* **Memory Bank** — буфер хранимых негативных эмбеддингов, полученных с предыдущих шагов

### 4.1 Зачем нужен momentum encoder?

Если использовать один encoder и постоянно обновлять параметры, негативы в InfoNCE быстро устаревают и становятся несогласованными. MoCo решает это так:

* `encoder_q` обучается градиентами
* `encoder_k` не обновляется напрямую, а **медленно догоняет** `encoder_q`, благодаря чему:

  * Поддерживается согласованность между ключами (`keys`) и query
  * Негативные примеры становятся более стабильными и полезными

### 4.2 Что такое Memory Bank

**Memory Bank** — это очередь фиксированной длины, в которую после каждой итерации добавляются новые эмбеддинги `key`, а старые удаляются.

#### 4.2.1 Назначение:

* Обеспечивает **большое множество негативов** для каждого `query`
* Снижает необходимость увеличения batch size

#### 4.2.2 Принцип работы:

1. Каждый `query` сравнивается с:

   * Позитивом (`positive key`) из текущей пары
   * Множеством `negative keys` из Memory Bank (например, 65,536 векторов)
2. Loss считается по InfoNCE:

$$
L = -\log\left( \frac{\exp(\text{sim}(q, k^+))}{\exp(\text{sim}(q, k^+)) + \sum_j \exp(\text{sim}(q, k_j^-))} \right)
$$

### 4.3 Преимущества MoCo

* **Масштабируемость:** можно использовать десятки тысяч негативов без увеличения размера батча
* **Устойчивость:** momentum encoder обеспечивает стабильность ключей
* **Гибкость:** применим к разным типам энкодеров (vision, text, audio)

### 4.4 Пример применения

В retrieval или representation learning на больших коллекциях (веб-документы, изображения и т.д.), где один батч покрывает <0.01% пространства:

* batch size = 128
* memory bank = 65,536
* каждый `query` получает обучающий сигнал по 1 позитиву + 65,535 негативам

---

