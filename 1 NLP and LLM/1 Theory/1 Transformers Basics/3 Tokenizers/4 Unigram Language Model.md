## 1. Основы Unigram Language Model

Unigram LM токенизатор базируется на статистической модели, в которой каждое возможное субслово (юнит) имеет свою вероятность независимо от контекста. При разбиении текста выбирается комбинация токенов с максимальной суммарной вероятностью.

- **Независимость токенов**: модель оценивает вероятность каждого токена отдельно, не учитывая соседние.
- **Подбор словаря**: изначально составляется большой набор кандидатов (последовательностей символов), а затем методом жадной оптимизации удаляются наименее вероятные.
- **Гибкость**: легко адаптируется к разным языкам и доменам благодаря вероятностному отбору.

## 2. Архитектура токенизатора

Конвейер состоит из следующих этапов:

1. **Нормализация**: преобразование текста в единый формат (NFC/NFKC, нижний регистр, удаление лишних пробелов).
2. **Предтокенизация**: разбиение текста на «основные блоки» (слова или байты) с помощью правила вроде пробелов.
3. **Модель Unigram**:
   - Хранит словарь кандидатов с начальными вероятностями.
   - При токенизации для каждого слова находит все возможные сегментации.
   - Выбирает разбиение, максимизирующее сумму логарифмов вероятностей токенов.
4. **Пост-обработка**: добавление служебных меток (начало/конец, разделитель пар текстов).
5. **Декодер**: обратная сборка текста из токенов, удаление служебных префиксов.

## 3. Процесс обучения словаря

Обучение состоит из двух этапов:

### 3.1 Подготовка кандидатов

- Собрать корпус текстов и выполнить нормализацию.
- Сгенерировать из корпуса начальный набор кандидатов: все подстроки до максимальной длины.
- Оценить частоты появления каждого кандидата.

### 3.2 Алгоритм удаления маловероятных

1. Инициализировать вероятности кандидатов пропорционально частотам.
2. Итеративно удалять по одному наиболее «вредному» кандидату:
   - Для каждого кандидата рассчитывать ухудшение качества модели при его удалении (разность суммарного лог‑likelihood).
   - Удалять кандидата с наименьшим влиянием.
   - Нормализовать вероятности оставшихся.
3. Повторять, пока не достигнут заданный размер словаря.

## 4. Пример реализации на Python

```python
from tokenizers import Tokenizer
from tokenizers.models import Unigram
from tokenizers.trainers import UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace

# 1. Инициализация модели Unigram
tokenizer = Tokenizer(Unigram())

# 2. Настройка предтокенизатора
tokenizer.pre_tokenizer = Whitespace()

# 3. Конфигурация тренера
trainer = UnigramTrainer(
    vocab_size=32000,
    special_tokens=["[PAD]", "[CLS]", "[SEP]", "[UNK]", "[MASK]"],
    unk_token="[UNK]"
)

# 4. Обучение на корпусе (список путей к текстовым файлам)
files = ["path/to/corpus1.txt", "path/to/corpus2.txt"]
tokenizer.train(files, trainer)

# 5. Сохранение модели
tokenizer.save("./unigram-tokenizer.json")
```

## 5. Валидация и рекомендации

- **Проверка обратной декодировки**: для выборочных примеров убедиться, что `decode(encode(text)) == text`.
- **Анализ покрытия**: оценить процент OOV-токенов на валидационном корпусе.
- **Настройка гиперпараметров**: варьировать `vocab_size` и параметры `min_frequency` (если поддерживается) для оптимального баланса между granularity и скоростью инференса.

