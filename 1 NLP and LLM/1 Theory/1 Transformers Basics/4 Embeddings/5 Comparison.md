# Сравнение Word2Vec (CBOW, Skip-Gram), GloVe и FastText

## 0. Общая идея

Все они реализуют **распределительную гипотезу**:

> Слова, которые встречаются в похожих контекстах, должны иметь похожие вектора.

Разница только в том, **как именно** они используют контекст и что пытаются предсказывать.

---

## 1. Word2Vec: общая схема

Word2Vec (CBOW и Skip-Gram) — это **прогностические (predictive)** модели:

- Берём окно вокруг слова (контекст).
- Пытаемся **что-то предсказать**:
  - либо целевое слово по контексту (CBOW),
  - либо контекст по целевому слову (Skip-Gram).
- На входе и выходе — one-hot по словарю.
- Посередине — скрытый слой фиксированного размера $d$ → это и есть **embedding**.

Обычно обучают с:

- **negative sampling** или **hierarchical softmax**,
- скользящим окном по тексту.

---

## 2. Word2Vec CBOW

**Continuous Bag-of-Words**.

### Идея

- Есть контекст из нескольких слов $C = \{w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+k}\}$.
- Цель — предсказать **центральное слово** $w_t$ по контексту.

Схематично:

1. Для каждого слова контекста берём его embedding-вектор.
2. Усредняем / суммируем их:
   $$
   h = \frac{1}{|C|} \sum_{w \in C} v_w
   $$
3. По $h$ линейным слоем предсказываем распределение по словарю и максимизируем вероятность правильного $w_t$.

### Свойства

- Контекст — **bag-of-words**: порядок слов внутри окна теряется.
- **Быстрее** тренируется, чем Skip-Gram, потому что:
  - один пример → один выход.
- Хорошо работает на **частых словах**, сглаживает шум (усреднение).

---

## 3. Word2Vec Skip-Gram

**Skip-Gram** — почти обратная задача.

### Идея

- Берём **один центральный токен** $w_t$.
- Пытаемся предсказать **каждое слово контекста** вокруг него.

Схематично:

1. Вход: one-hot/embedding $v_{w_t}$.
2. Выход: несколько задач вида  
   $$
   P(w_{t+j} \mid w_t),\quad j \in [-k, k], j \neq 0
   $$
3. На практике это делается через negative sampling: обучаем бинарный классификатор  
   «(центральное слово, контекстное слово) — реальная пара или случайная?».

### Свойства

- Один вход → много пар $(w_\text{center}, w_\text{context})$.
- **Медленнее**, т.к. на каждый токен больше апдейтов.
- Лучше учит вектора **редких слов**:
  - каждое встреченное слово используется как «центр» и генерирует много контекстных примеров.

---

## 4. CBOW vs Skip-Gram — главное отличие

**Кратко:**

- **CBOW**: контекст → слово.  
  Быстрее, сглаживает, лучше для частых слов.
- **Skip-Gram**: слово → контекст.  
  Медленнее, но лучше для редких слов и более детальных связей.

Короткая формулировка:

> CBOW предсказывает слово по окрестности, Skip-Gram — окрестность по слову.  
> CBOW быстрее и хорош для частых слов, Skip-Gram даёт более качественные эмбеддинги редким словам, но дороже по вычислениям.

---

## 5. GloVe: чем принципиально отличается

**GloVe (Global Vectors)** — не прогностическая, а **count-based** модель.

### Идея

- Берём **матрицу совместных встреч** слов:
  - $X_{ij}$ — сколько раз слово $j$ встречается в контексте слова $i$.
- Ищем такие вектора $w_i$ и $\tilde w_j$, чтобы:
  $$
  w_i^T \tilde w_j + b_i + \tilde b_j \approx \log X_{ij}
  $$

Оптимизируем функцию:

$$
J = \sum_{i,j} f(X_{ij}) \left(w_i^T \tilde w_j + b_i + \tilde b_j - \log X_{ij} \right)^2
$$

где $f(X_{ij})$ — весовая функция, уменьшающая вклад очень редких и очень частых пар.

### Интуиция

- GloVe использует **глобальную статистику корпуса**:
  - не только локальное окно, но *все* co-occurrence counts.
- Важны **отношения** частот, типа:
  - «ice» чаще рядом с «cold», чем с «hot»,
  - «water» встречается и с тем, и с другим.
- Логарифм совместной частоты и её отношения несут семантику.

### Отличия от Word2Vec

- **Word2Vec**:
  - обучается по **локальным** примерам (окно вокруг слова),
  - оптимизирует задачу **предсказания**.
- **GloVe**:
  - фактически делает **факторизацию матрицы co-occurrence** (но с умным лоссом),
  - использует **глобальную** статистику сразу.

Формулировка:

> Word2Vec — прогностическая модель: предсказываем контекст или слово.  
> GloVe — факторизационная: берём матрицу совместных частот и подгоняем эмбеддинги так, чтобы их скалярное произведение аппроксимировало $\log X_{ij}$.  
> Word2Vec опирается на локальные окна, GloVe — на глобальные статистики.

---

## 6. FastText: как он связан с Word2Vec?

FastText — **расширение Word2Vec** (и CBOW, и Skip-Gram) на **subword-уровень**.

### Что делает FastText

- Каждое слово представляется как **набор символьных n-грамм**:
  - например, `"<apple>"` → `"<ap"`, `"app"`, `"ppl"`, `"ple"`, `"le>"`, …
- Для каждого n-грамма есть свой embedding.
- Эмбеддинг слова:
  $$
  v_\text{word} = \sum_{g \in G(\text{word})} z_g
  $$
  где $G(\text{word})$ — множество n-грамм, $z_g$ — их вектора.

Дальше:

- **Задача предсказания** остаётся Word2Vec-подобной:
  - для эмбеддингов — либо CBOW, либо Skip-Gram-лосс.
- Архитектура та же, но вместо «вектора слова» → «вектор как композиция subword-векторов».

### Зачем это нужно

- Поддержка **OOV** слов:
  - можно составить embedding из их n-грамм.
- Лучшая работа на **морфологически богатых языках**:
  - корни, приставки, суффиксы автоматически учитываются (для русского это особенно полезно).

### Связь с CBOW

На вопрос: *«FastText — аналог W2V CBOW на subwords?»*

- По духу — да: это **Word2Vec-стиль** (CBOW/Skip-Gram), но на уровне subwords.
- Но:
  - FastText реализует **и Skip-Gram, и CBOW**;
  - в unsupervised-режиме эмбеддингов его чаще используют в варианте **Skip-Gram**.

Формулировка:

> FastText — это Word2Vec (CBOW/Skip-Gram), где вектор слова — сумма векторов его символьных n-грамм.  
> Архитектура и лосс как у Word2Vec, отличие только в представлении слов через subwords.

---

## 7. Мини-резюме

- **CBOW**: контекст → слово, быстро, сглаживает, хорошо для частых слов.
- **Skip-Gram**: слово → контекст, дороже, но лучше для редких слов и более тонких связей.
- **GloVe**: факторизация глобальной матрицы co-occurrence, аппроксимирует $\log X_{ij}$ скалярным произведением эмбеддингов.
- **FastText**: Word2Vec-подобная модель (CBOW/Skip-Gram), но embedding слова = композиция эмбеддингов символьных n-грамм → лучше для морфологии и OOV.
