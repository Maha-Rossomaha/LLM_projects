# FastText 

## Что такое FastText?
**FastText** — это метод векторизации слов и текстов, предложенный Facebook AI Research в 2016 году, который улучшает word2vec за счёт использования **subword-информации (n-грамм)**. Он также эффективен как инструмент для **текстовой классификации**.

Основные статьи:
- [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759) — классификация
- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) — эмбеддинги слов

## Зачем нужен FastText?
- Повышает устойчивость эмбеддингов к **редким и неизвестным словам** (через subwords)
- Быстрее и проще в обучении, чем сложные нейросети
- Работает в **мультиязычной среде**, не требует токенизации на уровне словаря
- Отлично подходит для классификации коротких текстов и задач на слабых устройствах

## Основная идея
В отличие от word2vec, где каждому слову соответствует один вектор, в **FastText слово разбивается на символные n-граммы**, и его вектор — это сумма векторов этих n-грамм:

$$
\vec{w} = \sum_{g \, \in \, G(w)} \vec{g}
$$

Где:
- $w$ — слово
- $G(w)$ — множество n-грамм, из которых состоит слово (например, `<pla`, `lay`, `aye`, `yer`, `yer>` для `player`)
- $\vec{g}$ — вектор соответствующего n-грамма

Это даёт обобщающую способность: **незнакомое слово** можно представить через его n-граммы.

## Архитектура классификации (Bag of Tricks)
FastText для классификации — это модель вида:
1. Представление текста: усреднение (или сумма) эмбеддингов слов/грамм
2. Линейный классификатор (softmax или hierarchical softmax)

Очень быстро обучается и работает:
- Без рекуррентных слоёв
- Без attention
- Только embedding + линейный слой

## Трюки, используемые FastText:
1. **Subword n-grams** — устойчивость к rare words
2. **Hierarchical softmax** — ускорение обучения при большом числе классов
3. **Negative sampling** — альтернатива softmax
4. **Quantization** — уменьшение размера модели до мегабайт

## Качество и производительность
- На задачах классификации (AG News, Yelp, Amazon, DBpedia) FastText показывает **высокую точность**, сопоставимую с RNN и CNN
- При этом **обучается в десятки раз быстрее**
- Хорошо масштабируется на большие корпус и количество классов

## Use-cases
- Быстрая текстовая классификация (short texts, title классификация)
- Эмбеддинги слов в мультиязычном пространстве
- Встраивание в мобильные приложения
- Простой baseline перед использованием LLM

## Инструменты
- [Официальная библиотека fastText (C++)](https://github.com/facebookresearch/fastText)
- Python-обёртка: `pip install fasttext`
- Предобученные эмбеддинги на >150 языках: https://fasttext.cc

## Сравнение с другими методами
| Метод      | Контекст | Subwords | Скорость | Качество | Размер модели |
|------------|----------|----------|----------|----------|----------------|
| word2vec   | нет      | нет      | быстро   | средне   | компактный     |
| GloVe      | нет      | нет      | быстро   | средне   | компактный     |
| fastText   | нет      | да       | быстро   | хорошее  | компактный     |
| BERT       | да       | частично | медленно | лучшее   | тяжёлый        |

## Вывод
FastText — это **лёгкий, быстрый и мощный** инструмент для эмбеддинга и классификации. Он отлично подходит для:
- быстрого прототипирования,
- low-resource систем,
- и задач, где важна интерпретируемость и скорость.

Хотя он уступает LLM в гибкости и контексту, **его производительность и простота делают его идеальной первой линией атаки в NLP-задачах.**

