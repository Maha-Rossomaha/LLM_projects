# –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ Transformer

URL:   
üîó [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/) 

## 1. –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –ø—Ä–æ–µ–∫—Ü–∏–∏ Q, K, V

–ü—É—Å—Ç—å $X \in \mathbb{R}^{n \times d_{\mathrm{model}}}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã $n$, –≥–¥–µ $d_{\mathrm{model}}$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ú–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–æ–≤ $Q$, –∫–ª—é—á–µ–π $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏–π $V$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∫–∞–∫:
$$
Q = X W^Q,\quad
K = X W^K,\quad
V = X W^V,
$$
–≥–¥–µ $W^Q, W^K, W^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$, –∞ $d_k$ –∑–∞–¥–∞—ë—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π. –û–±–æ–∑–Ω–∞—á–∏–º $d_v = d_k$.

## 2. Scaled Dot-Product Attention

–î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:
$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}}\Bigr)\,V.
$$
–î–µ–ª–µ–Ω–∏–µ –Ω–∞ $\sqrt{d_k}$ –Ω–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Å–¥–≤–∏–≥ softmax –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π.

## 3. Masked Self-Attention –≤ –¥–µ–∫–æ–¥–µ—Ä–µ

–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è ¬´–≥–ª—è–¥–µ–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ¬ª –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Å–∫–∞ $M \in \{0, -\infty\}^{n \times n}$:
$$
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}} + M\Bigr)\,V,
$$
–≥–¥–µ $M_{i,j} = -\infty$ –ø—Ä–∏ $j > i$, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

## 4. Multi-Head Attention

–ú–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ $H$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤:
$$
\mathrm{head}_h = \mathrm{Attention}(Q W_h^Q,\;K W_h^K,\;V W_h^V),\quad h=1,\dots,H,
$$
$$
\mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_H)\,W^O,
$$
–≥–¥–µ $W_h^Q, W_h^K, W_h^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$ –∏ $W^O \in \mathbb{R}^{H d_k \times d_{\mathrm{model}}}$.

## 5. Cross-Attention (Encoder‚ÄìDecoder Attention)

–í –±–ª–æ–∫–µ –≤–Ω–∏–º–∞–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä –∑–∞–ø—Ä–æ—Å—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–µ–∫–æ–¥–µ—Ä–∞ $X_{dec}$, –∞ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –∏–∑ –≤—ã—Ö–æ–¥–æ–≤ —ç–Ω–∫–æ–¥–µ—Ä–∞ $X_{enc}$:
$$
Q = X_{dec} W^Q,\quad
K = X_{enc} W^K,\quad
V = X_{enc} W^V.
$$

$$
\mathrm{CrossAttention}(Q, K, V) 
= \mathrm{softmax}\!\Bigl(\frac{QK^\top}{\sqrt{d_k}}\Bigr)\,V.
$$

## 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ—è Transformer

–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
1. **Multi-Head Attention** —Å residual connection –∏ –ø—Ä–µ–¥- –∏–ª–∏ –ø–æ—Å—Ç-LayerNorm.
2. **Position-wise Feed-Forward Network**:
$$
\mathrm{FFN}(x) = \mathrm{GELU}(x W_1 + b_1)\,W_2 + b_2.
$$
3. **LayerNorm** –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.