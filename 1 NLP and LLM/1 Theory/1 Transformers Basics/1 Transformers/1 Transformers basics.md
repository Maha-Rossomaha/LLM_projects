# –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ Transformer

URL:   
üîó [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/) 

## 1. –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –ø—Ä–æ–µ–∫—Ü–∏–∏ Q, K, V

–ü—É—Å—Ç—å $X \in \mathbb{R}^{n \times d_{\mathrm{model}}}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã $n$, –≥–¥–µ $d_{\mathrm{model}}$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ú–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–æ–≤ $Q$, –∫–ª—é—á–µ–π $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏–π $V$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∫–∞–∫:
$$
Q = X W^Q,\quad
K = X W^K,\quad
V = X W^V,
$$
–≥–¥–µ $W^Q, W^K, W^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$, –∞ $d_k$ –∑–∞–¥–∞—ë—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π. –û–±–æ–∑–Ω–∞—á–∏–º $d_v = d_k$.

–ï—Å–ª–∏ –Ω–µ –¥–µ–ª–∏—Ç—å, —Å–∫–∞–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è $q_i\cdot k_j$ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º–∏ –ø—Ä–∏ —Ä–æ—Å—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_k$.
- –ü—É—Å—Ç—å $q_i, k_j \sim N(0, 1)$
- –¢–æ–≥–¥–∞:
$$
\mathbb{E}[q_i \cdot k_j] = 0, \quad \mathrm{Var}[q_i \cdot k_j] = d_k
$$
–¢–æ –µ—Å—Ç—å —Ä–∞–∑–±—Ä–æ—Å –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞—Å—Ç—ë—Ç –ª–∏–Ω–µ–π–Ω–æ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é.  

–ï—Å–ª–∏ –ø–æ–¥–µ–ª–∏–º –Ω–∞ $\sqrt{d_k}$, —Ç–æ –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ–ª—É—á–∏—Ç—Å—è:
$$
    \mathrm{Var}\left[\frac{q_i \cdot k_j}{\sqrt{d_k}}\right] = 1
$$
–≠—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.  
–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:
- Softmax –Ω–µ –≤—ã—Ä–æ–∂–¥–∞–µ—Ç—Å—è –≤ 0 –∏–ª–∏ 1
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –∏—Å—á–µ–∑–∞—é—Ç –∏ –Ω–µ –≤–∑—Ä—ã–≤–∞—é—Ç—Å—è
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –ø—Ä–∏ –ª—é–±–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_k$


## 2. Scaled Dot-Product Attention

–î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:
$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}}\Bigr)\,V.
$$
–î–µ–ª–µ–Ω–∏–µ –Ω–∞ $\sqrt{d_k}$ –Ω–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Å–¥–≤–∏–≥ softmax –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π.

## 3. Masked Self-Attention –≤ –¥–µ–∫–æ–¥–µ—Ä–µ

–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è ¬´–≥–ª—è–¥–µ–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ¬ª –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Å–∫–∞ $M \in \{0, -\infty\}^{n \times n}$:
$$
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}} + M\Bigr)\,V,
$$
–≥–¥–µ $M_{i,j} = -\infty$ –ø—Ä–∏ $j > i$, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

## 4. Multi-Head Attention

Multi-head ‚Äî —ç—Ç–æ –∞–Ω—Å–∞–º–±–ª—å –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º, —Å learnable redundancy (–º–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –¥–ª—è –Ω–µ–µ –≤–∞–∂–Ω–æ, –∞ —á—Ç–æ –∏–∑–ª–∏—à–Ω–µ).  
–ú–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ $H$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤:
$$
\mathrm{head}_h = \mathrm{Attention}(Q W_h^Q,\;K W_h^K,\;V W_h^V),\quad h=1,\dots,H,
$$
$$
\mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_H)\,W^O,
$$
–≥–¥–µ $W_h^Q, W_h^K, W_h^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$ –∏ $W^O \in \mathbb{R}^{H d_k \times d_{\mathrm{model}}}$. 

> –ú—ã –¥–µ–ª–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_{model}$ –Ω–∞ $h$ –≥–æ–ª–æ–≤. –≠—Ç–æ —Å–¥–µ–ª–∞–Ω–æ —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—â—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥–µ: –µ—Å–ª–∏ –±—ã –∫–∞–∂–¥–∞—è head –∏–º–µ–ª–∞ $d_k=d_{model}$, –∏—Ç–æ–≥–æ–≤–∞—è concat-–≥–æ–ª–æ–≤–∞ –±—ã–ª–∞ –±—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $h\cdot d_{model}$, –∏ –ª–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å—Ç–∞–ª–∞ –±—ã –Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω–æ –æ–≥—Ä–æ–º–Ω–æ–π.

> –†–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö: —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫–∞, —Å—Ç–∏–ª—å, –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è ‚Üí coreference, –¥—Ä—É–≥–∞—è ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º).

### 4.1 –ü–æ—á–µ–º—É —ç—Ç–æ –ª—É—á—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –±–æ–ª—å—à–æ–π attention
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö heads:
1. –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è  
   –ö–∞–∂–¥–∞—è head –º–æ–∂–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö:
   - –æ–¥–Ω–∞ ‚Äî –Ω–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–µ,
   - –¥—Ä—É–≥–∞—è ‚Äî –Ω–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö,
   - —Ç—Ä–µ—Ç—å—è ‚Äî –Ω–∞ –ø–∞—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö (–≤ –∫–æ–¥–µ), –∏ —Ç.–¥.
2. –†–∞–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
   - Heads –æ–±—É—á–∞—é—Ç—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏—è—Ö, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ–∫—É—Å–æ–≤.
   - –≠—Ç–æ –∫–∞–∫ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã—Ö "—É–≥–ª–æ–≤ –∑—Ä–µ–Ω–∏—è"
3. –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –ù–µ—Å–∫–æ–ª—å–∫–æ ‚Äú—É–∑–∫–∏—Ö‚Äù attention —Å–ª–æ—ë–≤ (—Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ –ø–æ –ø–∞–º—è—Ç–∏, —á–µ–º –æ–¥–Ω–∞ –±–æ–ª—å—à–∞—è ‚Äú–∂–∏—Ä–Ω–∞—è‚Äù –º–∞—Ç—Ä–∏—Ü–∞ c $d_{model}$

### 4.2 –ü—Ä–æ–±–ª–µ–º–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–æ–ª–æ–≤
1. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –≥–æ–ª–æ–≤ —É–º–µ–Ω—å—à–∞–µ—Ç $d_k$, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—é –µ–º–∫–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏, –∏–∑-–∑–∞ —á–µ–≥–æ –≥–æ–ª–æ–≤—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ—Ä–∞–∑–ª–∏—á–∏–º—ã–º–∏. 
2. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è, –¥—Ä—É–≥–∞—è —á–∞—Å—Ç—å —É–º–∏—Ä–∞–µ—Ç
    - –î–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Ä—Ç–≤—ã—Ö –≥–æ–ª–æ–≤ –º–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è head sparsity, head importance metrics
    - –ê–Ω–∞–ª–∏–∑ attention maps, DropHead, regularization –¥–ª—è diversity attention, L0/L1


## 5. Cross-Attention (Encoder‚ÄìDecoder Attention)

–í –±–ª–æ–∫–µ –≤–Ω–∏–º–∞–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä –∑–∞–ø—Ä–æ—Å—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–µ–∫–æ–¥–µ—Ä–∞ $X_{dec}$, –∞ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –∏–∑ –≤—ã—Ö–æ–¥–æ–≤ —ç–Ω–∫–æ–¥–µ—Ä–∞ $X_{enc}$:
$$
Q = X_{dec} W^Q,\quad
K = X_{enc} W^K,\quad
V = X_{enc} W^V.
$$

$$
\mathrm{CrossAttention}(Q, K, V) 
= \mathrm{softmax}\!\Bigl(\frac{QK^\top}{\sqrt{d_k}}\Bigr)\,V.
$$

## 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ—è Transformer

–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
1. **Multi-Head Attention** —Å residual connection –∏ –ø—Ä–µ–¥- –∏–ª–∏ –ø–æ—Å—Ç-LayerNorm.
2. **Position-wise Feed-Forward Network**:
$$
\mathrm{FFN}(x) = \mathrm{GELU}(x W_1 + b_1)\,W_2 + b_2.
$$
3. **LayerNorm** –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.