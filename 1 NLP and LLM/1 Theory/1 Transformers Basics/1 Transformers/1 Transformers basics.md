# –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ Transformer

URL:   
üîó [Transformers illustrated](https://jalammar.github.io/illustrated-transformer/) 

## 1. –í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –ø—Ä–æ–µ–∫—Ü–∏–∏ Q, K, V

–ü—É—Å—Ç—å $X \in \mathbb{R}^{n \times d_{\mathrm{model}}}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã $n$, –≥–¥–µ $d_{\mathrm{model}}$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ú–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–æ–≤ $Q$, –∫–ª—é—á–µ–π $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏–π $V$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∫–∞–∫:
$$
Q = X W^Q,\quad
K = X W^K,\quad
V = X W^V,
$$
–≥–¥–µ $W^Q, W^K, W^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$, –∞ $d_k$ –∑–∞–¥–∞—ë—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π. –û–±–æ–∑–Ω–∞—á–∏–º $d_v = d_k$.

**–ü—É—Å—Ç—å:**
- $d_\mathrm{model}=512$ - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- $h=8$ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
- $d_k = d_\mathrm{model} / h = 64$ - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Key, Quwry –≤ –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤–µ
- $Q$ –∏ $K$ - –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã —Å 
   - –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–∂–∏–¥–∞–Ω–∏–µ–º: $E[x] = 0$
   - –î–∏—Å–ø–µ—Ä—Å–∏–µ–π: $Var[x]=1$
   
**–¢–æ–≥–¥–∞:**  
–í–æ–∑—å–º–µ–º –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ $q \in R^{64}$ –∏ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –∫–ª—é—á–∞ $k \in R^{64}$. 
- **–°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ:** $q\cdot k = \sum_{i=1}^{64} q_i \cdot k_i$.  
- **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ:** $E[q\cdot k] = E[\sum q_i\cdot k_i] = \sum E[q_i \cdot k_i]$. 
   - $q_i$ –∏ $k_i$ - –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –∏ —Å –Ω—É–ª–µ–≤—ã–º —Å—Ä–µ–¥–Ω–∏–º $\rightarrow$ $E[q_i \cdot k_i] = E[q_i] \cdot E[k_i] = 0\cdot 0 = 0$
- **–î–∏—Å–ø–µ—Ä—Å–∏—è:** $Var[q\cdot] = Var[\sum q_i \cdot k_i]$. 
   - –¢–∞–∫ –∫–∞–∫ $q_i$ –∏ $k_i$ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã $\rightarrow$ $Var[q_i \cdot k_i] = E[(q_i\cdot k_i)^2] - (E[q_i\cdot k_i])^2 = E[q_i^2] \cdot E[k_i^2] - 0$.
   - –¢–∞–∫ –∫–∞–∫ $Var[x] = E[x^2] - (E[x])^2 = E[x^2] - 0 = E[x2]$, —Ç–æ:
      - $E[q_i^2] = Var[q_i] = 1$
      - $E[k_i^2] = Var[k_i] = 1$
   - –ò–∑ —ç—Ç–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç: $Var[q_i \cdot k_i] = 1 \cdot 1 = 1$
   - –ü–æ—Å–∫–æ–ª—å–∫—É —Ç—É—Ç 64 –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–∞–≥–∞–µ–º—ã—Ö: $Var[q¬∑k] = \sum Var[q_i \cdot k_i] = 64 \cdot 1 = 64$

**–ü—Ä–æ–±–ª–µ–º–∞:**  
–î–∏—Å–ø–µ—Ä—Å–∏—è —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è $q\cdot$ k —Ä–∞–≤–Ω–∞ $d_k = 64$.

–ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º softmax: $softmax(q\cdot k) = exp(q\cdot k) / \sum exp(q\cdot k)$, –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (—Å –±–æ–ª—å—à–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π) –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –æ—á–µ–Ω—å –æ—Å—Ç—Ä—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º, –≥–¥–µ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –∏–º–µ–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ—á—Ç–∏ 1, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ—á—Ç–∏ 0. –≠—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, —Ç–∞–∫ –∫–∞–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏

**–†–µ—à–µ–Ω–∏–µ:**  
–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Ç–∞–∫, —á—Ç–æ–±—ã –¥–∏—Å–ø–µ—Ä—Å–∏—è —Å–Ω–æ–≤–∞ —Å—Ç–∞–ª–∞ —Ä–∞–≤–Ω–∞ 1.
- **–î–µ–ª–∏–º –Ω–∞ $\sqrt{d_k}:$** $\text{scaled\_score} = (q\cdot k) / \sqrt{d_k}$
- **–ü—Ä–æ–≤–µ—Ä–∏–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è:** 
$$
Var[\text{scaled\_score}] = Var[(q\cdot k) / \sqrt{d_k}] = (1/d_k) \cdot Var[q\cdot k] = (1/64) \cdot 64 = 1
$$

–≠—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –≤–Ω–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.  
–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ:
- Softmax –Ω–µ –≤—ã—Ä–æ–∂–¥–∞–µ—Ç—Å—è –≤ 0 –∏–ª–∏ 1
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ "—Ä–∞–∑–º–∞–∑–∞–Ω–Ω—ã–º" –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –∏—Å—á–µ–∑–∞—é—Ç –∏ –Ω–µ –≤–∑—Ä—ã–≤–∞—é—Ç—Å—è
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –ø—Ä–∏ –ª—é–±–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_k$


## 2. Scaled Dot-Product Attention

–î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:
$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}}\Bigr)\,V.
$$
–î–µ–ª–µ–Ω–∏–µ –Ω–∞ $\sqrt{d_k}$ –Ω–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Å–¥–≤–∏–≥ softmax –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π.

## 3. Masked Self-Attention –≤ –¥–µ–∫–æ–¥–µ—Ä–µ

–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è ¬´–≥–ª—è–¥–µ–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ¬ª –≤–≤–æ–¥–∏—Ç—Å—è –º–∞—Å–∫–∞ $M \in \{0, -\infty\}^{n \times n}$:
$$
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\Bigl(\frac{Q K^\top}{\sqrt{d_k}} + M\Bigr)\,V,
$$
–≥–¥–µ $M_{i,j} = -\infty$ –ø—Ä–∏ $j > i$, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

## 4. Multi-Head Attention

Multi-head ‚Äî —ç—Ç–æ –∞–Ω—Å–∞–º–±–ª—å –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º, —Å learnable redundancy (–º–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –¥–ª—è –Ω–µ–µ –≤–∞–∂–Ω–æ, –∞ —á—Ç–æ –∏–∑–ª–∏—à–Ω–µ).  
–ú–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ $H$ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤:
$$
\mathrm{head}_h = \mathrm{Attention}(Q W_h^Q,\;K W_h^K,\;V W_h^V),\quad h=1,\dots,H,
$$
$$
\mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_H)\,W^O,
$$
–≥–¥–µ $W_h^Q, W_h^K, W_h^V \in \mathbb{R}^{d_{\mathrm{model}} \times d_k}$ –∏ $W^O \in \mathbb{R}^{H d_k \times d_{\mathrm{model}}}$. 

> –ú—ã –¥–µ–ª–∏–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_{model}$ –Ω–∞ $h$ –≥–æ–ª–æ–≤. –≠—Ç–æ —Å–¥–µ–ª–∞–Ω–æ —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—â—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—Ö–æ–¥–µ: –µ—Å–ª–∏ –±—ã –∫–∞–∂–¥–∞—è head –∏–º–µ–ª–∞ $d_k=d_{model}$, –∏—Ç–æ–≥–æ–≤–∞—è concat-–≥–æ–ª–æ–≤–∞ –±—ã–ª–∞ –±—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $h\cdot d_{model}$, –∏ –ª–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å—Ç–∞–ª–∞ –±—ã –Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω–æ –æ–≥—Ä–æ–º–Ω–æ–π.

> –†–∞–∑–Ω—ã–µ –≥–æ–ª–æ–≤—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö: —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫–∞, —Å—Ç–∏–ª—å, –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–¥–Ω–∞ –≥–æ–ª–æ–≤–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è ‚Üí coreference, –¥—Ä—É–≥–∞—è ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º).

### 4.1 –ü–æ—á–µ–º—É —ç—Ç–æ –ª—É—á—à–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –±–æ–ª—å—à–æ–π attention
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö heads:
1. –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è  
   –ö–∞–∂–¥–∞—è head –º–æ–∂–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö:
   - –æ–¥–Ω–∞ ‚Äî –Ω–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–µ,
   - –¥—Ä—É–≥–∞—è ‚Äî –Ω–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö,
   - —Ç—Ä–µ—Ç—å—è ‚Äî –Ω–∞ –ø–∞—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö (–≤ –∫–æ–¥–µ), –∏ —Ç.–¥.
2. –†–∞–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
   - Heads –æ–±—É—á–∞—é—Ç—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ü–∏—è—Ö, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ–∫—É—Å–æ–≤.
   - –≠—Ç–æ –∫–∞–∫ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã—Ö "—É–≥–ª–æ–≤ –∑—Ä–µ–Ω–∏—è"
3. –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   - –ù–µ—Å–∫–æ–ª—å–∫–æ ‚Äú—É–∑–∫–∏—Ö‚Äù attention —Å–ª–æ—ë–≤ (—Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏ –¥–µ—à–µ–≤–ª–µ –ø–æ –ø–∞–º—è—Ç–∏, —á–µ–º –æ–¥–Ω–∞ –±–æ–ª—å—à–∞—è ‚Äú–∂–∏—Ä–Ω–∞—è‚Äù –º–∞—Ç—Ä–∏—Ü–∞ c $d_{model}$

### 4.2 –ü—Ä–æ–±–ª–µ–º–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–æ–ª–æ–≤
1. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ –≥–æ–ª–æ–≤ —É–º–µ–Ω—å—à–∞–µ—Ç $d_k$, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—é –µ–º–∫–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏, –∏–∑-–∑–∞ —á–µ–≥–æ –≥–æ–ª–æ–≤—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ—Ä–∞–∑–ª–∏—á–∏–º—ã–º–∏. 
2. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ —á–∞—Å—Ç—å –≥–æ–ª–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è, –¥—Ä—É–≥–∞—è —á–∞—Å—Ç—å —É–º–∏—Ä–∞–µ—Ç
    - –î–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Ä—Ç–≤—ã—Ö –≥–æ–ª–æ–≤ –º–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è head sparsity, head importance metrics
    - –ê–Ω–∞–ª–∏–∑ attention maps, DropHead, regularization –¥–ª—è diversity attention, L0/L1


## 5. Cross-Attention (Encoder‚ÄìDecoder Attention)

–í –±–ª–æ–∫–µ –≤–Ω–∏–º–∞–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä –∑–∞–ø—Ä–æ—Å—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–µ–∫–æ–¥–µ—Ä–∞ $X_{dec}$, –∞ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî –∏–∑ –≤—ã—Ö–æ–¥–æ–≤ —ç–Ω–∫–æ–¥–µ—Ä–∞ $X_{enc}$:
$$
Q = X_{dec} W^Q,\quad
K = X_{enc} W^K,\quad
V = X_{enc} W^V.
$$

$$
\mathrm{CrossAttention}(Q, K, V) 
= \mathrm{softmax}\!\Bigl(\frac{QK^\top}{\sqrt{d_k}}\Bigr)\,V.
$$

## 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ—è Transformer

–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
1. **Multi-Head Attention** —Å residual connection –∏ –ø—Ä–µ–¥- –∏–ª–∏ –ø–æ—Å—Ç-LayerNorm.
2. **Position-wise Feed-Forward Network**:
$$
\mathrm{FFN}(x) = \mathrm{GELU}(x W_1 + b_1)\,W_2 + b_2.
$$
3. **LayerNorm** –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è.