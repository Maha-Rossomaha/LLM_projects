# Self-Attention from Scratch

URL:  
üîó [Transformers code](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
## 1. –ú–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å Self-Attention

1. **–í—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏**  
   –ü—É—Å—Ç—å  
   $$
   X \in \mathbb{R}^{n 	\times d_{	{model}}}
   $$  
   –≥–¥–µ $n$ ‚Äî –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, $d_{\text{model}}$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.

2. **–õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏**  
   –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤—ã—á–∏—Å–ª—è–µ–º —Ç—Ä–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è:
   $$
   Q = X W^Q,\quad
   K = X W^K,\quad
   V = X W^V,
   $$
   –≥–¥–µ  
   $$
   W^Q, W^K, W^V \in \mathbb{R}^{d_{	{model}} 	\times d_k},
   $$  
   –∞ $d_k$ ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ ¬´–∫–ª—é—á–µ–π¬ª –∏ ¬´–∑–∞–ø—Ä–æ—Å–æ–≤¬ª.  

---

## 2. Scaled Dot-Product Attention

1. **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è**  
   $$
   S = \frac{Q K^T}{\sqrt{d_k}}
   \quad\in\;\mathbb{R}^{n \times n}.
   $$

2. **Softmax –ø–æ —Å—Ç—Ä–æ–∫–∞–º**  
   $$
   A = \mathrm{softmax}(S)
   $$

3. **–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ**  
   $$
   \mathrm{Attention}(Q, K, V) = A V
   \quad\in\;\mathbb{R}^{n \times d_k}.
   $$

---

## 3. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è: ¬´loops¬ª vs –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ

- **–ù–∞–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–º–µ–¥–ª–µ–Ω–Ω–æ)**  
  ```python
  scores = torch.zeros(n, n)
  for i in range(n):
      for j in range(n):
          scores[i, j] = (Q[i] * K[j]).sum() / math.sqrt(d_k)
  A = softmax(scores, dim=-1)
  output = A @ V
  ```
- **–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (–±—ã—Å—Ç—Ä–æ)**  
  ```python
  scores = (Q @ K.T) / math.sqrt(d_k)
  A = torch.softmax(scores, dim=-1)
  output = A @ V
  ```

---

## 4. Causal Masked Self-Attention (–¥–µ–∫–æ–¥–µ—Ä)

–ß—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ ¬´–∑–∞–≥–ª—è–¥—ã–≤–∞–ª–∞ –≤ –±—É–¥—É—â–µ–µ¬ª, –≤–≤–æ–¥–∏–º –º–∞—Å–∫—É  
$$
M_{i,j} =
\begin{cases}
  0,      & j \le i,\
  -\infty,& j > i,
\end{cases}
\quad M\in\{0,-\infty\}^{n	\times n}.
$$  
–ò —Å—á–∏—Ç–∞–µ–º:
$$
\mathrm{MaskedAttention}(Q, K, V)
= \mathrm{softmax}\!\Bigl(\frac{QK^\top}{\sqrt{d_k}} + M\Bigr)\,V.
$$

---

## 5. Multi-Head Attention

1. **–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ $H$ –≥–æ–ª–æ–≤**  
   –î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã $h = 1,\dots,H$:
   $$
   Q_h = Q W_h^Q,\quad
   K_h = K W_h^K,\quad
   V_h = V W_h^V,
   $$
   –≥–¥–µ $W_h^Q, W_h^K, W_h^V \in \mathbb{R}^{d_{	{model}} \times d_k}$.

2. **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ –≥–æ–ª–æ–≤–∞–º**  
   $$
   \mathrm{head}_h = \mathrm{Attention}(Q_h, K_h, V_h)
   \quad\in\;\mathbb{R}^{n 	\times d_k}.
   $$

3. **–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è**  
   $$
   \mathrm{MultiHead}(Q, K, V)
   = \mathrm{Concat}(\mathrm{head}_1, \dots, \mathrm{head}_H) W^O,
   $$
   –≥–¥–µ  
   $$
   W^O \in \mathbb{R}^{H d_k 	\times d_{	{model}}}.
   $$

---

## 6. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è SelfAttention –≤ PyTorch

```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.Wq = nn.Linear(d_model, d_model)
        self.Wk = nn.Linear(d_model, d_model)
        self.Wv = nn.Linear(d_model, d_model)
        self.Wo = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        # x: [batch, seq_len, d_model]
        B, N, _ = x.size()
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –∏ reshape
        Q = self.Wq(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)
        K = self.Wk(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)
        V = self.Wv(x).view(B, N, self.num_heads, self.d_k).transpose(1, 2)
        # Scaled dot-product
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        A = torch.softmax(scores, dim=-1)
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç
        context = (A @ V)
        context = context.transpose(1, 2).reshape(B, N, -1)
        return self.Wo(context)
```

---

## 7. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

- **–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏** —Ü–∏–∫–ª–æ–≤ –∏ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è:

  ```python
  assert torch.allclose(scores_loop, scores_vec)
  ```

- **–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã**:

  ```python
  import seaborn as sns
  sns.heatmap(A[0, 0].detach().cpu())
  ```

---