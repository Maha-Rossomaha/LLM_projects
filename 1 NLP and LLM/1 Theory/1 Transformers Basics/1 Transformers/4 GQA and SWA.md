# Механизмы внимания: Grouped-Query Attention (GQA) и Sliding-Window Attention (SWA)

---

## 1. Grouped-Query Attention (GQA)

**GQA** — это модификация механизма multi-head attention, направленная на уменьшение вычислительной сложности и памяти, особенно при inference.

### Идея:

В обычном Multi-Head Attention у каждой головы есть своя пара (Q, K, V). В GQA **несколько query голов используют одну и ту же (K, V) пару**. То есть:

* Пусть `n_q` — число query-голов,
* `n_kv` — число групп (обычно `n_kv < n_q`),
* Каждая из `n_q` голов берёт Q из своей головы, но K и V из одной из `n_kv` общих групп.

### Зачем это нужно:

* Уменьшается количество параметров и вычислений на K и V,
* Особенно полезно при генерации токенов по одному (autoregressive inference),
* Не ухудшает качество при достаточной экспрессии в Q.

### Формально:

Если $Q \in \mathbb{R}^{T \times h_q \times d}$, $K, V \in \mathbb{R}^{T \times h_{kv} \times d}$, то attention считается так:

$$
\text{Attention}(Q_i, K_j, V_j), \quad \text{где } j = \left\lfloor i \cdot \frac{h_{kv}}{h_q} \right\rfloor
$$

---

## 2. Sliding-Window Attention (SWA)

**SWA** — техника, которая ограничивает область внимания соседними токенами, эффективно снижая вычислительную сложность с $O(n^2)$ до $O(nw)$, где $w$ — ширина окна.

### Идея:

* Вместо того, чтобы каждый токен смотрел на все предыдущие (как в causal attention), он смотрит только в окно фиксированной длины назад.
* Например, при $w=128$, токен на позиции $t$ может видеть токены $[t-w, ..., t-1]$.

### Преимущества:

* Линейное время по длине контекста,
* Эксплуатирует локальность: часто важная информация близко,
* Применяется в моделях вроде Longformer, BigBird, RWKV (частично).

### Ограничения:

* Модель не видит удалённый контекст напрямую,
* Нужно дополнять другими механизмами (global tokens, dilated windows) для захвата дальних зависимостей.

### Формально:

Для каждой позиции $t$ attention считается по:

$$
A_t = \text{softmax}\left(\frac{Q_t K_{[t-w:t]}^T}{\sqrt{d}}\right) V_{[t-w:t]}
$$

---

## Сравнение

| Характеристика         | GQA                                 | SWA                             |
| ---------------------- | ----------------------------------- | ------------------------------- |
| Тип оптимизации        | Архитектурная, параметрическая      | Вычислительная, срез по времени |
| Цель                   | Уменьшить количество (K, V)         | Уменьшить сложность attention   |
| Поддержка глобальности | Да                                  | Нет (без дополнений)            |
| Используется в         | LLaMA 2, Mistral, Mixtral           | Longformer, BigBird, RWKV       |
| Зависимость от длины   | Полная (можно видеть весь контекст) | Ограниченное окно               |

---

## Заключение

GQA и SWA — два разных подхода к оптимизации внимания. GQA сохраняет глобальное внимание, но сокращает объём KV. SWA же агрессивно ограничивает область внимания для ускорения и масштабирования. Часто они комбинируются с другими приёмами (FlashAttention, rotary embeddings, KV-кэш) для достижения баланса между скоростью и качеством.
