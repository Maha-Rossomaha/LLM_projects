## Residual Connections в Transformer

### Что такое Residual Connection?

Residual connection — это механизм прямого проброса входа `x` через нелинейный блок `F(x)`, то есть:

$$
\text{Output} = x + F(x)
$$

В Transformer-блоке их **два**:

* После **Self-Attention**
* После **Feedforward** слоя (FFN)

Они позволяют сохранить исходную информацию и стабильные градиенты.

---

### Где находятся Residual Connections?

В каждом Transformer-блоке:

**Post-LN (оригинальный Transformer):**

```text
x → Sublayer → x + Sublayer(x) → LayerNorm
```

**Pre-LN (современные модели):**

```text
x → LayerNorm → Sublayer(x) → x + Sublayer(x)
```

---

### Зачем они нужны?

1. **Стабильность градиентов**: residual path обеспечивает градиент $\frac{dL}{dx} = 1$ в простом случае, не давая ему затухнуть.
2. **Сохранение сигнала**: модель может передавать полезную информацию напрямую, минуя сложные преобразования.
3. **Feature reuse**: помогает переиспользовать ранее выученные представления.

---

### Что будет без residual connection?

* Градиенты **затухают** особенно при глубине 24+ слоёв
* **LayerNorm** снижает variance → градиенты становятся малы
* Модель **не обучается** без агрессивного warmup и других костылей

---

### Как residual влияет на forward и backward pass?

* **Forward**: сохраняет variance, передаёт исходный сигнал
* **Backward**: создаёт стабильный путь градиента (identity path)

$$
\frac{dL}{dx} = \frac{dL}{d(x + F(x))} = 1 + \frac{dL}{dF(x)} \cdot F'(x)
$$

---

### Что если складывать без нормализации?

* **Variance будет расти** со слоями
* **Среднее сместится**, особенно если добавляются несбалансированные векторы
* Может привести к **взрыву активаций**

Поэтому нормализация (LayerNorm или RMSNorm) — обязательна.

---

### Когда residual может мешать?

* В **multitask fine-tuning**: старые задачи продолжают протекать через residual path и мешают адаптации
* В **Adapter-тюнинге**: adapter может не успевать компенсировать основной сигнал
* В **low-rank adaptation (LoRA)**: слишком сильный residual мешает дельте влиять на выход

---

### Модификации Residual:

* **Weighted residual**: $x + \alpha \cdot F(x)$
* **Gated residual**: $x + \sigma(W_1x) \cdot F(x)$
* **Residual dropout/rescale**: $x + \text{dropout}(F(x))$

---

### Post-LN vs Pre-LN

|                 | Post-LN                      | Pre-LN                               |
| --------------- | ---------------------------- | ------------------------------------ |
| LayerNorm стоит | После сложения               | До sublayer                          |
| Градиент        | Затухает через LN            | Проходит напрямую через skip         |
| Стабильность    | Плохо при глубине > 12 слоёв | Отлично работает даже при 100+ слоях |
| Требует warmup  | Да                         | Нет                                |

**Pre-LN** сейчас используется почти во всех LLM: GPT, LLaMA, T5 и т.д.

---

### Вывод

Residual connection — критически важный компонент стабильного обучения в Transformer.

* Без него: сеть деградирует, градиенты исчезают
* С ним: модель обучается стабильно и масштабируется вглубь
* Pre-LN residual более устойчив и масштабируем, чем Post-LN
