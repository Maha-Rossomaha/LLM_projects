## Конспект: WordPiece токенизатор — устройство, суть и реализация на Python

**Краткое содержание:** WordPiece — алгоритм субсловной токенизации, разработанный Google для BERT и используемый в ряде трансформеров. Он сочетает статистику частот и условные вероятности для выбора слияний, поддерживает редкие слова и обеспечивает эффективную лексемизацию. В конспекте рассмотрены архитектурные блоки, процедура обучения и примеры реализации на Python.

## 1. Обзор WordPiece

### 1.1. Истоки и назначение

WordPiece был впервые применён Google для токенизации в модели BERT, чтобы справляться с OOV-словами и снизить размер словаря при сохранении качества представления текста. Алгоритм изначально создавался для голосового поиска на японском и корейском языках, позже оказался эффективен для любых языков.

### 1.2. Отличие от BPE

WordPiece близок к BPE, но при выборе пары для слияния учитывает не только частоту, но и изменение общей вероятности корпуса под условной языковой моделью, что даёт более информативные подслова.

## 2. Архитектура токенизатора

### 2.1. Нормализация и предтокенизация

Входной текст сначала нормализуется (Unicode NFC/NFKC, приведение регистра) и разбивается на базовые слова или символы с помощью rule-based предтокенизатора.

### 2.2. Модель словаря (Vocabulary Model)

WordPiece хранит статический словарь подслов с их идентификаторами. При токенизации отдельного слова применяется жадный алгоритм longest-match-first — выбирается самая длинная префиксная подстрока, присутствующая в словаре.

### 2.3. Пост-обработка и декодинг

После сегментации слов вставляются специальные токены (,  и т. п.), а при обратном преобразовании проводится merge подслов и удаление служебных символов (например ‘##’).

## 3. Алгоритм обучения WordPiece

### 3.1. Инициализация

Словарь начинается с набора единичных символов или токенов, полученных после предтокенизации корпуса.

### 3.2. Подсчёт условных вероятностей

Для каждой возможной пары подслов вычисляется оценка выигрыша в лог-лиikelihood при их слиянии, основанная на условной языковой модели unigram или bigram.

### 3.3. Выбор и слияние

На каждом шаге выбирается пара с максимальным приростом вероятности, она объединяется в новый токен, а частоты и вероятности пересчитываются до достижения заданного размера словаря.

## 4. Пример реализации на Python

### 4.1. Быстрая токенизация с Transformers

```python
from transformers import AutoTokenizer

# Загрузка предобученного WordPiece токенизатора BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")  # поддерживает WordPiece

# Токенизация строки
tokens = tokenizer.tokenize("Example sentence for WordPiece.")
print(tokens)

# Получение идентификаторов
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(token_ids)
```

*AutoTokenizer автоматически подгружает WordPiece модель и оптимизирован для высокой скорости.*

### 4.2. Построение кастомного WordPiece через Tokenizers Library

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.trainers import WordPieceTrainer
from tokenizers.pre_tokenizers import Whitespace

# 1. Инициализация модели WordPiece
tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

# 2. Настройка предтокенизатора
tokenizer.pre_tokenizer = Whitespace()

# 3. Тренер для обучения словаря
trainer = WordPieceTrainer(
    vocab_size=30000,
    special_tokens=["[PAD]", "[CLS]", "[SEP]", "[UNK]", "[MASK]"]
)

# 4. Обучение на корпусе
files = ["path/to/corpus.txt"]
tokenizer.train(files, trainer)

# 5. Сохранение модели
tokenizer.save("./wordpiece-tokenizer.json")
```

*Этот подход позволяет контролировать все этапы: от предтокенизации до спецификации специальных токенов.*

## 5. Валидация и рекомендации

- Проверять, что обратная декодировка восстанавливает исходный текст без потерь.
- Анализировать распределение длин токенов и покрытие редких терминов.
- Настраивать hyperparameters (min\_frequency, vocab\_size) для баланса между granularity и скоростью инференса.