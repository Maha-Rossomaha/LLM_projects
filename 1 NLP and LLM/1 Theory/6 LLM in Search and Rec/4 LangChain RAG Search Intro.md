# LangChain Search + RAG

## 1. Мотивация

LangChain предоставляет готовые компоненты для построения Retrieval-Augmented Generation (RAG) пайплайнов, комбинируя поиск по внешнему источнику с генерацией ответа LLM. Это упрощает реализацию интеллектуальных агентов, способных отвечать на сложные вопросы с опорой на актуальные данные.

---

## 2. Основная идея RAG в LangChain

1. **Search** — нахождение релевантной информации с помощью поискового API или базы знаний.
2. **Retrieval** — преобразование результатов поиска в единый формат и отбор top-K документов.
3. **LLM Generation** — генерация ответа на основе найденного контекста.

---

## 3. Компоненты из примера

### 3.1 Источник данных

- В примере используется **TavilySearchAPIWrapper** для веб-поиска.
- Может быть заменён на любой retriever (Elasticsearch, vector DB и т.д.).

### 3.2 Интеграция поиска и LLM

- Результаты поиска объединяются в строку или список документов.
- Контекст передаётся в prompt LLM для генерации ответа.
- Используются шаблоны (PromptTemplate) для структурирования запроса.

### 3.3 Пример пайплайна

```python
from langchain_community.utilities import TavilySearchAPIWrapper
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Поиск
search = TavilySearchAPIWrapper()
results = search.run("вопрос")

# Формирование prompt
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""Ответь на вопрос, используя только следующий контекст:
{context}

Вопрос: {question}
Ответ:"""
)

# Генерация ответа
chain = LLMChain(llm=..., prompt=prompt)
answer = chain.run(context=results, question="вопрос")
```

---

## 4. Возможности кастомизации

- **Замена источника поиска**: использовать vector store retriever вместо веб-поиска.
- **Каскад retrieval**: добавлять reranking перед генерацией.
- **Post-processing**: фильтрация и нормализация результатов перед подачей в LLM.
- **Citation**: добавление ссылок на источники в финальный ответ.

---

## 5. Лучшие практики

- Ограничивать количество и длину документов, передаваемых в LLM.
- Явно указывать модели, что можно использовать только предоставленный контекст.
- Логировать поисковые запросы и ответы для последующей оптимизации.

---

## 6. Расширения

- Multi-hop RAG: последовательный поиск по нескольким вопросам.
- Memory-augmented RAG: сохранение результатов прошлых поисков.
- Dynamic context selection: выбор наиболее полезных фрагментов из top-K.
