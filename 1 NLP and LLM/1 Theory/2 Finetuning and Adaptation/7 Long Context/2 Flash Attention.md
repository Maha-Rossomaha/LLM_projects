# FlashAttention

**FlashAttention** — это высокоэффективный алгоритм вычисления softmax attention, разработанный для устранения узкого места по памяти и ускорения внимания в трансформерах. Он делает attention **точным** (не приближённым) и при этом **в 2–4 раза быстрее** и **в разы экономнее по памяти**.

---

## Проблема обычного attention

Обычная реализация attention требует:

* Вычисления матриц Q, K, V.
* Построения матрицы внимания размером $L \times L$ (где L — длина последовательности).
* Умножения этой матрицы на V: $\text{softmax}(QK^T) \cdot V$.

**Сложность по памяти** — $O(L^2)$, что делает невозможным обработку длинных последовательностей (8k, 32k, 128k токенов).

---

## Идея FlashAttention

FlashAttention переосмысляет реализацию attention-механизма, чтобы обойти два ключевых ограничения стандартного подхода:

1. Неэффективное использование GPU памяти — особенно при длинных последовательностях.
2. Избыточные обращения к global memory при промежуточных вычислениях.

### Что делает FlashAttention по-другому:

* Делит входной тензор на **чанки (blocks)** — обычно блоки размером 128–512 токенов. Деление происходит по batch × head × seq.

* В каждом блоке **attention вычисляется поэтапно**:

  * Загружается блок Q, затем поочерёдно блоки K и V.
  * Для каждой пары Q и K вычисляется логит, сразу применяется softmax, затем умножается на соответствующую часть V.
  * Результат агрегируется **на лету**, не создавая полную матрицу attention.

* Все этапы (attention → softmax → умножение на V) выполняются **в одном GPU kernel**, используя **shared memory** и **registers**:

  * Это позволяет избегать аллокации больших массивов в global memory.
  * Данные загружаются один раз, обрабатываются и сразу выгружаются в выходной тензор.

* Такой **streaming attention** позволяет:

  * Обрабатывать последовательности длиной 32k+, не превышая лимит памяти.
  * Избежать накопления $L 	\times L$ матриц и лишнего копирования между слоями.

> FlashAttention — это **точная**, а не приближённая реализация softmax(QKᵗ)V. Она даёт ту же точность, но реализуется с учётом микроархитектуры GPU (fused kernel + shared memory + memory coalescing).

---

## Плюсы

* **Сильная экономия памяти**: нет хранения всей матрицы attention.
* **Скорость**: в 2–4 раза быстрее на длинных последовательностях.
* **Точность**: численно эквивалентен обычному attention (в отличие от approximated attention).
* Поддерживает **masking**, **dropout**, **causal** режим (decoder-only).
* Совместим с **FlashAttention 2**, **Multiquery**, **rotary**, **checkpointing**.

---

## Минусы

* Требует специфических условий:

  * GPU с Ampere или новее (A100, H100, RTX 30xx/40xx, и т.п.).
  * PyTorch ≥ 1.12, CUDA ≥ 11.6.
* Может быть сложен в интеграции в произвольные кастомные модели.
* Для encoder-decoder нужна ручная поддержка cross-attention.

---

## Реализация

FlashAttention доступен как библиотека:

* [https://github.com/HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)

Пример использования с HuggingFace:

```python
from transformers import AutoModel
model = AutoModel.from_pretrained("...")
model = model.to("cuda")
model.gradient_checkpointing_enable()
model.enable_flash_attention()
```

Если используешь `flash_attn` напрямую:

```python
from flash_attn import flash_attn_unpadded
out = flash_attn_unpadded(q, k, v, dropout_p=0.0, causal=True)
```

---

## FlashAttention v2

**FlashAttention-2** — это вторая версия алгоритма, в которой были устранены ограничения и добавлены ключевые улучшения по скорости, гибкости и поддержке сложных сценариев.

### Что нового:

* **Улучшенная поддержка cross-attention**:

  * Теперь работает из коробки и с encoder-decoder архитектурами.
  * Реализованы более гибкие формы маскирования (например, separate encoder/decoder causal masks).

* **Поддержка variable-length sequences (unpadded)**:

  * Перешли к формату без паддингов, что экономит вычисления при батчинге примеров разной длины.
  * Используется специальный layout, при котором всё выравнивается в packed representation, без пустых токенов.

* **Ещё более быстрое ядро**:

  * Новый layout (в том числе для interleaved QKV) оптимизирован под tensor cores.
  * Используется fused kernel, объединяющий softmax, dropout, и умножение на V — без промежуточного сохранения.
  * Минимизируются обращения к глобальной памяти — всё держится в регистре и shared memory.

### Дополнительно:

* Поддерживается через `flash_attn_2` API (отдельный entrypoint).
* Совместим с FlashAttention-compatible конфигами моделей HuggingFace (`flash_attn_2=True` при инициализации).
* Позволяет запускать обучение с `seq_len > 64k`, если GPU и модель поддерживают соответствующую точку входа (обычно в LLaMA-style архитектурах).

> FlashAttention-2 быстрее, чем FlashAttention-1, особенно на длинных последовательностях и в сценариях inference, благодаря снижению накладных расходов и поддержке flexible input batching.
