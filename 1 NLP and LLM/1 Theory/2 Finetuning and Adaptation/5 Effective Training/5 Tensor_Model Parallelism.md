# Tensor / Model Parallelism

Когда **один слой модели не помещается даже на одну GPU**, ни DDP, ни pipeline-параллелизм не спасают. Тогда используют **tensor (или model) parallelism** — способ **разбить вычисления внутри одного слоя** между несколькими устройствами.

---

## Что это такое

Tensor Parallelism — это форма **внутрислойного параллелизма**: мы делим большие матрицы, участвующие в линейных слоях, между несколькими GPU. Например, вместо одного матричного умножения весов размером $[H \times H]$ делаем $H \times H/2$ на первой GPU и $H \times H/2$ на второй, а результат объединяем.

> Делить можно либо по **выходному измерению (output dim)** — это `ColumnParallelLinear`, либо по **входному измерению (input dim)** — это `RowParallelLinear`. В первом случае каждый GPU считает часть выходов, во втором — часть входов.

В Transformers обычно делят:

- Attention-матрицы: $W_q$, $W_k$, $W_v$, $W_o$
- FFN-слои: $W_1$, $W_2$

Используется в:

- Megatron-LM
- DeepSpeed (через `deepspeed.initialize()` с `zero+tp`)
- ColossalAI

---

## Зачем нужен tensor parallelism

- Чтобы обучать **модели с огромными слоями**, которые не помещаются в память одной GPU.
- Для **высокоэффективного использования памяти и вычислений**.
- Для **низколатентного обучения** в пределах одного хоста.

---

## Плюсы:

- **Позволяет обучать модель, у которой не вмещается даже один слой.**
- Нет недозагрузки GPU, как в pipeline — все устройства работают одновременно.
- Работает даже без необходимости microbatching.

---

## Минусы:

- Плохо масштабируется в multi-node: требует высокой пропускной способности между GPU.
- Лучше всего работает **в рамках одного хоста с NVLink / HBM**.
- Требует **кастомных слоёв** (например, `ColumnParallelLinear`, `RowParallelLinear`).
- Нужно вручную продумывать **топологию** — какие части слоя на каких устройствах.
- Если размер слоя не делится на число GPU — нужны **паддинги, маски, костыли**.
- **Softmax, LayerNorm, Attention output** часто требуют глобальной синхронизации — это дополнительная накладная операция.

---

## Как реализовать (пример на Megatron-LM)

Megatron-LM предоставляет готовые обёртки для tensor-parallel слоёв:

```python
from megatron.model import ColumnParallelLinear

# вместо обычного Linear(H, H)
fc = ColumnParallelLinear(input_size=hidden_size, output_size=hidden_size, gather_output=True)
```

Настройка топологии (TP size, PP size, DP size) задаётся через конфиг или аргументы запуска.

В DeepSpeed:

```json
"tensor_parallel": {
    "tp_size": 2
}
```

---

## Как работает forward-проход

1. Входной тензор передаётся на все GPU.
2. Каждый GPU применяет свою часть матрицы.
3. Результаты либо **конкатенируются**, либо **редуцируются** (в зависимости от направления разбиения).
4. Обратный проход аналогично требует обмена градиентами.

> Используется внутри одного слоя, в отличие от pipeline, где границы — это слои.

---

## Как TP вписывается в общую схему

TP часто комбинируется с другими формами параллелизма:

- **TP × DDP** — один слой разбит между GPU, DDP между узлами
- **TP × PP × DDP** — используется в Megatron-LM на 128+ GPU

Пример: 8 GPU = 2 TP × 2 PP × 2 DDP

---

## Сравнение с другими подходами

|                       | DDP                   | Pipeline Parallel  | Tensor Parallel            |
| --------------------- | --------------------- | ------------------ | -------------------------- |
| Разделение            | по данным (batch)     | по слоям           | внутри слоя (матрицы)      |
| Использование памяти  | дублируется           | разбито по стадиям | разбито по частям тензоров |
| Поддержка HuggingFace | отличная              | ограниченная       | почти отсутствует          |
| Подходит для          | малых/средних моделей | больших моделей    | экстремально больших       |
| Сложность реализации  | низкая                | средняя            | высокая                    |

---

## Когда использовать

- Когда **вес одного слоя > памяти GPU**.
- При наличии **высокоскоростной коммуникации** между GPU (один хост, NVLink).
- Когда вы используете Megatron, DeepSpeed или ColossalAI и готовы писать кастомные модели.

---

## Вывод

**Tensor Parallelism** — это последний рубеж, когда уже нельзя впихнуть модель даже по слоям. Даёт отличное распределение нагрузки, но требует значительных усилий по реализации и плохо переносится между машинами. На практике часто комбинируется с другими способами: TP + PP + DDP.

> Лучшее место для TP — это один сервер с 4–8 GPU и линейными слоями в 10–100+ GB.

