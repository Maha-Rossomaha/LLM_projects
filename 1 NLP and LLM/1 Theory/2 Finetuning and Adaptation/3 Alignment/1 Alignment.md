# 1 Зачем нужно согласование (alignment)

* **Проблема**: В данных для обучения языковых моделей отсутствует явный лосс на такие характеристики, как:

  * креативность,
  * правдивость,
  * безопасность,
  * корректность кода,
  * полезность ответа.

* **Цель**: Научить модель выдавать не просто вероятностно правдоподобный, а соответствующий заданным целям и нормам ответ (aligned output).

* **Мотивация**: Без согласования модель может генерировать ответы, которые формально соответствуют запросу, но при этом нарушают этические, юридические или технические ограничения.

* **Вторая причина**: Согласование также **улучшает общее качество модели**.
  Исследование OpenAI показало, что модели, обученные с помощью RLHF (в частности, PPO и PPO-ptx), стабильно выигрывают у модели, обученной только через SFT, на всех масштабах от 1.3B до 175B параметров.

## Полезные термины

1. **Pretraining** — предобучение модели на огромном, разнородном корпусе текстов. Это позволяет модели выучить общие языковые закономерности, факты и стили из больших объёмов данных без конкретной задачи.

2. **Instruction fine-tuning (IFT)** — обучение модели следовать инструкциям. В отличие от предобучения, где модель продолжает текст, здесь она учится распознавать структуру "запрос → ответ" и давать релевантный, связный отклик.

3. **Supervised fine-tuning (SFT)** — дообучение модели на размеченных примерах задач: например, Named Entity Recognition (NER), классификация, генерация ответа. Модель по-прежнему обучается предсказывать следующий токен, но теперь в рамках конкретной задачи.

> Все эти этапы используют предсказание следующего токена с функцией потерь cross-entropy.
>
> Instruction fine-tuning — частный случай SFT. Поэтому под SFT часто подразумевают обучение модели отвечать на вопросы по парам "инструкция → ответ".

## Замечание о человеческой разметке

* Давать абсолютные (точечные) оценки качества ответа трудно — разные аннотаторы могут интерпретировать критерии по-разному.
* Гораздо проще сравнивать два или более ответа между собой и выбирать лучший — **относительная оценка**.
* Такие ранжирования можно перевести в числовые значения через систему ELO или другие схемы нормализации:

  * Пример: A4 > A2 > A1 > A3 → ELO-оценки → нормализованные значения от 1.0 до 0.0
* Это позволяет собрать два типа датасетов:

  1. (вопрос, ответ, оценка)
  2. (вопрос, лучший\_ответ, худший\_ответ)

# 2 Этапы обучения согласованной модели (pipeline alignment)

### Шаг 1: Pretraining

* Обучение модели на массивном корпусе данных (до триллиона токенов).
* Данные зачастую некачественные, взяты из интернета.
* Цель — научить модель статистике языка: предсказывать следующий токен (next-token prediction).

### Шаг 2: Supervised Fine-Tuning (SFT)

* Начало этапа RLHF.
* Разметка: люди создают хорошие ответы на промпты.
* Модель дообучается на этих примерах prompt → response.
* Это формирует базовую модель, которая уже может следовать инструкциям.
* Объём данных: 10K–100K примеров.

### Шаг 3: Обучение reward-модели

* На каждый промпт генерируются несколько ответов (с помощью модели из SFT).
* Люди ранжируют ответы по качеству.
* Reward-модель обучается воспроизводить эти предпочтения.
* Объём данных: 100K–1M сравнений (prompt, лучший ответ, худший ответ).

### Шаг 4: Обучение финальной модели с помощью RL

* Reward-модель + модель из SFT используются в reinforcement learning.
* Алгоритм (обычно PPO) обучает новую модель, которая оптимизирует ожидаемое вознаграждение.
* Результат — модель, согласованная с человеческими предпочтениями (aligned model).
* Объём данных: 10K–100K промптов.

> Примеры моделей: InstructGPT, ChatGPT, Claude, StableVicuna  
> Открытые реализации: Falcon, Pythia, StableLM, Dolly-v2 и др.


# 3 Reward-модели

## 3.1 Point-wise Reward Model

* Reward-модель (RM) принимает на вход текст (например, промпт и ответ), и возвращает **одно число** — скалярную награду.
* Такой подход называется **point-wise**: каждое наблюдение обучается независимо, как в регрессии.
* Датасет содержит тройки вида: (x, y\_w, y\_l), где:

  * `x` — входной текст (промпт),
  * `y_w` — лучший ответ (winner),
  * `y_l` — худший ответ (loser).

### Обучение:

Reward-модель обучается так, чтобы вероятность предпочтения `y_w` над `y_l` была как можно выше:

$$
\mathbb{P}_\psi(y_w > y_l | x) = \frac{\exp(r_\psi(x, y_w))}{\exp(r_\psi(x, y_w)) + \exp(r_\psi(x, y_l))} = \sigma(r_\psi(x, y_w) - r_\psi(x, y_l))
$$

* Здесь `r_ψ(x, y)` — предсказанная награда,
* `σ` — сигмоида (см. график).

Это позволяет превратить относительные предпочтения в непрерывную функцию награды.

### Функция потерь:

Reward-модель обучается с помощью log-loss, где модель максимизирует вероятность правильного ранжирования:

$$
\mathcal{L}(r_\psi) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_{rm}}\left[\log \sigma(r_\psi(x, y_w) - r_\psi(x, y_l))\right]
$$

* Модель штрафуется, если присваивает худшему ответу вышее значение, чем лучшему.
* Такая форма лосса аналогична бинарной логистической регрессии на разности reward-оценок.

---

## 3.2 Pair-wise Reward Model

* Вместо предсказания скалярной награды для каждого ответа отдельно, модель сразу сравнивает **два ответа** и выбирает лучший.
* На вход она получает строку вида:

  ```
  <T> Текст запроса
  <A> Ответ X
  <B> Ответ Y
  ```
* На выходе — метка (например, `A` или `B`), указывающая, какой из ответов предпочтительнее.

### Особенности:

* Модель обучается как классификатор: задача — предсказать, какой из двух ответов лучше.
* Подходит, когда важен **выбор**, а не оценка качества в абсолютных числах.
* Часто используется, когда хочется напрямую имитировать поведение аннотаторов.

### Преимущества:

* Не требует построения шкалы наград (в отличие от point-wise подхода).
* Может быть устойчивее при шумных предпочтениях в данных.

### Недостатки:

* Для получения глобального порядка между ответами нужно делать все возможные попарные сравнения.
* Это приводит к **квадратичной сложности** по числу кандидатов и росту вычислительных затрат при большом числе ответов.

---
# 4 RL алгоритмы
### Почему нужен RL

* Мы хотим напрямую максимизировать reward:
  $\max \text{Reward}(\text{LLM.generate}(x))$

* Но возникает проблема:

  * `LLM.generate(x)` — это **недифференцируемая** операция: она выдаёт дискретные токены через сэмплирование.
  * Reward тоже определяется **на дискретных сэмплах**, а не на вероятностных распределениях.

* Поэтому стандартная градиентная оптимизация не работает.

#### Вывод:

Чтобы оптимизировать поведение модели под награду, нам нужен **усиленный способ обучения** — **reinforcement learning (RL)**.

### Дополнительно: KL-дивергенция

* Мера удалённости двух распределений $P$ и $Q$
* **Неотрицательная**: всегда $\geq 0$
* **Несимметричная**: $KL(P\|\|Q) \neq KL(Q\|\|P)$
* Интерпретируется как **дополнительная информация**, которую потребовалось бы учесть, если использовать $Q$ вместо $P$
* В случае LLM: $X$ — это токены, а $P(x), Q(x)$ — вероятности генерации токена $x$ в двух политиках

$$
KL(P \|\| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$


## 4.1 Proximal Policy Optimization (PPO)

#### Что у нас есть:

* Reward-модель $r_\phi$
* Политика (policy), инициализированная из дообученной LLM после IFT: $\pi_\theta$
* Датасет промптов $D$

#### Идея PPO:

1. Берём промпт $x \in D$
2. Генерируем для него ответ $y \sim \pi_\theta$
3. Оцениваем ответ через reward-модель: $r_\phi(x, y)$
4. Обновляем параметры $\pi_\theta$, чтобы увеличивать награду
5. При этом ограничиваем, насколько сильно политика $\pi_\theta$ может измениться — добавляем штраф за отклонение от исходной (старой) версии модели

#### Зачем это нужно:

* Без ограничений модель может **переобучиться на отдельные примеры**, теряя обобщающую способность
* Ограничение ("proximal") делает шаги обучения более **стабильными и предсказуемыми**

#### Использование KL-дивергенции
* В PPO KL-дивергенция используется как регуляризатор: она ограничивает, насколько новая политика может отклоняться от старой
* Это помогает сохранять **стабильность обучения** и предотвращает деградацию поведения модели

### Целевая функция PPO

Proximal Policy Optimization (PPO) оптимизирует следующую функцию:

$$
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D},\ y \sim \pi_\theta(y|x)} \left[ r_\phi(x, y) - \beta \cdot D_{\text{KL}}\left(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)\right) \right]
$$

#### Расшифровка:

* $\pi_\theta$ — текущая политика (модель), которую мы оптимизируем
* $r_\phi(x, y)$ — оценка награды за ответ $y$ на запрос $x$, рассчитанная reward-моделью $r_\phi$
* $\pi_{\text{ref}}$ — фиксированная политика (например, до RLHF)
* $D_{\text{KL}}$ — KL-дивергенция между новой и старой политиками
* $\beta$ — коэффициент, контролирующий силу штрафа за отклонение от референсной модели

#### Интерпретация:

* Мы хотим находить такие ответы $y$, которые получают высокую награду $r_\phi(x, y)$
* При этом мы не хотим, чтобы поведение модели слишком резко изменилось по сравнению с $\pi_{\text{ref}}$
* KL-дивергенция выступает в роли регуляризатора: она "сдерживает" модель вблизи предыдущего состояния

> **Важно**: тут решается задача максимизации. Она потом немного преображается в дифференцируемую функцию потерь.

### PPO: общий обзор процесса обучения
![alt_text](../../0%20images/image_1.png)

#### 1. Rollout

* **Query**: отправляем запрос (например, "This movie is") в языковую модель (LM)
* **LM**: текущая активная модель генерирует **response** (например, "really great!")

#### 2. Evaluation

* **Query + Response** отправляется в **reward model**
* Reward-модель (обучена на человеческих предпочтениях, правилах или классификаторах) возвращает скалярную **награду** — насколько хороший ответ

#### 3. Optimization (PPO)

* Сравниваются **log-probability** ответа из двух моделей:

  * **Active model** $\pi_\theta$: текущая политика
  * **Reference model** $\pi_\text{ref}$: зафиксированная модель до RL (например, после SFT)

* Вычисляется:

  * **Reward** за текущий ответ
  * **KL-дивергенция** между текущей и референсной моделью
  * **Скорректированная награда** = `reward - β × KL`

* С помощью policy gradient (градиентной оптимизации по политике) веса $\pi_\theta$ обновляются таким образом, чтобы увеличивать награду, но не уходить слишком далеко от начального поведения

> Это и есть суть **Proximal Policy Optimization** — обновлять политику осторожно, с ограничением по KL

## 4.2 Direct Preference Optimization (DPO)

Direct Preference Optimization (DPO) — это метод обучения языковых моделей, позволяющий напрямую учитывать предпочтения пользователя, **без необходимости обучать отдельную reward model**, как в классическом RLHF (Reinforcement Learning with Human Feedback).

### Основная идея

Вместо обучения отдельной модели вознаграждения и использования PPO (Proximal Policy Optimization), DPO **учит саму языковую модель изменять вероятности генерации ответов**, чтобы «предпочтительный» ответ имел более высокую вероятность, чем «непредпочтительный».

#### Пример:

**Вопрос:** Какой газ самый легкий?

* A1 (предпочтительный): Водород
* A2 (непредпочтительный): Я не знаю, я просто языковая модель

> В DPO модель получает пару (A1, A2) и настраивает свои логиты так, чтобы A1 генерировался с большей вероятностью.

#### Преимущества DPO

* Не требует обучения отдельной модели вознаграждения (RM)
* Простота реализации: одна модель, один лосс
* Потенциально быстрее и стабильнее, чем RLHF

### Алгоритм DPO (интуитивно)

1. Имеем датасет из троек $(x, y_w, y_l)$, где:

   * $x$ — вход (например, вопрос)
   * $y_w$ — предпочтительный ответ (winner)
   * $y_l$ — менее предпочтительный ответ (loser)

2. **Перед запуском DPO обычно проводят предварительное обучение (SFT)** на $y_w$, чтобы модель не была «пустой» и умела хотя бы что-то разумное. Это помогает DPO сходиться быстрее и стабильнее. SFT можно рассматривать как инициализацию модели в точке, где она уже знает, как давать приличные ответы.

3. Вычисляем логиты моделей:

   * $\pi_\theta$ — обучаемая модель
   * $\pi_{\text{ref}}$ — замороженная reference-модель (например, изначальный SFT чекпойнт)

4. Считаем формулу:

$$
\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
$$

где:

* $\sigma$ — сигмоида,
* $\beta$ — температурный гиперпараметр (обычно $\beta = 1$ или немного выше).

### Связь с RLHF

DPO можно рассматривать как приближение к RLHF, но без использования:

* модели вознаграждения,
* policy optimization,
* sampling через environment.

Таким образом, DPO — **off-policy метод**, использующий уже собранные пары предпочтений, а не интерактивное обучение.

## 4.3 Kahneman-Tversky Optimization (KTO, Интуиция через поведенческую экономику)

Принцип DPO можно сопоставить с идеями Канемана и Тверски о восприятии ценности:

> Воспринимаемая ценность потери чего-то, что у вас уже есть, **больше**, чем воспринимаемая ценность приобретения эквивалентного количества.

На практике это означает: если модель «теряет» вероятность на хорошем (предпочтительном) ответе — это воспринимается сильнее, чем прирост вероятности на плохом.

Этот эффект можно визуализировать через классическую **Value Function** из поведенческой экономики: она асимметрична — потери болят больше, чем радуют приобретения.

Именно поэтому DPO стремится резко понижать вероятность плохих ответов, даже если это сопровождается небольшим уменьшением уверенности в хороших.

#### Расширение: KTO как Human-Aware Loss

KTO (Kahneman-Tversky Optimization) расширяет подход DPO и оптимизирует генерации, учитывая **склонность человека избегать потерь**. Это проявляется в форме **Human-Aware Loss Function**, где:

* Потери штрафуются значительно сильнее, чем поощряются приобретения
* Применяется **асимметричная функция полезности**, аналогичная кривой Канемана-Тверски

#### Преимущества KTO:

* Работает с **непарными примерами** (например, просто хорошие и плохие ответы, как лайки/дизлайки), без необходимости в парных сравнениях
* Особенно подходит для **несбалансированных датасетов**, что часто встречается в реальных приложениях
* Более точно отражает реальные предпочтения пользователей

#### Сравнение с другими методами:

На графике «Implied Human Value» видно:

* **DPO** обучается на парных сравнениях и реализует гладкую, почти линейную кривую
* **PPO-Clip** добавляет вогнутость, но не учитывает loss aversion
* **KTO** реализует **ярко выраженную асимметрию** и кривую с сильной вогнутостью в зоне выигрыша и крутым падением в зоне потерь

### Формализация KTO Loss

Формула функции потерь KTO:

$\mathcal{L}_{\text{KTO}}(\pi_\theta, \pi_{\text{ref}}) = \mathbb{E}_{x,y \sim \mathcal{D}}[\lambda_y - v(x,y)]$

**Обозначения:**

* $\pi_\theta(y | x)$ — вероятность сгенерировать ответ $y$ на входе $x$ моделью $\pi_\theta$
* $\pi_{\text{ref}}(y | x)$ — вероятность того же ответа у reference-модели (например, после SFT)
* $r_\theta(x, y) = \log \frac{\pi_\theta(y | x)}{\pi_{\text{ref}}(y | x)}$ — log-ratio вероятностей, показывающее, насколько текущая модель предпочитает $y$ сильнее, чем reference
* $z_0 = \text{KL}(\pi_\theta(y' | x) \| \pi_{\text{ref}}(y' | x))$ — KL-дивергенция между полными распределениями моделей (по всем возможным ответам $y'$) — задаёт референсную точку "нейтральности"
* $\beta$ — температурный коэффициент, контролирующий крутизну сигмоиды и чувствительность к отклонению от $z_0$
* $\sigma$ — сигмоида (например, логистическая), сглаживающая переход между зонами поощрения и штрафа
* $\lambda_D$ — вес поощрения для желательных (desirable) примеров
* $\lambda_U$ — вес штрафа для нежелательных (undesirable) примеров (обычно $\lambda_U > \lambda_D$)
* $\lambda_y$ — целевое значение полезности для примера $y$

**Функция полезности:**

$$
 v(x, y) = 
\begin{cases}
  \lambda_D \sigma(\beta(r_\theta(x, y) - z_0)) & \text{если } y \sim y_{\text{desirable}} \\
  \lambda_U \sigma(\beta(z_0 - r_\theta(x, y))) & \text{если } y \sim y_{\text{undesirable}} 
\end{cases}
$$

Таким образом:

* Для хороших ответов: полезность максимальна, если $r_\theta(x, y) > z_0$, то есть модель значительно чаще генерирует $y$, чем reference
* Для плохих ответов: наоборот, штраф идёт за приближение к $z_0$ — модель должна делать их **менее** вероятными, чем в reference

## 4.4 Сравнение алгоритмов

| Критерий                         | RLHF                                | DPO               | KTO                             |
| -------------------------------- | ----------------------------------- | ----------------- | ------------------------------- |
| Что храним в памяти при обучении | LLM\_ref, LLM\_ft, Reward Model     | LLM\_ref, LLM\_ft | LLM\_ref, LLM\_ft               |
| Формат входных данных            | (x, y\_w, y\_l) — пары предпочтений | (x, y\_w, y\_l)   | Likes: (x, y), Dislikes: (x, y) |

### Объяснение:

* **RLHF** требует три модели в памяти:

  * замороженную reference LLM (после SFT)
  * обучаемую LLM
  * модель вознаграждения (reward model), обучаемую отдельно

* **DPO** обходится без reward-модели. Она напрямую использует logit-разницу между предпочтительным и непредпочтительным ответом, чтобы подстроить вероятности.

* **KTO** ещё проще с точки зрения формата данных: не нужны пары (winner, loser). Достаточно просто разметить ответы как "хорошие" и "плохие" (например, по лайкам/дизлайкам). Это делает его пригодным для слабоупорядоченных датасетов.

> Главное отличие: **KTO** работает с "односторонней" обратной связью, где не нужно явно сравнивать ответы попарно.

---
# 5. Инструменты

## 5.1 TRL (Transformers Reinforcement Learning Library)

**TRL** — это библиотека от HuggingFace, предназначенная для обучения LLM с учётом предпочтений, в первую очередь через RLHF и его упрощённые версии.

### Что реализовано:

* PPOTrainer — классический RLHF на основе reward model
* DPOTrainer — прямое обучение предпочтениям (Direct Preference Optimization)
* ORPOTrainer — off-policy RL-альтернатива (реже используется)

### Особенности:

* Поддерживает как `transformers`, так и `peft`
* Совместима с `Accelerate` (для multi-GPU)
* Имеет хорошие примеры и документацию

### Ограничения:

* Всё ещё нет нативной поддержки KTO
* Некоторые методы требуют ручной подготовки датасета (особенно PPO)
* Без встроенной поддержки inference/диалога — это только train-time toolkit

## 5.2 Unsloth

**Unsloth** — это проект, направленный на максимальное ускорение обучения LLM на потребительских GPU.

### Что реализовано:

* Поддержка LoRA-интеграции с HuggingFace и `peft`
* Быстрая загрузка моделей (например, Mistral, Llama) с оптимизированными `forward`
* Совместимость с TRL (в частности DPOTrainer)

### Ограничения:

* Основной фокус — производительность, а не разнообразие методов
* Меньше документации, чем у TRL
* Требует ручной подготовки данных и careful matching LoRA configs
