# PPO, DPO и KTO: сравнение методов обучения с предпочтениями

Эти методы относятся к семейству обучения с человеческими предпочтениями (preference-based fine-tuning), но различаются по принципам, вычислительной сложности и совместимости с пайплайнами inference.

---

## PPO (Proximal Policy Optimization)

### Идея:
Метод из reinforcement learning: LLM обучается максимизировать человеческую награду (reward model), но с регуляризацией — чтобы не отклоняться сильно от базовой модели. Используется policy gradient.

### Ключевая формула (clipped objective):
$$
L_{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \operatorname{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
$$
где $r_t(\theta)$ — отношение вероятностей действий, $\hat{A}_t$ — advantage, $\epsilon$ — допустимое отклонение.

### Особенности:
- Требуется обученная reward model
- Используются rollouts: генерация, оценка, обновление
- Чувствителен к гиперпараметрам, неустойчив к дестабилизации policy
- Высокие требования к compute (особенно при длинных последовательностях)

---

## DPO (Direct Preference Optimization)

### Идея:
Метод напрямую обучается на парах ("предпочтительный ответ", "отклонённый ответ") без reward model и без генерации на каждом шаге. Оценка производится на логитах.

### Формула:
$$
L_{DPO} = -\log \sigma\left(\beta (\log \pi(y^+|x) - \log \pi(y^-|x))\right)
$$

### Особенности:
- Обучается по логитам, без генерации → быстрое обучение (в 4–5 раз быстрее PPO)
- Совместим с batch-инференсом, эффективен в API-интеграциях
- Поддерживается в HuggingFace TRL, реализуется просто

---

## KTO (Kullback-Leibler Preference Tuning)

### Идея:
Альтернатива DPO, в которой ошибка на плохих ответах $y_l$ penalized экспоненциально сильнее. Это KL-penalized версия DPO с bias в сторону наказания.

### Формула:
$$
L_{KTO} = -\log \sigma\left(\beta (\log \pi_\theta(y_w|x) - \log \pi_\theta(y_l|x)) \right)
$$
— та же формула, что и у DPO, но со сдвигом в сторону более сильного наказания за плохой ответ. Интерпретируется как приближение градиента от KL между "предпочтительным поведением" и "отклонённым поведением".

### Особенности:
- Более чувствителен к плохим примерам
- Ведёт себя как KL-регуляризация
- Не требует генерации, работает по логитам
- Меньше используется на практике, но показывает теоретически обоснованное поведение

---

## Сравнение

| Характеристика              | PPO                         | DPO                         | KTO                         |
|-----------------------------|------------------------------|------------------------------|------------------------------|
| Reward model                | Требуется                   | Не требуется                | Не требуется                |
| Обучение                   | RL (policy gradient)        | Supervised (логпробы)       | Supervised (логпробы, KL)   |
| Генерация в цикле обучения | Да                          | Нет                         | Нет                         |
| Простота реализации        | Сложная                     | Простая                     | Средняя                     |
| Скорость обучения          | Медленная                   | Быстрая (x4–5)              | Быстрая                     |
| Совместимость с API        | Плохо                       | Хорошо                      | Хорошо                      |

*Совместимость с API:* означает, что метод можно реализовать и дообучать **без необходимости генерации текста внутри цикла обучения**. Это позволяет работать с логитами (выходами модели до софтмакса), не обращаясь к API с множественными запросами на текстовую генерацию. Такие методы, как DPO и KTO, можно эффективно использовать в средах, где модель доступна только как логитный интерфейс (например, через `forward()`), без генерации (`generate()`), что существенно ускоряет fine-tuning и снижает нагрузку на систему.

---

## Когда использовать

| Условие                                       | Рекомендуемый метод         |
|-----------------------------------------------|------------------------------|
| Максимальный контроль качества (InstructGPT) | PPO                         |
| Ограниченные вычислительные ресурсы          | DPO                         |
| Нет доступа к reward model                   | DPO / KTO                   |
| Важно штрафовать плохие ответы               | KTO                         |
| Необходимо обучение без генерации            | DPO / KTO                   |
| API-compatible RLHF пайплайн                 | DPO                         |

---

## Выводы
- PPO — классический и мощный, но тяжёлый в реализации и требующий больших ресурсов.
- DPO — практичный стандарт в современных LLM пайплайнах, совместим с API и эффективен по ресурсам.
- KTO — усиливает ограничения на плохие ответы, подходит там, где важно явно наказывать undesired behavior.

На практике DPO становится предпочтительным вариантом для RLHF, особенно в условиях ограниченных ресурсов или необходимости быстрой интеграции.

