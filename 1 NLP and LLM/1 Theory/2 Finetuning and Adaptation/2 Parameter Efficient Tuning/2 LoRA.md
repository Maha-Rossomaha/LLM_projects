# LoRA (Low-Rank Adaptation)

## Что такое LoRA?

**LoRA (Low-Rank Adaptation)** — это метод эффективной адаптации больших моделей, при котором не изменяются основные веса модели. Вместо этого добавляются обучаемые low-rank матрицы к определённым слоям (обычно attention), что позволяет дообучать модель при низких затратах памяти и без риска перезаписи оригинальных весов.

**Основная идея:**
LoRA предполагает, что изменение весов $W$ можно аппроксимировать как сумму исходного веса и низкорангового обновления:

$$
W' = W + \Delta W,\quad \Delta W = A B
$$

где $A \in \mathbb{R}^{d \times r}$ и $B \in \mathbb{R}^{r \times k}$ — обучаемые low-rank матрицы ($r \ll d, k$).

**Rank-deficiency** (ранговая недостаточность) — это ситуация, когда матрица имеет меньший ранг, чем её максимальный возможный (полный) размер. В контексте LLM и LoRA это не ошибка, а ключевой принцип.

## Мотивация

* Полный fine-tuning требует большого объёма GPU-памяти (FP32) и ресурсов.
* LoRA позволяет адаптировать модель при тех же качествах, но с заметно меньшими затратами.
* Уменьшает время обучения, энергопотребление, сохраняет оригинальные веса модели (не перезаписываются).
* Упрощает инкрементные обновления: можно хранить только $A, B$ и применять их к разным задачам.

## Как это работает

В LoRA мы не обновляем вес $W$, а вставляем в forward pass:

$$
Wx \rightarrow (W + AB)x = Wx + ABx
$$

Это можно реализовать как дополнительный путь (residual), причём $W$ остаётся неизменным. Обучаются только $A$ и $B$.

Где вставлять:

* Обычно LoRA применяют к **query** и **value** проекциям в attention:

$$
q = (W_q + A_q B_q)x,\quad v = (W_v + A_v B_v)x
$$

## Инфраструктура и инструменты

* `peft` (от HuggingFace) — основной инструмент
* `transformers`, `accelerate`, `deepspeed` — для запуска и ускорения
* `bitsandbytes` — совместимо с quantization (но уже QLoRA)

Пример кода с использованием `peft`:

```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],  # имена параметров (например, attention)
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

## Пояснение параметров LoRA

| Параметр             | Что значит                                                                                                                                                                                        | Как влияет                                                                                                                                    |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| **`r`**              | Ранг low-rank аппроксимации. LoRA заменяет обновление весов матрицами $A \in \mathbb{R}^{d×r}$ и $B \in \mathbb{R}^{r×k}$. Значение `r` определяет размер внутреннего пространства адаптации. | Чем больше `r`, тем выше выразительность адаптации, но и выше число параметров. Типичные значения: 4, 8, 16.                                  |
| **`lora_alpha`**     | Коэффициент масштабирования low-rank компоненты. При обучении LoRA фактически применяется как: $ABx \cdot \frac{\alpha}{r}$                                                                     | Помогает стабилизировать масштаб выхода LoRA. Если `alpha = r`, то масштаб не меняется. Можно повышать `alpha` для усиления влияния адаптера. |
| **`target_modules`** | Список имён модулей модели, к которым применяется LoRA. Обычно это слои `q_proj`, `v_proj` или `c_attn`,                                                                                           | Выбор правильных слоёв критичен: адаптация `q_proj` и `v_proj` даёт наибольший эффект в attention. Для GPT часто используется `c_attn`, который представляет собой объединённую матрицу для query, key и value проекций (QKV).      |
| **`lora_dropout`**   | Dropout, применяемый к LoRA-пути во время обучения.                                                                                                                                               | Может помочь регуляризации, особенно при небольшом размере данных. Обычно от 0 до 0.1.                                                        |
| **`bias`**           | Нужно ли обучать bias (смещения) в слоях модели. Возможные значения: "none", "all", "lora\_only".                                                                                                 | Чаще всего — "none", чтобы оставлять все смещения оригинальной модели нетронутыми.                                                            |
| **`task_type`**      | Тип задачи (например, `CAUSAL_LM`, `SEQ_CLS` и т.д.) — помогает библиотеке правильно применить адаптацию.                                                                                         | Автоматически выбирает нужные слои и стратегии применения LoRA.                                                                               |

## Подводные камни

* LoRA работает хорошо при условии, что задача не требует изменений во всей модели.
* Неправильно выбранные target\_modules могут привести к нулевому эффекту.
* Иногда нужно комбинировать с другими методами (prompt-tuning, adapters).


## Use-case примеры

1. **Адаптация GPT2 под генерацию рекламных текстов** — target\_modules = \["c\_attn"].
2. **Дообучение BLOOM на юридических документах** — вставка LoRA в `q_proj`, `v_proj`.
3. **T5 для генерации мета-тегов** — LoRA на encoder attention.

## Отличия от других подходов

| Метод          | Параметры     | Память        | Качество        | Применение                   |
| -------------- | ------------- | ------------- | --------------- | ---------------------------- |
| FP32 Fine-tune | Все           | Очень высокая | Высокое         | Полный контроль              |
| **LoRA**       | 0.5–2%        | Низкая        | Среднее/высокое | Баланс ресурсы/качество      |
| Prompt-tuning  | Только prompt | Очень низкая  | Низкое/среднее  | Быстрая адаптация, eval only |

## Вывод

LoRA — один из самых эффективных и популярных способов адаптации LLM, если нужно:

* сохранить память
* ускорить fine-tuning
* не трогать оригинальные веса
* быстро переключаться между задачами (за счёт хранения только low-rank адаптаций)
* на `inference`: адаптеры LoRA мерджатся с основными весами
  * До: $W_{eff} = W + \alpha \cdot A B^T$ (в два прохода)
  * После merge: $\tilde{W} = W + \alpha \cdot A B^T$ → инференс на $\tilde{W}$ как обычный linear
  > Это позволяет убрать LoRA overhead полностью, особенно при eval() и в int8/4 режиме
  > Merge разумен, если:
  > - не нужен PEFT‑swap по задачам,
  > - не будет адаптивного LoRA‑переключения в рантайме

Он особенно полезен в продакшн-средах и на малых объёмах данных.
