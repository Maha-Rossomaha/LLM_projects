# LoRA (Low-Rank Adaptation)

## Что такое LoRA?
**LoRA (Low-Rank Adaptation)** — это метод эффективной адаптации больших моделей, при котором не изменяются основные веса модели. Вместо этого добавляются обучаемые low-rank матрицы к определённым слоям (обычно attention), что позволяет дообучать модель при низких затратах памяти и без риска перезаписи оригинальных весов.

**Основная идея:**
LoRA предполагает, что изменение весов $W$ можно аппроксимировать как сумму исходного веса и низкорангового обновления:
$$
W' = W + \Delta W,\quad \Delta W = A B
$$
где $A \in \mathbb{R}^{d \times r}$ и $B \in \mathbb{R}^{r \times k}$ — обучаемые low-rank матрицы ($r \ll d, k$).


## Мотивация
- Полный fine-tuning требует большого объёма GPU-памяти (FP32) и ресурсов.
- LoRA позволяет адаптировать модель при тех же качествах, но с заметно меньшими затратами.
- Уменьшает время обучения, энергопотребление, сохраняет оригинальные веса модели (не перезаписываются).
- Упрощает инкрементные обновления: можно хранить только $A, B$ и применять их к разным задачам.


## Как это работает
В LoRA мы не обновляем вес $W$, а вставляем в forward pass:
$$
Wx \rightarrow (W + AB)x = Wx + ABx
$$

Это можно реализовать как дополнительный путь (residual), причём $W$ остаётся неизменным. Обучаются только $A$ и $B$.

Где вставлять:
- Обычно LoRA применяют к **query** и **value** проекциям в attention:
$$
q = (W_q + A_q B_q)x,\quad v = (W_v + A_v B_v)x
$$


## Инфраструктура и инструменты
- `peft` (от HuggingFace) — основной инструмент
- `transformers`, `accelerate`, `deepspeed` — для запуска и ускорения
- `bitsandbytes` — совместимо с quantization (но уже QLoRA)

Пример кода с использованием `peft`:
```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],  # имена параметров (например, attention)
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

## Типичные параметры LoRA
- `r`: 4–64 (ранг low-rank аппроксимации)
- `alpha`: 16–64 (коэффициент масштабирования)
- `dropout`: 0.05–0.1
- `target_modules`: `q_proj`, `v_proj`, `c_attn` и т.п. (зависит от архитектуры)

## Подводные камни
- LoRA работает хорошо при условии, что задача не требует изменений во всей модели.
- Неправильно выбранные target_modules могут привести к нулевому эффекту.
- Иногда нужно комбинировать с другими методами (prompt-tuning, adapters).


## Use-case примеры
1. **Адаптация GPT2 под генерацию рекламных текстов** — target_modules = ["c_attn"].
2. **Дообучение BLOOM на юридических документах** — вставка LoRA в `q_proj`, `v_proj`.
3. **T5 для генерации мета-тегов** — LoRA на encoder attention.

## Отличия от других подходов
| Метод               | Параметры       | Память       | Качество       | Применение                   |
|--------------------|-----------------|--------------|----------------|------------------------------|
| FP32 Fine-tune     | Все             | Очень высокая| Высокое        | Полный контроль              |
| **LoRA**           | 0.5–2%          | Низкая       | Среднее/высокое| Баланс ресурсы/качество      |
| Prompt-tuning      | Только prompt   | Очень низкая | Низкое/среднее | Быстрая адаптация, eval only |


## Вывод
LoRA — один из самых эффективных и популярных способов адаптации LLM, если нужно:
- сохранить память
- ускорить fine-tuning
- не трогать оригинальные веса
- быстро переключаться между задачами (за счёт хранения только low-rank адаптаций)

Он особенно полезен в продакшн-средах и на малых объёмах данных.