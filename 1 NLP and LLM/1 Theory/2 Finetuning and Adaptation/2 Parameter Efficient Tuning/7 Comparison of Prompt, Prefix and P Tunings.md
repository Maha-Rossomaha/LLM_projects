# Сравнение Prompt Tuning, Prefix Tuning и P-Tuning v2

Ниже — концептуальное сравнение трёх популярных методов параметро-эффективной адаптации больших языковых моделей: **Prompt Tuning**, **Prefix Tuning**, **P-Tuning v2**.

---

## 1. Prompt Tuning

### Идея
Добавление обучаемых виртуальных токенов только к input, без изменений внутри модели.

### Как работает
- Вместо обычного текстового prompt ("Translate this: ...") вставляется обучаемая embedding-матрица `E ∈ ℝ^{N × d_model}`.
- Эти токены не имеют текстового представления, но воспринимаются моделью как часть входа.
- Все веса модели заморожены.

### Плюсы
- Простота и универсальность.
- Очень мало параметров.

### Минусы
- Низкое качество на сложных задачах.
- Не влияет на поведение attention слоёв.
- Зависит от точной формулировки входа.

---

## 2. Prefix Tuning (с `prefix_projection=True`)

### Идея
Добавление обучаемых ключей и значений (K_prefix, V_prefix) к attention-механизму модели. Без изменения input.

### Как работает
- В каждый слой self-attention добавляются специальные key/value, будто бы перед реальной последовательностью шёл некий "контекст".
- Эти K/V генерируются через MLP (если `prefix_projection=True`) из embedding.
- Веса модели заморожены.

### Плюсы
- Влияет на поведение attention напрямую.
- Хорошо работает в генеративных задачах.

### Минусы
- Не влияет на input-embedding.
- Качество ограничено по сравнению с P-Tuning v2.
- Менее гибко, чем LoRA или P-Tuning.

---

## 3. P-Tuning v2

### Идея
Комбинация prompt- и prefix-подходов: добавление обучаемых токенов в input и во все attention-слои.

### Как работает
- Обучаемые виртуальные токены вставляются в input.
- Параллельно из этих же embedding с помощью MLP генерируются K_prefix, V_prefix для каждого attention-слоя.
- Таким образом, P-Tuning v2 влияет и на вход, и на внутренние вычисления.

### Плюсы
- Почти full finetuning-производительность.
- Гибкость и высокая выразительность.
- Работает на длинных input, в zero-shot и few-shot.

### Минусы
- Чуть сложнее реализовать.
- Чуть больше параметров, чем prompt или prefix tuning.

---

## Сравнительная таблица

| Метод             | Input Influence | Attention Influence | Использует MLP | Качество   | Объём параметров |
|------------------|-----------------|----------------------|----------------|------------|------------------|
| Prompt Tuning     | Да              | Нет                  | Нет            | Низкое     | Очень низкий     |
| Prefix Tuning     | Нет             | Да                   | Да (если projection) | Среднее | Средний          |
| P-Tuning v2       | Да              | Да                   | Да             | Высокое    | Средне-высокий   |

---

## Выводы
- Prompt Tuning — самый простой и быстрый, но даёт ограниченный контроль и слабое качество.
- Prefix Tuning — влияет глубже, но не трогает вход.
- P-Tuning v2 — наиболее мощный из PEFT-подходов, объединяет оба мира и приближается к full fine-tune при сохранении компактности.

Если необходима лучшая адаптация с минимальными затратами — предпочтителен P-Tuning v2.

