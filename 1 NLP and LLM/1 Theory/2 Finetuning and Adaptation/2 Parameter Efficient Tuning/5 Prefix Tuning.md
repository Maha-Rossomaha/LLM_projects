# Prefix-Tuning 

## Что такое Prefix-Tuning?
**Prefix-tuning** — это метод дообучения больших языковых моделей, при котором мы добавляем к каждому слою модели специальные "префиксы" — наборы векторов, которые учатся вместе с задачей. Главное — мы не трогаем сами веса модели. Всё обучение происходит только в этих новых добавленных векторах.

Префиксы добавляются в механизм внимания (attention), как будто к каждому слою мы добавляем дополнительную "память":

$$
\text{Attention}(Q, [K_{\text{prefix}}; K], [V_{\text{prefix}}; V])
$$

То есть, модель теперь смотрит не только на вход, но и на этот обучаемый "контекст".

> **Важно:** градиенты текут только через префиксы (если `prefix_projection=False` - через префикс внутри каждого слоя, если `prefix_projection=True` - через дополнительную матрицу эмбеддингов для префиксов и MLP слой)

## Зачем это нужно?
- Меняем только малую часть параметров (обычно меньше 1%)
- Требует мало видеопамяти и вычислений
- Можно использовать одну и ту же модель для разных задач, просто переключая префикс
- Не портим оригинальную модель — она остаётся неизменной
- Подходит для больших моделей, таких как GPT, BERT, T5 и др.

## Как это работает
В каждый слой трансформера добавляются два набора префикс-векторов:
- Один для ключей (key)
- Один для значений (value)

Эти векторы обучаются, как обычные параметры нейросети. Они создаются в начале (например, случайным образом), и модель учится использовать их в процессе обучения.

Пример (концептуальный) на PyTorch:
```python
K_full = torch.cat([prefix_K, K], dim=1)
V_full = torch.cat([prefix_V, V], dim=1)
attention_out = attention(Q, K_full, V_full)
```

## Какие инструменты использовать
- `transformers` от Hugging Face
- `peft` — библиотека для эффективного дообучения (PEFT)
- Работает для задач: генерация текста, классификация, суммаризация и др.

### Пример кода:
```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import get_peft_model, PrefixTuningConfig, TaskType

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
tokenizer = AutoTokenizer.from_pretrained("t5-small")

prefix_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    num_virtual_tokens=30,
    prefix_projection=True,
    inference_mode=False
)

model = get_peft_model(model, prefix_config)
model.print_trainable_parameters()
```

## Как настроить параметры
- `num_virtual_tokens`: сколько токенов будет в префиксе (10–100 — разумный диапазон)
- `prefix_projection`: если True — добавляется небольшая нейросеть, которая улучшает обучение
- `learning_rate`: скорость обучения (примерно 2e-4 — 5e-4)
- `batch_size`: размер батча (обычно 8–32)
- `scheduler`: план изменения learning rate (linear или cosine)

## Prefix projection
### prefix\_projection=False

Когда `prefix_projection=False`, мы **напрямую храним и обучаем префиксные ключи и значения для каждого слоя**. Это значит:

* Для каждой пары `K_prefix`, `V_prefix` в каждом слое создаются отдельные обучаемые параметры.
* Эти параметры имеют форму: `num_layers × 2 × prefix_length × num_heads × head_dim`.
* Они инициализируются и обучаются напрямую, без использования дополнительных нейросетей.

Этот вариант проще в реализации, но:

* Требует больше памяти, особенно при большом числе слоёв и голов.
* Менее гибок для обобщения между задачами.

### prefix\_projection=True

Если `prefix_projection=True`, то используется **одна общая MLP**, которая получает на вход общий embedding `E` размерности `prefix_length × hidden_dim`, и выдаёт все префиксные ключи и значения сразу:

1. В начале инициализируется обучаемая embedding-матрица `E`:

   $$
   E ∈ ℝ^{prefix\_length × hidden\_dim}
   $$

2. Одна и та же двухслойная MLP применяется к каждому токену из `E` и выдаёт:

   $$
   \text{MLP}(E) → \text{тензор формы } prefix\_length × num\_layers × 2 × num\_heads × head\_dim
   $$

3. Этот тензор содержит **K/V-префиксы для всех слоёв трансформера**. Во время forward-pass attention слоя ℓ просто выбирает соответствующий блок:

```python
kv = projected_prefix[:, layer_idx]  # shape: prefix_length × 2 × num_heads × head_dim
K_prefix, V_prefix = kv[:, 0], kv[:, 1]
```

Таким образом:

* Все префиксы обучаются через один общий embedding + MLP.
* Память экономится, так как параметры масштабируются только по `prefix_hidden_dim`, а не по числу слоёв.
* Подходит для задач, где важно обобщение или переносимость адаптации.

> **Важно:** принцип работы в `prefix_projection=True` ровно такой же, как у **P-tuning v.2**, за исключением того, что сгенерированные во втором алгоритме префиксы будут идти вместе с input_ids через всю модель, а в **Prefix Tuning** только через слои внимания.

## Что показывают эксперименты
- В задачах GLUE и SuperGLUE prefix-tuning даёт почти такие же результаты, как и полное дообучение всей модели
- Лучше всего работает, когда важен стиль или формат генерации
- Если правильно подобрать длину префикса, можно достичь 95–99% качества от full fine-tune

## Примеры применения
1. Настройка T5 для генерации вопросов по тексту
2. Классификация текстов с помощью BERT с разными префиксами
3. Настройка GPT под юридический стиль текста
4. Создание мультиязычной модели с разными префиксами для каждого языка

## Недостатки
- Префиксы нельзя интерпретировать — они выглядят как набор чисел
- На очень коротких входах могут не успеть повлиять на результат
- Для каждой новой задачи нужно обучать свой префикс
- При очень длинном контексте может возникнуть замедление
- Не работает на black-box API

## Сравнение с другими подходами
| Метод           | Что обновляется | Требует памяти | Качество        | Когда использовать             |
|----------------|------------------|----------------|------------------|-------------------------------|
| Full fine-tune | Все веса         | Много          | Максимум       | Есть ресурсы, нужен максимум   |
| LoRA           | 1–2% весов       | Мало           | Почти как full | Хорошо работает с attention   |
| QLoRA          | LoRA + int4      | Очень мало     | Почти как full | Под малые GPU, экономия памяти|
| Prefix-tuning  | Только префикс   | Очень мало     | Хорошее        | Быстрая адаптация              |
| Prompt-tuning  | Только input     | Минимум        | Ограниченно    | Очень лёгкие задачи            |


## Итоги
**Prefix-tuning** — это простой и эффективный способ дообучать большие языковые модели, не меняя их основные веса. Это очень полезно:
- когда важно сэкономить ресурсы,
- когда нельзя испортить оригинальную модель,
- когда нужно быстро адаптироваться под новую задачу.

Метод особенно хорош для генерации, классификации, обучения под стиль или язык.

