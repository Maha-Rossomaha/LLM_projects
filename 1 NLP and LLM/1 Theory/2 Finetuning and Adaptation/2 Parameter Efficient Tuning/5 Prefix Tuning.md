# Prefix-Tuning 

## Что такое Prefix-Tuning?
**Prefix-tuning** — это метод дообучения больших языковых моделей, при котором мы добавляем к каждому слою модели специальные "префиксы" — наборы векторов, которые учатся вместе с задачей. Главное — мы не трогаем сами веса модели. Всё обучение происходит только в этих новых добавленных векторах.

Префиксы добавляются в механизм внимания (attention), как будто к каждому слою мы добавляем дополнительную "память":

$$
\text{Attention}(Q, [K_{\text{prefix}}; K], [V_{\text{prefix}}; V])
$$

То есть, модель теперь смотрит не только на вход, но и на этот обучаемый "контекст".

## Зачем это нужно?
- Меняем только малую часть параметров (обычно меньше 1%)
- Требует мало видеопамяти и вычислений
- Можно использовать одну и ту же модель для разных задач, просто переключая префикс
- Не портим оригинальную модель — она остаётся неизменной
- Подходит для больших моделей, таких как GPT, BERT, T5 и др.

## Как это работает
В каждый слой трансформера добавляются два набора префикс-векторов:
- Один для ключей (key)
- Один для значений (value)

Эти векторы обучаются, как обычные параметры нейросети. Они создаются в начале (например, случайным образом), и модель учится использовать их в процессе обучения.

Пример (концептуальный) на PyTorch:
```python
K_full = torch.cat([prefix_K, K], dim=1)
V_full = torch.cat([prefix_V, V], dim=1)
attention_out = attention(Q, K_full, V_full)
```

## Какие инструменты использовать
- `transformers` от Hugging Face
- `peft` — библиотека для эффективного дообучения (PEFT)
- Работает для задач: генерация текста, классификация, суммаризация и др.

### Пример кода:
```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import get_peft_model, PrefixTuningConfig, TaskType

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
tokenizer = AutoTokenizer.from_pretrained("t5-small")

prefix_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    num_virtual_tokens=30,
    prefix_projection=True,
    inference_mode=False
)

model = get_peft_model(model, prefix_config)
model.print_trainable_parameters()
```

## Как настроить параметры
- `num_virtual_tokens`: сколько токенов будет в префиксе (10–100 — разумный диапазон)
- `prefix_projection`: если True — добавляется небольшая нейросеть, которая улучшает обучение
- `learning_rate`: скорость обучения (примерно 2e-4 — 5e-4)
- `batch_size`: размер батча (обычно 8–32)
- `scheduler`: план изменения learning rate (linear или cosine)

### Prefix projection
По сути, `Prefix Tuning` - просто добавление в каждый слой LLM несколько виртуальных токенов (например, 10–30), и обучение только их. Остальная модель остаётся замороженной.  

Эти виртуальные токены нужно каким-то образом превратить в `key` и `value` для каждого слоя модели.  

Если `prefix_projection=False`:
- Храним сразу готовые key/value матрицы для каждого слоя трансформера
- Например: 12 слоёв $\times$ 2 (key и value) $\times$ 30 токенов $\times$ размерность — это много параметров
- Всё обучается напрямую

Если `prefix_projection=True`:
- Храним маленький embedding — просто матрицу `num_virtual_tokens` $\times$ `prefix_hidden_dim`
- Дальше — пропускаем его через один и тот же MLP, который для каждого слоя выдаёт нужный размер key/value


## Что показывают эксперименты
- В задачах GLUE и SuperGLUE prefix-tuning даёт почти такие же результаты, как и полное дообучение всей модели
- Лучше всего работает, когда важен стиль или формат генерации
- Если правильно подобрать длину префикса, можно достичь 95–99% качества от full fine-tune

## Примеры применения
1. Настройка T5 для генерации вопросов по тексту
2. Классификация текстов с помощью BERT с разными префиксами
3. Настройка GPT под юридический стиль текста
4. Создание мультиязычной модели с разными префиксами для каждого языка

## Недостатки
- Префиксы нельзя интерпретировать — они выглядят как набор чисел
- На очень коротких входах могут не успеть повлиять на результат
- Для каждой новой задачи нужно обучать свой префикс
- При очень длинном контексте может возникнуть замедление

## Сравнение с другими подходами
| Метод           | Что обновляется | Требует памяти | Качество        | Когда использовать             |
|----------------|------------------|----------------|------------------|-------------------------------|
| Full fine-tune | Все веса         | Много          | Максимум       | Есть ресурсы, нужен максимум   |
| LoRA           | 1–2% весов       | Мало           | Почти как full | Хорошо работает с attention   |
| QLoRA          | LoRA + int4      | Очень мало     | Почти как full | Под малые GPU, экономия памяти|
| Prefix-tuning  | Только префикс   | Очень мало     | Хорошее        | Быстрая адаптация              |
| Prompt-tuning  | Только input     | Минимум        | Ограниченно    | Очень лёгкие задачи            |


## Итоги
**Prefix-tuning** — это простой и эффективный способ дообучать большие языковые модели, не меняя их основные веса. Это очень полезно:
- когда важно сэкономить ресурсы,
- когда нельзя испортить оригинальную модель,
- когда нужно быстро адаптироваться под новую задачу.

Метод особенно хорош для генерации, классификации, обучения под стиль или язык.

