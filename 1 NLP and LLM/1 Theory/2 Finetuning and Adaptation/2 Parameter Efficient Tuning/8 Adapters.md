# Adapter-Tuning 

## Что такое Adapter-Tuning?
**Adapter-Tuning** — это метод дообучения больших языковых моделей (LLM), при котором все основные веса модели **замораживаются**, а внутрь трансформерных слоёв встраиваются **небольшие обучаемые модули** — *adapter-слои*. Этот подход позволяет обучать лишь **0.1–5% параметров**, значительно снижая требования к памяти и упрощая хранение/переключение между задачами.


## Мотивация и преимущества
- **Параметроэффективность:** адаптеры добавляют минимум параметров (по сравнению с полным fine-tune).
- **Универсальность:** можно обучить разные adapter-слои под разные задачи и динамически их переключать.
- **Совместимость:** хорошо работают с большими frozen LLM (например, BERT, RoBERTa, T5, GPT).
- **Лёгкая дистрибуция:** adapter веса можно распространять отдельно, без копирования всей модели.

## Архитектура адаптеров
Adapter добавляется внутрь каждого слоя трансформера — обычно **в post-FFN** и/или **в post-attention** блоках. Он представляет собой узкий обучаемый путь, встроенный в основной поток модели. Его структура напоминает «бутылочное горлышко»:

$$
\text{Adapter}(x) = x + W_{\text{up}}(\sigma(W_{\text{down}}(x)))
$$

Где:
- $W_{\text{down}}$ проецирует вектор из $d_{model} \to d_{bottleneck}$
- $\sigma$ — нелинейность (ReLU, GELU)
- $W_{\text{up}}$ восстанавливает размерность $d_{bottleneck} \to d_{model}$
- Итог: residual блок с обучаемой вставкой

Adapter реализован как residual-добавка: он сохраняет поведение предобученной модели, обеспечивает стабильность обучения и добавляет обучаемое преобразование, не нарушая поток оригинальной информации.

## Поддержка и фреймворки
- [PEFT](https://github.com/huggingface/peft) (HuggingFace): `AdapterConfig`, `get_peft_model`
- [AdapterHub.ml](https://adapterhub.ml/): сотни уже обученных адаптеров для BERT
- `transformers` (через PEFT или Adapter-Transformers fork)


### Пример кода с HuggingFace PEFT:
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import AdapterConfig, get_peft_model, TaskType

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

adapter_config = AdapterConfig(
    task_type=TaskType.SEQ_CLS,
    adapter_size=16,  # bottleneck
    non_linearity="relu"
)

model = get_peft_model(model, adapter_config)
model.print_trainable_parameters()
```


## Гиперпараметры и тонкости

### Практический совет по выбору `adapter_size`:
- Для моделей до 100M (если модель маленькая) параметров используйте `adapter_size=32–64`
- Для BERT-large, T5-base и выше — разумный диапазон 16–48
- Меньшие значения ускоряют обучение, но могут снизить точность

Отдельные гиперпараметры:  
- `adapter_size`: размер бутылочного горлышка (например, 8–64); чем меньше — тем эффективнее, но хуже обучение.
- `non_linearity`: `relu`, `gelu` и т.д. — влияет на обучение.
- `task_type`: классификация, генерация и др.
- `location`: куда вставлять (after attention / FFN / оба)
- `init_weights`: способ инициализации (обычно `normal` или `xavier`)


## Use-case примеры

Adapter используется как в академических, так и в индустриальных сценариях. Ниже представлены ключевые варианты применения — как из исследовательской практики, так и из индустриальных сценариев: (например, HuggingFace AdapterHub или генерация ответов в open-source чатботах).
1. **Классификация:** обучение адаптеров под каждую метку (multi-head setup)
2. **Мультиязычность:** один frozen BERT + адаптеры на разных языках
3. **Стиль/жанр генерации:** под каждый стиль свой adapter
4. **On-device deployment:** adapter легко подгружается без переразвертывания модели

## Ограничения
- Требуется модификация архитектуры (вставка adapter блоков)
- Сложнее встроить в модели, не поддерживающие insert hooks
- Могут не дать прирост, если задача сильно отличается по логике (reasoning-heavy)


## Отличия от других PEFT подходов
| Метод           | Что обучается       | Память   | Качество        | Когда использовать                    |
|----------------|----------------------|----------|------------------|---------------------------------------|
| Full fine-tune | Все веса             | Высокая | Максимум       | Есть ресурсы, нужен максимум качества |
| LoRA / QLoRA   | Low-rank вставки     | Низкая | Высокое        | Лучше всего на attention-heavy задачах|
| Prefix-tuning  | Prefix в attention   | Очень низкая | Хорошее     | Адаптация стиля / генерация          |
| Prompt-tuning  | Embedding'и на входе | Очень низкая | Среднее     | Классификация, API-среды             |
| Adapter-tuning | Bottleneck-слои внутри трансформера | Низкая | Почти как full | Универсальный, хорошо для multi-task |


## Вывод

### Сравнение выбора между Adapter и LoRA:

| Критерий                        | Adapter                          | LoRA                                 |
|--------------------------------|----------------------------------|--------------------------------------|
| Изоляция задач                 | Хорошо поддерживается         | Требует объединения весов         |
| Эффективность в multi-task     | Высокая                       | Ограничена                        |
| Производительность (внимание)  | Немного медленнее              | Быстрее на attention-heavy задачах|
| Простота внедрения             | Стандартизировано             | Поддерживается в PEFT             |

**Adapter-Tuning** — это универсальный и параметроэффективный способ дообучения LLM. Он сочетает:
- **простоту внедрения** (по сравнению с LoRA);
- **меньшие требования к памяти**, чем full-tune;
- **возможность мультизадачного обучения**, когда одна модель держит адаптеры под разные задачи/языки.

Adapter-тюнинг отлично подходит для:
- **Энтерпрайз-решений**, где каждая задача или клиент требует изоляции;
- **Распределённого обучения**, когда адаптеры обучаются независимо и объединяются;
- **Мобильных и edge-сред**, где невозможно обучать всю модель;
- **Модульных LLM-систем**, в которых подгружается нужный адаптер под конкретную задачу.