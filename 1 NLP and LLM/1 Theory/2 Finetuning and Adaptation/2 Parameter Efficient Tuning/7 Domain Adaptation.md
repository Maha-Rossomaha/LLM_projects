# 1. Введение: Что такое домен и доменная адаптация

**Домен** — это специфическая область знаний, в которой работает модель.

Типы доменов:

- **Языковой** — например, русский, китайский
- **Функциональный** — например, ответы в стихах, в формате markdown
- **Предметный** — например, медицина, право, финансы

**Доменная адаптация** — это дообучение или настройка LLM на специфических данных, чтобы она лучше понимала терминологию, стиль и нюансы конкретной области.

---

# 2. Почему не всегда можно использовать ChatGPT

**Обычно можно!**

1. Это как минимум хороший baseline
2. Стоит попробовать system prompt или продуманный prompting

**Но иногда использовать ChatGPT нельзя, потому что:**

1. У вас NDA-данные, которые нельзя передавать во внешний сервис
2. ChatGPT может "морозиться" (например, на медицинские запросы)
3. Домен может быть слишком специфическим
4. Иногда нужно быстрое и дешёвое решение

---

# 3. Методы дообучения моделей

ML-методы дообучения делятся на два типа:

1. Параметрические: изменяют параметры модели. Примеры: `Continual Pretraining`, `SFT (Supervised Fine-Tuning)`, `LoRA`
2. Непараметрические: не затрагивают параметры модели напрямую. Пример: `RAG (Retrieval-Augmented Generation)`
---

# 4. Параметрические методы дообучения моделей

## 4.1 SFT (Supervised Fine-Tuning)

- Дообучение модели на инструкциях
- Происходит обучение **полных весов** модели
- Эффективен, когда нужно научить **конкретному навыку**
- **Не добавляет** новых знаний, а учит **извлекать уже имеющиеся**

## 4.2 CPT (Continual Pretraining)

- CPT — это дообучение уже обученной модели с нуля на новых данных ("претрейн поверх претрейна")
- Самый эффективный из параметрических методов (уступает только обучению модели с нуля)
- Требует **много ресурсов**
- Требует много экспериментов с выбором и качеством данных
- Для популярных моделей существуют **готовые рецепты** (pipeline'ы, скрипты, best practices)

## 4.3 LoRA (Low-Rank Adaptation)

- LoRA — дешёвый способ дообучить LLM
- Это метод обучения, применяемый **поверх SFT или CPT**
- Очень экономичен по ресурсам
- Простая инфраструктура обучения
- Отсутствует проблема **catastrophic forgetting** (катастрофического забывания)

## 4.4 Сводная таблица методов адаптации

| Метод                              | Когда использовать?      | Недостатки                        |
| ---------------------------------- | ------------------------ | --------------------------------- |
| System Prompt / Prompt Engineering | MVP, простая задача      | Совсем не гибкий                  |
| RAG                                | Сторонняя база знаний    | Инфра, не меняет поведение модели |
| SFT (+ Alignment)                  | Поведение, стиль         | Не обновляем знания               |
| CPT (+ SFT + Alignment)            | Сильная адаптация        | Очень дорого                      |
| LoRA                               | Чтобы удешевить обучение | Не сильная адаптация              |
---

# 5. Описание методов Domain Adaptation

## 5.0 Tokenizer Adaptation
**Зачем это делать?**
- Меньше токенов — быстрее инференс
- Быстрее инференс — дешевле
- Лучше разбиение токенов — модель «умнее» (особенно для русскоязычных данных)
- Но: придётся дообучать модель с новым токенизатором (режим SFT — supervised fine-tuning)

**Когда это нужно?**
- Если вы адаптируете LLM под русский язык (или другой язык с плохой поддержкой в оригинальном токенизаторе)
- Если работаете с особыми доменами, где много специфической терминологии
- Если хотите уменьшить расходы на инференс при большом трафике

**Алгоритм**  
1. **Обучаем токенизатор** на своих данных

   * Можно использовать подмножество данных, на которых обучалась оригинальная модель
   * Важно подобрать нужный размер словаря (возможно, сохранить часть оригинального BPE)
   * Подходящие алгоритмы: Byte-Pair Encoding (BPE), SentencePiece, Unigram

2. **Инициализация эмбеддингов новых токенов**

   Если новый токен разбивается на старые токены, можно инициализировать его эмбеддинг как среднее эмбеддингов старых токенов:

   $$
   v_{\text{new}}(t^n_i) = \frac{1}{K} \sum_{j=1}^K v_{\text{old}}(t^o_j)
   $$

   где:

   * $t^n_i$ — новый токен,
   * $t^o_j$ — токены из старого токенизатора, на которые он разбивается,
   * $K$ — их количество

3. **Заморозить или нет:**

   * Можно заморозить часть эмбеддингов (старых), чтобы сохранить знание модели
   * Либо разрешить подстраиваться во время дообучения

4. **Варианты стратегии инициализации:**

   * Среднее по старым (как выше)
   * Использование автокодировщика/LLM для генерации эмбеддингов
   * Случайная инициализация (плохо сходимо)

5. **После адаптации токенизатора:**

   * Нужно заменить embedding слой в модели
   * Возможно, потребуется заново пересобрать positional embeddings, если используется абсолютная позиция
   * Сравнить производительность и длину токенов на валидации (можно использовать Perplexity или Entropy)

--- 

## 5.1 CPT + SFT

Это самый дорогой, но и самый мощный способ доменной адаптации. Он объединяет два подхода:

- **Continual Pretraining** (CPT) — обновляет знания модели
- **Supervised Fine-Tuning** (SFT) — обучает модель следовать инструкциям

Ниже — краткий рецепт по шагам:

#### Рецепт CPT + SFT

Процесс CPT + SFT обычно включает следующие этапы:

- **Выбираем модель** — выбираем LLM, подходящую по архитектуре и лицензии
- **Собираем данные** — тексты, диалоги, инструкции из домена
- **Адаптируем токенизатор** — особенно если терминология специфична
- **Обучаем специальным образом** — сначала CPT (masked/causal LM), потом SFT (instruction tuning)
- **Валидируем качество** — тестируем поведение, знание, обобщение
- **Боремся с проблемами** — от деградации качества до переобучения

---
### 5.1.1 CPT
#### 5.1.1.1 Как выбрать модель

Перед запуском доменной адаптации важно выбрать базовую модель. Для этого используются:

1. **Арены** — например, LMSYS Arena. Однако стоит учитывать, что модели могут подстраиваться под такие рейтинги, и высокая позиция не всегда означает реальную пригодность под ваш случай.
2. **Бенчмарки** — выбираются релевантные задачам вашего домена (медицинские QA, юридические классификации и др.).
3. **Свои "корзинки"** — внутренние задачи от бизнеса, под которые нужна адаптация. Это часто лучший ориентир, так как напрямую отражает целевое применение.

#### 5.1.1.2 Откуда брать данные

Данные для CPT должны быть максимально близки к оригинальному претрейну модели, но с усиленным присутствием доменных примеров. Полностью заменять на доменные данные нельзя — модель может потерять общее понимание языка и логики.

Оптимальной считается **двухстадийная стратегия обучения**:

| Stage 1                           | Stage 2                      |
| --------------------------------- | ---------------------------- |
| Датасет очень похож на претрейн   | Смесь из Stage 1 остаётся    |
| Увеличенные веса у нужных доменов | Большие веса у доменов       |
| Более качественные данные         | Около половины датасета — QA |

**Данные претрейна**:

1. Целиком датасеты претрейна обычно не публикуются
2. Однако многие разработчики пишут, какие источники использовались
3. Почти всегда можно собрать приближённую смесь (датамикс) самостоятельно

**Где искать приближённые данные**:

- [FineWeb](https://huggingface.co/datasets/FineWeb)
- [C4 (Colossal Clean Crawled Corpus)](https://www.tensorflow.org/datasets/catalog/c4)

##### 5.1.1.2.1 Первая стадия: претрейн + домен

На первом этапе рекомендуется смешивать данные оригинального претрейна с доменными, сохраняя баланс между общей языковой компетентностью и специализацией. Типичный пример состава датасета:

- **Common Crawl** — 52.6%
- **Code** — 23.6%
- **Books** — 8.9%
- **Domains** — 5.9%
- **Wiki** — 5.9%
- **Instruct** — 5.7%
- **Math / Science** — небольшая доля

Такой состав позволяет адаптироваться к домену, не теряя универсальности модели.

##### 5.1.1.2.2 Вторая стадия: перевзвешивание + QA

На втором этапе продолжается обучение, но с более агрессивным усилением доменных данных. Основные особенности:

Веса доменных данных значительно увеличиваются

Часто около 50% датасета составляет формат вопрос-ответ (QA), что способствует улучшению способности к генерации точных и полных ответов

При этом сохраняется некоторая доля общего корпуса, чтобы не потерять общие языковые способности

Типичный состав датасета на втором этапе:

- Instruct — 45.1%
- Common Crawl — 14.2%
- Domains — 14.4%
- Code — 6.3%
- Math — 8.0%
- Books — 6.0%
- Wiki — 2.5%
- Science / Parallel — небольшая доля

Такой подход помогает обучить модель как точности, так и практическим навыкам в нужной области, не жертвуя общей адекватностью.

##### 5.1.1.2.3 — Почему важны две стадии

| Stage 1| Stage 2|
|------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |
| Не позволяет модели сильно отклониться от изначальных весов (*no catastrophic forgetting*) | Позволяет модели научиться извлекать знания <br> *(Важно проверять, чтобы в QA были только вопросы по Stage 1)* |
| Позволяет обучиться на доменах |

**Вывод:** двухстадийная настройка модели обеспечивает баланс между сохранением базовых знаний (Stage 1) и обучением на новой информации (Stage 2).

---

### 5.1.2 SFT

#### 5.1.2.1 — Как выбрать данные для SFT

Для Supervised Fine-Tuning (SFT) критически важно качество и релевантность обучающего датасета. Ниже — три ключевых источника, откуда можно брать данные:

##### 5.1.2.1.1 Перевод англоязычных instruct-датасетов

* Это самый распространённый способ создать русскоязычные обучающие выборки.
* Пример — перевод датасета **Synthia** (или другого OpenAssistant-like формата).
* Риски: искажение смысла при машинном переводе, отсутствие культурной адаптации.
* Способ решения: использовать LLM с alignment-целями в переводе и постредактирование.

##### 5.1.2.1.2 Использование готовых датасетов на русском языке

* Например, датасет [GrandMaster-PRO-MAX](https://huggingface.co/datasets/Vikhrmodels/GrandMaster-PRO-MAX) от VikhrModels на HuggingFace.
* Обычно такие датасеты уже содержат разметку в формате "вопрос-ответ", "инструкция-реакция", или даже цепочки размышлений (Chain-of-Thought).
* Плюсы: быстро, готово к использованию.
* Минусы: непонятна юридическая чистота, часто требуется доп. фильтрация и выравнивание стиля.

##### 5.1.2.1.3 Разметка людьми

* Самый затратный, но самый надёжный способ.
* Даёт максимально качественные примеры: инструкции, ответы, рассуждения, а также может включать мультирольевые сценарии.
* Подходит для построения RLAIF (Reinforcement Learning from AI Feedback), а также для обучения цепочкам рассуждений.
* Можно использовать: краудсорсинг, собственных экспертов, подсказки от LLM с ревизией.

---

> Важно: при выборе источника всегда проверяй согласованность формата, юридическую чистоту и разнообразие тем (покрытие доменов: общие знания, математика, логика, диалоги и т.п.).

### 5.1.3 Роль Alignment в Domain Adaptation

1. Alignment, как и SFT, **не добавляет новые знания**, но помогает **лучше извлекать уже имеющиеся**. Например, это может выражаться в том, что модель начинает подкреплять утверждения ссылками или примерами.

2. Alignment **особенно полезен для обучения модели по принципам helpful, honest, harmless (3H)**. То есть, он не усиливает содержание модели, а корректирует форму выдачи, делая её более соответствующей ожиданиям пользователей и нормам общения.

---

## 5.2. LoRA (CPT + SFT)

LoRA (Low-Rank Adaptation) — один из самых дешевых и популярных методов дообучения больших языковых моделей. Вместо того чтобы дообучать всю матрицу весов, LoRA вставляет адаптационные слои низкого ранга (low-rank matrices) и обучает только их. Это позволяет:

* Дообучать < 1% параметров модели,
* Существенно экономить память (примерно в 3 раза),
* Не влиять на скорость инференса (во время вывода веса можно объединить).

Метод хорошо работает как в режиме Continued Pretraining (CPT), так и Supervised Fine-Tuning (SFT), особенно при ограниченных ресурсах.

### 5.2.1 Устройство LoRA

LoRA внедряется в линейные слои, заменяя матрицу $W \in \mathbb{R}^{d \times d}$ следующей конструкцией:

$$
W' = W + \Delta W,\quad \Delta W = BA,\quad A \in \mathbb{R}^{r \times d},\ B \in \mathbb{R}^{d \times r}
$$

* При инициализации: $A \sim \mathcal{N}(0, \sigma^2)$, $B = 0$
* На инференсе: $\Delta W$ можно свернуть в обычную матрицу и использовать как $W'$
* Типичные размеры: $d = 768$, $r = 4$, тогда $A + B$ содержит $768 \times 4 + 4 \times 768 = 6144$ параметров против $768^2 = 589824$

Итого: дообучается всего около 1% параметров, а качество сохраняется почти на уровне полной донастройки.

**Потенциальные проблемы**:
- Если данных слишком много или они слишком отличаются от оригинального распределения, модель может "забыть" старые навыки.
- Особенно актуально при дообучении на фактах — наблюдается снижение точности на предыдущих задачах

### 5.2.2 Рецепт: как делать LoRA (CPT + SFT)

Модель дообучается в две стадии с использованием LoRA-адаптеров:

- **Stage 1: CPT (Continual Pretraining)**  
- **Stage 2: SFT (Supervised Fine-Tuning) + Alignment**

#### Ключевые особенности:

- **Гиперпараметры (HP)** придётся перебрать — особенно `r`, `alpha`, `dropout`, `target_modules`, `learning_rate` и др.
- **Возможно, придётся пересобрать датасет** — особенно, если токенизатор адаптировался или был заменён.
- **Tokeniser Adaptation** — важна, если у новой модели другой словарь (см. раздел 5.1.3).
- **Оценка** производится через набор бенчмарков и SBS (Single Benchmark Scores).
- Возможна итеративная подстройка CPT/SFT после оценки.

---
# 6. Валидация качества
## 6.1 Бенчмарки
1. Бенчмарки компании.
2. Eсли их нет, то начать можно отсюда:
[EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

Эта библиотека позволяет запускать популярные задачи вроде:

* LAMBADA
* HellaSwag
* ARC
* MMLU
* TruthfulQA
* и многих других

Оценка проводится по множеству метрик: accuracy, multiple-choice accuracy, perplexity и др.

> Хорошая практика — запускать бенчмарки после каждого крупного этапа адаптации (например, после CPT и после SFT).

## 6.2 Типичные проблемы

Какие бывают проблемы при адаптации модели:

1. **Галлюцинации** — модель начинает выдумывать факты или давать неадекватные ответы даже на простые вопросы.
2. **Lang-swaps** — внезапная смена языка посреди генерации (например, с русского на английский).
3. **Падают общие бенчмарки** — несмотря на улучшение в SFT, общая способность модели решать стандартные задачи может ухудшиться.

### 6.2.1 Галлюцинации

Модель придумывает ответ, которого не знает:

#### Как бороться:

1. **Синтетические QA-пары** на данных из претрейна (например, генерация вопросов по текстам, входившим в pretraining).

2. **Правильный батчлоадер**:

   * При формировании батча важно следить, чтобы вход и соответствующий ответ были вместе, иначе возможна генерация "ответов на незаданные вопросы".
   * При data packing несколько коротких примеров объединяются в длинную последовательность. Если не задать attention mask, модель может начать использовать информацию из других сегментов внутри батча.
   * Решение — использовать attention mask с длинами (например, `attention_mask_in_length`), чтобы модель видела только в пределах каждого фрагмента.
   * Так работает `flash_attn_varlen_qkvpacked_func` из [FlashAttention](https://github.com/Dao-AILab/flash-attention): он принимает список длин сегментов и корректно применяет маску внимания к каждому.

### 6.2.2 Lang-swaps

Иногда даже ChatGPT позволяет себе внезапно переключиться на другой язык. Обычно это происходит в ответах, где есть математические или кодовые формулировки, и модель неустойчиво удерживает язык контекста.

#### Как бороться:

1. **Нужно понять источник проблемы** — найти, в каких именно типах данных модель учится вставлять английский текст.
2. **Главные подозреваемые** — код, математика, англоязычные RLHF-наборы.
3. **Проводим ablation** — проверяем поведение модели на каждом из датасетов, входящих в смесь дообучения.
4. **Удаляем / переписываем** такие фрагменты вручную или с помощью другой LLM.

### 6.2.3 Падают общие бенчмарки

Если после дообучения наблюдается снижение качества на стандартных задачах (MMLU, HellaSwag и т.п.), скорее всего:

* смесь данных для Stage 1 (CPT) оказалась слишком далека от оригинального претрейна,
* или была выбрана слишком агрессивная learning rate.

#### Что делать:

1. **Уменьшить количество новых знаний** — сделать смесь более похожей на pretrain, добавить туда «старые» данные (например, Common Crawl).
2. **Перебрать LR (learning rate)** — уменьшение может снизить катастрофическое забывание.

# 7. Разбор кейсов

## 7.1 Адаптация под русский язык

### Какую базовую модель возьмём?

* Qwen 3, Gemma 3, LLaMA 3.2 — любая SOTA-модель с multilingual поддержкой и открытым исходным кодом.

### На каких данных обучим?

* **Общие:** Fineweb, C4
* **Русские:** ruWiki, книги, новости, форумы
* **Instruct:** аксессоры, синтетические задания, переводы

### Адаптируем ли токенизатор?

* Сначала смотрим сжатие (compression ratio) на русскоязычных текстах
* Если сильно хуже английского — адаптируем токенизатор или делаем merge

### Как обучаем?

* Reuse don’t retrain
* Используем LoRA
* CPT обязателен перед SFT, иначе качество сильно падает

### Как валидируем?

* LM Evaluation Harness
* MERA (наборы для русской оценки)
* Аксессоры и ручные проверки

---

## 7.2 Сборка данных code + math

### С чего начать?

* Готовые датасеты: The Stack, Starcoder Data, OpenMathInstruct-1, BigMath

### Откуда брать ещё?

* Парсим вручную: github-repo-parser, arXiv parser

### А если парсить не хочется?

* Вытаскиваем релевантные документы из C4 с помощью классификаторов: `python-edu-scorer`, `bert-based classification`

### Как выбираем смесь?

* Дообучаем LoRA на разных смесях, делаем много ablation-экспериментов

### Как валидируем?

* **Code:** HumanEval (целимся в \~100%), Swebench
* **Math:** MMLU-Pro, GSM8K (когда насытился), AIME