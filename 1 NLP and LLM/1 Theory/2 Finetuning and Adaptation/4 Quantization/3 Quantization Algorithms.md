# Алгоритмы квантования

## 1. GPTQ (Post-training quantization)

GPTQ (Gradient Post-Training Quantization) — это метод квантования, позволяющий преобразовать веса модели в low-bit представление **без переобучения**, сохраняя при этом высокую точность. Он ориентирован на LLM и применяется к уже обученной модели (PTQ).

### Основные идеи

* Выполняется после обучения (post-training).
* Квантуются только **веса** (weight-only quantization), активации остаются в FP16/FP32.
* Основан на **greedy-алгоритме**, минимизирующем ошибку восстановления выходного тензора:

$$
\text{minimize} \; ||Y - \hat{Y}||_F^2
$$

где $Y$ — выход слоя до квантования, $\hat{Y}$ — после квантования весов.

* Для выбора ближайшего кванта используется **codebook** на группу весов.
* Поддерживает **group-wise квантование**: веса разбиваются на группы (например, по 128), и внутри группы используется общий codebook и scale.

### Этапы GPTQ

1. Прогон модели на калибровочном датасете, сохранение выходов.
2. Поэлементный перебор возможных значений весов из codebook, чтобы минимизировать ошибку выхода.
3. Сохранение квантованных весов + scale.
4. (Опционально) добавление компенсации ошибки с помощью bias или residual error compensation.

### Преимущества

* Очень эффективен при int4 / int3 квантовании.
* Не требует переобучения.
* Гибко работает с разными группами, шагами и схемами.

### Недостатки

* Относительно медленный квантовочный этап (особенно на больших моделях).
* Требует калибровочного датасета и прогонов модели.
* Используется только для весов, не применим к активациям.

### Используется в:

* `AutoGPTQ` — популярная библиотека с интеграцией в HuggingFace.
* `llama.cpp` — C++ реализация LLaMA с поддержкой 4bit GPTQ.
* `text-generation-webui` — Web UI, позволяющий загружать и использовать GPTQ-модели.

---

## 2. AWQ (Activation-aware Weight Quantization)

AWQ — это постобучающий метод квантования весов, который **учитывает влияние весов на активации**, чтобы сохранить качество выходных представлений модели.

### Основные идеи

* Квантуются **только веса**, активации остаются в float.
* В отличие от GPTQ, при подборе весов учитывается **их вклад в выходной сигнал (активации)**.
* Используется метод под названием **WNAM (Weight-only Non-uniform Asymmetric Mixed quantization)**.

### WNAM в деталях

* **Индексы кодируются в INT4**, указывая на позиции в индивидуальном **codebook** (на группу).
* Для каждой группы весов (например, 128 элементов) используется свой **scale** и **zero-point**.
* Это **non-uniform**, **асимметричное** и **group-wise** квантование.
* Используется кодирование с высокой чувствительностью к важным каналам (например, через importance scores).

### Этапы AWQ (Activation-aware Weight Quantization)

1. **Forward-pass с захватом активаций:**

   * Модель прогоняется на калибровочном датасете.
   * Сохраняются промежуточные активации для всех слоёв.

2. **Оценка важности каналов:**

   * Вычисляется contribution каждого веса к выходу слоя (например, через градиент или sensitivity).
   * Полученные importance scores используются для выбора оптимального scale/codebook.

3. **Квантование весов с учётом влияния на активации:**

   * Подбирается INT4 код и индивидуальный codebook так, чтобы сохранить выход модели максимально близким к оригинальному.
   * Применяется asymmetric quantization: индивидуальный scale и zero-point для каждой группы весов.

4. **Оптимизация и свёртка:**

   * Группы весов кодируются в INT4 + метаданные (scale, codebook).
   * Квантованные веса вставляются обратно в модель без изменения архитектуры forward-pass.

5. **Проверка финального качества (опционально):**

   * На калибровочном или валидационном датасете проверяется падение метрик (например, perplexity).

Эти шаги делают AWQ эффективным методом для production-инференса LLM в INT4 режиме.


### Преимущества

* Очень быстрый и лёгкий inference.
* Лучше сохраняет качество выходов, особенно при int4.
* Не требует модификации forward-pass.

### Недостатки

* Применяется только к весам (не снижает требования к памяти активаций).
* Может не давать выигрыша, если модель уже очень устойчива к квантованию.

### Используется в:

* `awq` (PyTorch-библиотека от MIT)
* `llama-awq` — CLI и quantizer на базе llama.cpp

---  
## 3 RTN (Random Tensor Normalization)

RTN — это минималистичный и сверхбыстрый baseline для post-training quantization, в котором веса просто нормализуются и округляются без учёта влияния на выход модели.

### Основные особенности

* **Weight-only PTQ**: только веса квантованы, активации остаются float.
* Использует **равномерное (uniform) квантование**.
* Не требует калибровочного датасета или дополнительных прогонов.
* Применяется как baseline для оценки других методов.

### Этапы RTN

1. **Нормализация весов:**

   * Для каждой группы или всего тензора весов вычисляется максимум по абсолютному значению:
     $scale = \max(|W|) / (2^{k-1} - 1)$
   * Пример: для INT4, $2^{4-1} - 1 = 7$ (размах от -7 до +7).

2. **Округление:**

   * Веса квантуются по формуле:
     $w_{int} = \text{round}(W / scale)$

3. **Обрезание переполнения:**

   * Значения clamped в допустимый диапазон (например, \[-7, 7] для int4).

4. **Сохранение scale и квантованных весов:**

   * Для восстановления используется:
     $W \approx scale \times w_{int}$

### Преимущества

* Очень быстрый, может применяться к любой модели без подготовки.
* Лёгкая реализация, не требует кода анализа активаций.

### Недостатки

* Значительно хуже по точности, особенно на чувствительных слоях.
* Не учитывает структуру модели и важность каналов.
* Используется только для веса, без поддержки активаций.

### Применяется в:

* baseline сравнения методов (GPTQ, AWQ)
* быстрая предварительная конвертация моделей

