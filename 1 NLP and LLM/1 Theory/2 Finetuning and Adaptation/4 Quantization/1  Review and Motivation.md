# Обзор и мотивация low-bit inference

## Зачем нужно квантование?

Квантование (quantization) — это преобразование весов и/или активаций модели из 32-битных float значений в более компактные представления: int8, int4 и даже binary. Это критически важно для:

* **Снижения задержек (latency):** меньшее количество битов ускоряет матричные операции.
* **Экономии памяти:** можно запускать огромные модели (LLaMA 65B) на устройствах с 16 ГБ VRAM.
* **Inference на edge-устройствах и low-resource окружениях.**
* **Уменьшения стоимости инференса в проде.**

Пример: запуск LLaMA 65B в 4-битах на 16GB GPU — без квантования это невозможно.

## Подходы к квантованию

### Post-Training Quantization (PTQ)

PTQ — это применение квантования к уже обученной модели. Используется небольшая калибровочная выборка для сбора статистики весов и активаций. Затем применяются алгоритмы вроде min-max или percentile-based для расчета параметров scale/zero-point.

**Преимущества:**

* Быстрое и простое применение.
* Не требует доступа к полному датасету или пайплайну обучения.
* Идеально подходит для продакшн-оптимизации больших моделей.

**Недостатки:**

* Может приводить к падению точности, особенно в int4 и ниже.
* Не учитывает влияние квантования на градиенты — возможны артефакты.

**Популярные реализации:**

* GPTQ (Greedy оптимизация по ошибке)
* AWQ (weight-only + компенсация активаций)
* Rtn (простая нормализация, высокая скорость, но ниже точность)

### Quantization-Aware Training (QAT)

QAT — это обучение модели с имитацией квантованных операций внутри. Квантование вставляется в граф во время тренировки, что позволяет модели адаптироваться к ограниченной точности.

**Преимущества:**

* Существенно выше точность, особенно при int4/int2/binary.
* Модель учится быть устойчивой к шуму квантования.

**Недостатки:**

* Требует доступа к данным и обучающему пайплайну.
* Долгое время обучения, высокий compute cost.

**Когда использовать:**

* Крайне ограниченные окружения (edge, мобильные чипы).
* Модели, чувствительные к снижению точности.

**Фреймворки:**

* PyTorch QAT API
* TensorFlow Model Optimization Toolkit

## Примеры моделей с low-bit inference

* **LLaMA 65B в 4-bit**:

  * Запускается на 16 ГБ VRAM.
  * Использует GPTQ или AWQ.
  * Производительность сравнима с 16-bit, но вес меньше в 8 раз.

* **Mistral, LLaMA2, LLaMA3**:

  * Активно используются с AutoGPTQ и bitsandbytes.
  * Часто применяют AWQ или GPTQ.

## Кейсы использования

* **Мобильные/edge inference:** где критичны размер и скорость.
* **Запуск больших моделей на бюджетных GPU:** RTX 3060, A10, T4.
* **Продакшн-инфраструктуры с высокой нагрузкой:** сокращение затрат на GPU-инстансы.

## Различия методов квантования

| Метод | Тип | Особенности                               | Точность |
| ----- | --- | ----------------------------------------- | -------- |
| GPTQ  | PTQ | Greedy оптимизация, поддержка int4        | Высокая  |
| AWQ   | PTQ | Только веса, активации остаются FP16/FP32 | Высокая  |
| Rtn   | PTQ | Простая нормализация тензоров, быстрая    | Ниже     |
