# –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π fine-tuning (FP32, full finetune)

URL:  
üîó https://huggingface.co/course/chapter3


## –ß—Ç–æ —Ç–∞–∫–æ–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π fine-tuning?

–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π fine-tuning ‚Äî —ç—Ç–æ –ø–æ–ª–Ω–∞—è –¥–æ–æ–±—É—á–∞–µ–º–æ—Å—Ç—å –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –ü–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ FP32 (32-–±–∏—Ç–Ω—ã–µ –≤–µ—Å–∞), –±–µ–∑ –∑–∞–º–æ—Ä–æ–∑–∫–∏ —Å–ª–æ—ë–≤, adapter-—Å–ª–æ—ë–≤, LoRA –∏ –¥—Ä—É–≥–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π.

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**

- –ö–æ–≥–¥–∞ —É –≤–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ—Å—É—Ä—Å–æ–≤.
- –ö–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ.
- –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –≥–ª—É–±–æ–∫–æ –ø–æ–¥ –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–¥–∞–ø—Ç–∞—Ü–∏—è GPT –∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º).

## –ú–æ—Ç–∏–≤–∞—Ü–∏—è

- –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–±–æ–±—â—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, Common Crawl).
- –ó–∞–¥–∞—á–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–æ–∂–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –ø–æ —Å—Ç–∏–ª—é, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∏–ª–∏ –¥–æ–º–µ–Ω—É.
- –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (fine-tuning) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∑–∞–¥–∞—á—É, —É–ª—É—á—à–∞—è –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.

## –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç

–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –æ–±—ã—á–Ω–æ, —Å –Ω—É–ª—è ‚Äî –Ω–æ –Ω–∞—á–∏–Ω–∞—è –Ω–µ —Å —Ä–∞–Ω–¥–æ–º–Ω—ã—Ö –≤–µ—Å–æ–≤, –∞ —Å –≤–µ—Å–æ–≤ —É–∂–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (pretrained checkpoint).

–§–æ—Ä–º–∞–ª—å–Ω–æ:

$$
\theta^* = \arg\min_{\theta} \sum_{(x, y) \in D} \mathcal{L}(f_\theta(x), y)
$$

–ì–¥–µ:

- $\theta$ ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
- $D$ ‚Äî –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ (fine-tune dataset)
- $\mathcal{L}$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, cross-entropy –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)

## –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

- **–§—Ä–µ–π–º–≤–æ—Ä–∫–∏:** `transformers`, `accelerate`, `Trainer`, `deepspeed`, `bitsandbytes` (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
- **–î–µ–≤–∞–π—Å—ã:** GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π FP32, –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ >= A100 40GB –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å LLM
- **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã:** AdamW (–Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–µ–Ω)
- **Scheduler:** –ª–∏–Ω–µ–π–Ω—ã–π —Å warm-up, cosine decay

–ü—Ä–∏–º–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `transformers`:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

train_encodings = tokenizer(train_texts, truncation=True, padding=True)

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encodings,
)

trainer.train()
```

## –¢–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è

- batch\_size: 4‚Äì32 (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞–º—è—Ç–∏)
- learning\_rate: 2e-5 ‚Äî 5e-5 (–∏–Ω–æ–≥–¥–∞ –º–µ–Ω—å—à–µ)
- warmup\_steps: 500‚Äì2000
- weight\_decay: 0.01

## –ü–æ–¥–≤–æ–¥–Ω—ã–µ –∫–∞–º–Ω–∏

- **Catastrophic forgetting** ‚Äî –º–æ–¥–µ–ª—å "–∑–∞–±—ã–≤–∞–µ—Ç" –æ–±—â–µ–µ –∑–Ω–∞–Ω–∏–µ, –µ—Å–ª–∏ fine-tune –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
- **Overfitting** ‚Äî –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.
- **–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π LR** –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ –∏ —Å–Ω–∏–∂–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞.
- **–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–∫–∞—Ç–∏—Ç—å** ‚Äî –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π fine-tune –∑–∞—Ç–∏—Ä–∞–µ—Ç –≤–µ—Å–∞, –Ω–µ –¥–∞–≤–∞—è —Å—Ä–∞–≤–Ω–∏—Ç—å –ª–µ–≥–∫–æ –¥–æ/–ø–æ—Å–ª–µ.

## –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞

- –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –∑–∞–¥–∞—á–∏: Rouge/BLEU (summarization), Accuracy/F1 (classification), Perplexity (LM).
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å zero-shot –∏ few-shot baseline.

## Use-case –ø—Ä–∏–º–µ—Ä—ã

1. **GPT2 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∫–∞–∑–æ–∫ –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–º —è–∑—ã–∫–µ** ‚Äî –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–æ—Ä–ø—É—Å–µ —Å–ª–∞–≤—è–Ω—Å–∫–∏—Ö —Å–∫–∞–∑–æ–∫.
2. **BERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤** ‚Äî fine-tune –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö.
3. **T5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤** ‚Äî –ø–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ text2sql.

## –û—Ç–ª–∏—á–∏—è –æ—Ç –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

| –ú–µ—Ç–æ–¥               | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã      | –ü–∞–º—è—Ç—å       | –ö–∞—á–µ—Å—Ç–≤–æ       | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ                   |
| ------------------- | -------------- | ------------ | -------------- | ---------------------------- |
| FP32 full fine-tune | –í—Å–µ            | –í—ã—Å–æ–∫–∞—è      | –í—ã—Å–æ–∫–æ–µ        | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è       |
| LoRA / QLoRA        | \~0.5‚Äì2% —Å–ª–æ—ë–≤ | –ù–∏–∑–∫–∞—è       | –°—Ä–µ–¥–Ω–µ–µ        | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã         |
| Prompt-tuning       | –¢–æ–ª—å–∫–æ prompt  | –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è | –ù–∏–∑–∫–æ–µ/—Å—Ä–µ–¥–Ω–µ–µ | –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è, eval only |